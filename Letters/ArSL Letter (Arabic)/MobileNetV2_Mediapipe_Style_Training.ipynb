{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c6fffb",
   "metadata": {},
   "source": [
    "# MobileNetV2 Training - Mediapipe Style\n",
    "\n",
    "This notebook demonstrates MobileNetV2 training and real-time inference for sign language recognition, styled and optimized like the Mediapipe notebooks. It includes:\n",
    "- GPU detection/configuration\n",
    "- Mixed precision logic (enabled if supported)\n",
    "- Batch size and memory tips\n",
    "- Training/evaluation summaries\n",
    "- Progress bars for long steps\n",
    "- Real-time camera input and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Install and Import Required Libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# For GPU detection/configuration\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import (GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "print('‚úì All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc51af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: GPU Detection and Configuration\n",
    "print('='*60)\n",
    "print('üîç GPU DETECTION & CONFIGURATION')\n",
    "print('='*60)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'Found GPUs: {gpus}')\n",
    "USE_GPU = False\n",
    "DEVICE = '/CPU:0'\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        USE_GPU = True\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(f'‚úÖ GPU configured: {gpus[0]}')\n",
    "    except RuntimeError as e:\n",
    "        print('‚ö†Ô∏è  GPU config error:', e)\n",
    "# Mixed precision (optional and beneficial on modern GPUs)\n",
    "try:\n",
    "    if USE_GPU:\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print('‚ö° Mixed precision enabled:', policy.name)\n",
    "except Exception as e:\n",
    "    print('‚ö†Ô∏è  Mixed precision not enabled:', e)\n",
    "print('Configuration complete. Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c030e771",
   "metadata": {},
   "source": [
    "## Batch Size and Memory Tips\n",
    "- If you get 'Out of Memory' errors, reduce `BATCH_SIZE` or disable mixed precision.\n",
    "- Monitor GPU usage with `nvidia-smi -l 1` in a separate terminal.\n",
    "- Close other GPU-intensive applications during training for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d120fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Load and Preprocess Mobile Camera Input\n",
    "# For demo, use sample video or webcam\n",
    "VIDEO_SOURCE = 0  # 0 for webcam, or path to video file\n",
    "IMG_SIZE = 128\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "frames = []\n",
    "print('Capturing 10 frames for preprocessing demo...')\n",
    "for i in tqdm(range(10), desc='Capturing frames'):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frames.append(frame)\n",
    "cap.release()\n",
    "frames = np.array(frames)\n",
    "print(f'Captured frames shape: {frames.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33da0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Initialize and Configure MobileNetV2 Model\n",
    "with tf.device(DEVICE):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    base_model.trainable = False\n",
    "    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(10, activation='softmax', dtype='float32')(x)  # 10 classes for demo\n",
    "    model = Model(inputs, outputs)\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print('Model summary:')\n",
    "    model.summary()\n",
    "print(f'Model will train on device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eeada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Run Real-Time Inference on Mobile Frames\n",
    "# For demo, run inference on captured frames\n",
    "print('Running inference on demo frames...')\n",
    "preds = []\n",
    "for frame in tqdm(frames, desc='Inference'):\n",
    "    input_tensor = np.expand_dims(frame, axis=0).astype('float32')\n",
    "    with tf.device(DEVICE):\n",
    "        pred = model.predict(input_tensor, verbose=0)\n",
    "    preds.append(pred)\n",
    "preds = np.array(preds)\n",
    "print(f'Predictions shape: {preds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59264163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Visualize Detection Results\n",
    "# For demo, overlay random predictions on frames\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i in range(len(frames)):\n",
    "    ax = plt.subplot(2, 5, i+1)\n",
    "    img = frames[i]\n",
    "    plt.imshow(img)\n",
    "    pred_class = np.argmax(preds[i][0])\n",
    "    plt.title(f'Pred: {pred_class}')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fa86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Save or Export Processed Frames\n",
    "# For demo, save annotated frames to disk\n",
    "os.makedirs('output_frames', exist_ok=True)\n",
    "for i, img in enumerate(frames):\n",
    "    out_path = f'output_frames/frame_{i}_pred_{np.argmax(preds[i][0])}.png'\n",
    "    cv2.imwrite(out_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "print('Saved annotated frames to output_frames/')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
