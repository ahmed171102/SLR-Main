{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3558d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import mediapipe as mp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc40c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# GPU DETECTION & CONFIGURATION\n",
    "# ===============================\n",
    "print('='*60)\n",
    "print('üîç GPU DETECTION & CONFIGURATION')\n",
    "print('='*60)\n",
    "print(f'\n",
    "TensorFlow version: {tf.__version__}')\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'Found GPUs: {gpus}')\n",
    "USE_GPU = False\n",
    "DEVICE = '/CPU:0'\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        USE_GPU = True\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(\n",
    "            f'‚úÖ GPU configured: {gpus[0]}')\n",
    "    except RuntimeError as e:\n",
    "        print('‚ö†Ô∏è  GPU config error:', e)\n",
    "# Mixed precision (optional and beneficial on modern GPUs)\n",
    "try:\n",
    "    if USE_GPU:\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print('‚ö° Mixed precision enabled:', policy.name)\n",
    "except Exception as e:\n",
    "    print('‚ö†Ô∏è  Mixed precision not enabled:', e)\n",
    "print('\n",
    "Configuration complete. Using device: ', DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7fafbc",
   "metadata": {},
   "source": [
    "GPU tips: If you run out of memory, reduce `BATCH_SIZE` or disable mixed precision. Use `nvidia-smi` in a separate terminal to monitor GPU usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74daf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# MEDIAPIPE KEYPOINT EXTRACTION (GPU-friendly)\n",
    "# ===============================\n",
    "CSV_PATH = 'asl_mediapipe_keypoints_dataset_gpu.csv'\n",
    "# Update this to your dataset location if different\n",
    "DATASET_DIR = r'M:erm 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ASL Letter (English)\\Datasets\\Asl_Sign_Data\\asl_alphabet_train'\n",
    "# If your ASL data is in a different path, change DATASET_DIR above\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print('Dataset CSV already exists:', CSV_PATH)\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f'Samples loaded: {len(df)}')\n",
    "else:\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7)\n",
    "    landmark_data = []\n",
    "    labels = []\n",
    "    # gather all image paths\n",
    "    class_labels = sorted([d for d in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, d))])\n",
    "    all_images = []\n",
    "    for label in class_labels:\n",
    "        folder = os.path.join(DATASET_DIR, label)\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "        for f in files:\n",
    "            all_images.append((label, os.path.join(folder, f)))\n",
    "    print(f'Found {len(all_images)} images across {len(class_labels)} classes')\n",
    "    # process with a progress bar\n",
    "    start = time.time()\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    for label, img_path in tqdm(all_images, desc='Extracting keypoints'):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            # For multi-hand cases, take the first detected hand (common for single-hand dataset)\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            lm = np.array([[p.x, p.y, p.z] for p in hand_landmarks.landmark]).flatten()\n",
    "            landmark_data.append(lm)\n",
    "            labels.append(label)\n",
    "            processed += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "    duration = time.time() - start\n",
    "    if len(landmark_data) == 0:\n",
    "        print('No landmarks extracted. Check dataset or MediaPipe settings.')\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        df = pd.DataFrame(landmark_data)\n",
    "        df['label'] = labels\n",
    "        df.to_csv(CSV_PATH, index=False)\n",
    "        print('Saved keypoints to', CSV_PATH)\n",
    "        print(f'Processed: {processed}, Skipped: {skipped}, Time: {duration:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfe980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# PREPROCESSING & SPLIT\n",
    "# ===============================\n",
    "df = pd.read_csv('asl_mediapipe_keypoints_dataset_gpu.csv')\n",
    "print('Loaded samples:', len(df))\n",
    "X = df.iloc[:, :-1].astype('float32').values\n",
    "y = df['label'].values\n",
    "encoder = LabelEncoder()\n",
    "y_enc = encoder.fit_transform(y)\n",
    "num_classes = len(encoder.classes_)\n",
    "# stratified split\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y_enc, test_size=0.2, random_state=42, stratify=y_enc)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "# one-hot encode labels\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "# cast features\n",
    "X_train = X_train.astype('float32')\n",
    "X_val = X_val.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "print('Train/Val/Test sizes:', X_train.shape[0], X_val.shape[0], X_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient tf.data pipeline helper\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "def make_dataset(features, labels, batch_size, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if training:\n",
    "        buffer = min(len(features), 10000)\n",
    "        ds = ds.shuffle(buffer_size=buffer, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eabfc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# MODEL CREATION (GPU-OPTIMIZED)\n",
    "# ===============================\n",
    "tf.keras.backend.clear_session()\n",
    "with tf.device(DEVICE):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-4), input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "# Use legacy Adam if mixed precision is set (works well with float16 activations)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "print('Model will train on device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381af846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# TRAINING (GPU-OPTIMIZED)\n",
    "# ===============================\n",
    "if USE_GPU:\n",
    "    BATCH_SIZE = 256\n",
    "else:\n",
    "    BATCH_SIZE = 64\n",
    "train_ds = make_dataset(X_train, y_train, BATCH_SIZE, training=True)\n",
    "val_ds = make_dataset(X_val, y_val, BATCH_SIZE, training=False)\n",
    "callbacks = [\n",
    "    ModelCheckpoint('asl_mediapipe_mlp_model_gpu_best.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1),\n",
    "]\n",
    "print('Training with batch size:', BATCH_SIZE)\n",
    "start = time.time()\n",
    "with tf.device(DEVICE):\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=callbacks, verbose=1)\n",
    "duration = time.time() - start\n",
    "print(f'Training finished in {duration:.2f}s')\n",
    "model.save('asl_mediapipe_mlp_model_gpu.h5')\n",
    "print('Saved model: asl_mediapipe_mlp_model_gpu.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d3486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# EVALUATION\n",
    "# ===============================\n",
    "model = tf.keras.models.load_model('asl_mediapipe_mlp_model_gpu.h5')\n",
    "eval_batch = 256 if USE_GPU else 128\n",
    "test_ds = make_dataset(X_test, y_test, eval_batch, training=False)\n",
    "with tf.device(DEVICE):\n",
    "    loss, acc = model.evaluate(test_ds, verbose=1)\n",
    "print(f'Test Loss: {loss:.4f}  Test Accuracy: {acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d1ddd2",
   "metadata": {},
   "source": [
    "## Webcam Inference (MediaPipe + MLP)\n",
    "The following cell runs webcam inference. Press `q` to quit. It uses GPU for model prediction when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff9eb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# WEBCAM INFERENCE\n",
    "# ===============================\n",
    "encoder = LabelEncoder()\n",
    "df_labels = pd.read_csv('asl_mediapipe_keypoints_dataset_gpu.csv')\n",
    "encoder.fit(df_labels['label'])\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "cap = cv2.VideoCapture(0)\n",
    "predicted_sentence = ''\n",
    "mlp_model = tf.keras.models.load_model('asl_mediapipe_mlp_model_gpu.h5')\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            lm = np.array([[p.x, p.y, p.z] for p in hand_landmarks.landmark]).flatten().reshape(1, -1)\n",
    "            input_tensor = tf.cast(lm, tf.float32)\n",
    "            with tf.device(DEVICE):\n",
    "                pred = mlp_model.predict(input_tensor, verbose=0)\n",
    "            pred_class = np.argmax(pred)\n",
    "            label = encoder.inverse_transform([pred_class])[0]\n",
    "            if label == 'SPACE':\n",
    "                predicted_sentence += ' '\n",
    "            elif label == 'DELETE':\n",
    "                predicted_sentence = predicted_sentence[:-1]\n",
    "            elif label == 'NOTHING':\n",
    "                pass\n",
    "            else:\n",
    "                predicted_sentence += label\n",
    "    # draw sentence bar\n",
    "    h, w, _ = frame.shape\n",
    "    cv2.rectangle(frame, (0, h-60), (w, h), (0, 0, 0), -1)\n",
    "    cv2.putText(frame, predicted_sentence, (10, h-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv2.imshow('GPU-Optimized Sign Prediction', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e68a8c",
   "metadata": {},
   "source": [
    "### Next steps / Notes:\n",
    "- If your dataset path differs, update `DATASET_DIR` (cell 5).\n",
    "- To re-extract keypoints, delete the CSV `asl_mediapipe_keypoints_dataset_gpu.csv` and re-run the extraction cell.\n",
    "- If you see OOM errors: lower `BATCH_SIZE` or disable mixed precision.\n",
    "- You can enable `nvidia-smi -l 1` in a separate PowerShell to monitor GPU while training."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
