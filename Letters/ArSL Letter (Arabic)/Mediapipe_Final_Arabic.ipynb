{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef53c74d",
   "metadata": {},
   "source": [
    "# Final MediaPipe Notebook for Arabic Sign Language Letter Recognition\n",
    "\n",
    "This comprehensive notebook combines the best features from all previous notebooks for training and deploying a MediaPipe-based Arabic sign language recognition system.\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "- **GPU Optimization**: Automatic GPU detection and configuration\n",
    "- **Mixed Precision Training**: Faster training on supported GPUs\n",
    "- **Data Extraction**: MediaPipe keypoint extraction from your Arabic dataset\n",
    "- **Model Training**: GPU-optimized MLP model with early stopping and learning rate scheduling\n",
    "- **Real-Time Inference**: Webcam-based live prediction with Arabic letter recognition\n",
    "- **Memory Management**: Efficient data pipelines and memory-conscious batch sizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a5891a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.13) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âœ… All libraries imported successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 1: Import Required Libraries\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "import mediapipe as mp\n",
    "\n",
    "print('=' * 60)\n",
    "print('âœ… All libraries imported successfully!')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104300b5",
   "metadata": {},
   "source": [
    "## Section 2: GPU Detection and Configuration\n",
    "\n",
    "This section automatically detects your GPU, configures TensorFlow for optimal GPU usage, and enables mixed precision training if supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6206cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ” GPU DETECTION & CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "TensorFlow version: 2.10.0\n",
      "Found GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "âœ… GPU configured: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  NVIDIA GeForce MX150, compute capability 6.1\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "âš¡ Mixed precision enabled: mixed_float16\n",
      "\n",
      "âœ… Configuration complete. Using device: /GPU:0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 2: GPU DETECTION & CONFIGURATION\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('ðŸ” GPU DETECTION & CONFIGURATION')\n",
    "print('=' * 60)\n",
    "print(f'\\nTensorFlow version: {tf.__version__}')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'Found GPUs: {gpus}')\n",
    "\n",
    "USE_GPU = False\n",
    "DEVICE = '/CPU:0'\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        USE_GPU = True\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(f'âœ… GPU configured: {gpus[0]}')\n",
    "    except RuntimeError as e:\n",
    "        print(f'âš ï¸  GPU config error: {e}')\n",
    "\n",
    "# Mixed precision (optional and beneficial on modern GPUs)\n",
    "try:\n",
    "    if USE_GPU:\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print(f'âš¡ Mixed precision enabled: {policy.name}')\n",
    "except Exception as e:\n",
    "    print(f'âš ï¸  Mixed precision not enabled: {e}')\n",
    "\n",
    "print(f'\\nâœ… Configuration complete. Using device: {DEVICE}')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921bf60",
   "metadata": {},
   "source": [
    "## Section 3: Batch Size and Memory Tips\n",
    "\n",
    "**Important Memory Management:**\n",
    "\n",
    "- If you get 'Out of Memory' errors, reduce `BATCH_SIZE` (try 128 or 64)\n",
    "- Or disable mixed precision by setting the policy to 'float32'\n",
    "- Monitor GPU usage with `nvidia-smi -l 1` in a separate PowerShell terminal\n",
    "- Close other GPU-intensive applications (browser, video software, etc.) during training\n",
    "- MLP models are memory-efficient - batch size 256 typically requires ~1.5-2.5 GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a9164fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ” MEDIAPIPE KEYPOINT EXTRACTION FOR ARABIC LETTERS\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Dataset path: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArSL_Data_Labels.csv\n",
      "\n",
      "ðŸ“‚ Scanning dataset...\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] The directory name is invalid: 'M:\\\\Term 9\\\\Grad\\\\Main\\\\Sign-Language-Recognition-System-main\\\\Sign-Language-Recognition-System-main\\\\Sign_to_Sentence Project Main\\\\Datasets\\\\Dataset (ArASL)\\\\ArSL_Data_Labels.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Gather all image paths\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“‚ Scanning dataset...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET_DIR\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     38\u001b[0m                       \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATASET_DIR, d))])\n\u001b[0;32m     40\u001b[0m all_images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m class_labels:\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid: 'M:\\\\Term 9\\\\Grad\\\\Main\\\\Sign-Language-Recognition-System-main\\\\Sign-Language-Recognition-System-main\\\\Sign_to_Sentence Project Main\\\\Datasets\\\\Dataset (ArASL)\\\\ArSL_Data_Labels.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 4: Extract MediaPipe Keypoints from Arabic Dataset\n",
    "# ============================================\n",
    "\n",
    "# âš ï¸ IMPORTANT: Update this path to your Arabic dataset location\n",
    "DATASET_DIR = r'M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArSL_Data_Labels.csv'\n",
    "\n",
    "CSV_PATH = 'arsl_mediapipe_keypoints_final.csv'\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸ” MEDIAPIPE KEYPOINT EXTRACTION FOR ARABIC LETTERS')\n",
    "print('=' * 60)\n",
    "\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print(f'\\nðŸ“ Dataset CSV already exists: {CSV_PATH}')\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f'âœ… Samples loaded: {len(df)}')\n",
    "    print('ðŸ’¡ Skipping extraction. Use existing dataset.')\n",
    "else:\n",
    "    print(f'\\nðŸ”„ Dataset path: {DATASET_DIR}')\n",
    "    \n",
    "    if not os.path.exists(DATASET_DIR):\n",
    "        print(f'\\nâŒ ERROR: Dataset directory not found!')\n",
    "        print(f'   Please update DATASET_DIR to your Arabic dataset location')\n",
    "        print(f'   Expected path format: M:\\\\path\\\\to\\\\ArSL\\\\Letter\\\\(Arabic)\\\\Dataset')\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        # Initialize MediaPipe Hands\n",
    "        mp_hands = mp.solutions.hands\n",
    "        hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7)\n",
    "        \n",
    "        landmark_data = []\n",
    "        labels = []\n",
    "        \n",
    "        # Gather all image paths\n",
    "        print('\\nðŸ“‚ Scanning dataset...')\n",
    "        class_labels = sorted([d for d in os.listdir(DATASET_DIR) \n",
    "                              if os.path.isdir(os.path.join(DATASET_DIR, d))])\n",
    "        \n",
    "        all_images = []\n",
    "        for label in class_labels:\n",
    "            folder = os.path.join(DATASET_DIR, label)\n",
    "            files = [f for f in os.listdir(folder) \n",
    "                    if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            for f in files:\n",
    "                all_images.append((label, os.path.join(folder, f)))\n",
    "        \n",
    "        total_images = len(all_images)\n",
    "        num_classes = len(class_labels)\n",
    "        \n",
    "        print(f'   Found {total_images} images across {num_classes} Arabic letter classes')\n",
    "        if len(class_labels) > 0:\n",
    "            print(f'   Classes: {\", \".join(class_labels[:10])}{\"...\" if num_classes > 10 else \"\"}')\n",
    "        \n",
    "        # Process images with progress bar\n",
    "        print('\\nðŸ”„ Processing images...')\n",
    "        start_time = time.time()\n",
    "        processed_count = 0\n",
    "        skipped_count = 0\n",
    "        \n",
    "        for label, img_path in tqdm(all_images, desc='Extracting keypoints', unit='img'):\n",
    "            try:\n",
    "                img = cv2.imread(img_path)\n",
    "                \n",
    "                if img is None:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                results = hands.process(img_rgb)\n",
    "                \n",
    "                if results.multi_hand_landmarks:\n",
    "                    # Take the first detected hand\n",
    "                    hand_landmarks = results.multi_hand_landmarks[0]\n",
    "                    lm = np.array([[p.x, p.y, p.z] for p in hand_landmarks.landmark]).flatten()\n",
    "                    landmark_data.append(lm)\n",
    "                    labels.append(label)\n",
    "                    processed_count += 1\n",
    "                else:\n",
    "                    skipped_count += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Save the extracted data\n",
    "        print('\\nðŸ’¾ Saving dataset...')\n",
    "        \n",
    "        if len(landmark_data) == 0:\n",
    "            print('âŒ ERROR: No landmarks extracted. Check dataset path and image formats.')\n",
    "            df = pd.DataFrame()\n",
    "        else:\n",
    "            df = pd.DataFrame(landmark_data)\n",
    "            df['label'] = labels\n",
    "            df.to_csv(CSV_PATH, index=False)\n",
    "            \n",
    "            print('=' * 60)\n",
    "            print('âœ… EXTRACTION COMPLETE!')\n",
    "            print('=' * 60)\n",
    "            print(f'ðŸ“Š Statistics:')\n",
    "            print(f'   Total images: {total_images}')\n",
    "            print(f'   Successfully extracted: {processed_count}')\n",
    "            print(f'   Skipped (no hand detected): {skipped_count}')\n",
    "            print(f'   Processing time: {processing_time/60:.2f} minutes')\n",
    "            print(f'   Average time per image: {processing_time/total_images:.3f} seconds')\n",
    "            print(f'   Dataset saved: {CSV_PATH}')\n",
    "            print(f'   Dataset size: {len(df)} samples')\n",
    "            print(f'   Number of classes: {num_classes}')\n",
    "            print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found Ready-Made CSV: Arabic Sign Language Letters Dataset.csv\n",
      "   -> Renamed column 'letter' to 'label'\n",
      "ðŸŽ‰ SUCCESS! Loaded 7137 training samples.\n",
      "   You can now skip to Section 5/6 to train the model!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# REPLACEMENT FOR SECTION 4: LOAD CSV DIRECTLY\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Point to the file you uploaded\n",
    "CSV_PATH = 'Arabic Sign Language Letters Dataset.csv'\n",
    "\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print(f\"âœ… Found Ready-Made CSV: {CSV_PATH}\")\n",
    "    \n",
    "    # Load it\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    # 2. Fix the column names to match what the training code expects\n",
    "    # (The file has 'letter', but our code wants 'label')\n",
    "    if 'letter' in df.columns:\n",
    "        df = df.rename(columns={'letter': 'label'})\n",
    "        print(\"   -> Renamed column 'letter' to 'label'\")\n",
    "        \n",
    "    print(f\"ðŸŽ‰ SUCCESS! Loaded {len(df)} training samples.\")\n",
    "    print(\"   You can now skip to Section 5/6 to train the model!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Error: Could not find '{CSV_PATH}'\")\n",
    "    print(\"   Make sure you dragged and dropped the CSV file into the notebook folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b80dc0",
   "metadata": {},
   "source": [
    "## Section 5: Preprocess and Split Data\n",
    "\n",
    "Load the extracted keypoints, prepare features and labels, and split into training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ”„ DATA PREPROCESSING AND SPLITTING\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Dataset Statistics:\n",
      "   Total samples: 7137\n",
      "   Features per sample: 63 (Should be 63)\n",
      "   Unique classes: 31\n",
      "\n",
      "ðŸ”¤ Arabic Letters in Dataset:\n",
      "   Ain: 209 samples\n",
      "   Al: 269 samples\n",
      "   Alef: 258 samples\n",
      "   Beh: 274 samples\n",
      "   Dad: 256 samples\n",
      "   Dal: 219 samples\n",
      "   Feh: 236 samples\n",
      "   Ghain: 216 samples\n",
      "   Hah: 188 samples\n",
      "   Heh: 203 samples\n",
      "   Jeem: 192 samples\n",
      "   Kaf: 252 samples\n",
      "   Khah: 196 samples\n",
      "   Laa: 240 samples\n",
      "   Lam: 253 samples\n",
      "   Meem: 240 samples\n",
      "   Noon: 214 samples\n",
      "   Qaf: 193 samples\n",
      "   Reh: 206 samples\n",
      "   Sad: 261 samples\n",
      "   Seen: 262 samples\n",
      "   Sheen: 274 samples\n",
      "   Tah: 196 samples\n",
      "   Teh: 278 samples\n",
      "   Teh_Marbuta: 230 samples\n",
      "   Theh: 271 samples\n",
      "   Waw: 201 samples\n",
      "   Yeh: 261 samples\n",
      "   Zah: 210 samples\n",
      "   Zain: 191 samples\n",
      "   thal: 188 samples\n",
      "\n",
      "âœ‚ï¸  Splitting data (60% train, 20% val, 20% test)...\n",
      "\n",
      "ðŸ“ˆ Data Split Summary:\n",
      "   Training samples: 4567\n",
      "   Validation samples: 1142\n",
      "   Test samples: 1428\n",
      "   Total: 7137\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 5: PREPROCESS AND SPLIT DATA\n",
    "# ============================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸ”„ DATA PREPROCESSING AND SPLITTING')\n",
    "print('=' * 60)\n",
    "\n",
    "if df.empty:\n",
    "    print('\\nâŒ ERROR: No dataset loaded. Please run the extraction cell first.')\n",
    "else:\n",
    "    # --- CORRECTED SLICING FOR YOUR CSV ---\n",
    "    # Your label is in the FIRST column (index 0)\n",
    "    # Your data (x,y,z) starts from the SECOND column (index 1 to end)\n",
    "    \n",
    "    # 1. Extract Features (X): Skip the first column (label)\n",
    "    X = df.iloc[:, 1:].astype('float32').values\n",
    "    \n",
    "    # 2. Extract Labels (y): Take ONLY the first column\n",
    "    y = df.iloc[:, 0].values\n",
    "    \n",
    "    print(f'\\nðŸ“Š Dataset Statistics:')\n",
    "    print(f'   Total samples: {len(df)}')\n",
    "    print(f'   Features per sample: {X.shape[1]} (Should be 63)')\n",
    "    print(f'   Unique classes: {len(np.unique(y))}')\n",
    "    \n",
    "    # Encode labels\n",
    "    encoder = LabelEncoder()\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "    num_classes = len(encoder.classes_)\n",
    "    \n",
    "    print(f'\\nðŸ”¤ Arabic Letters in Dataset:')\n",
    "    for i, letter in enumerate(encoder.classes_):\n",
    "        count = np.sum(y_encoded == i)\n",
    "        print(f'   {letter}: {count} samples')\n",
    "    \n",
    "    # Stratified split: Train (60%), Validation (20%), Test (20%)\n",
    "    print(f'\\nâœ‚ï¸  Splitting data (60% train, 20% val, 20% test)...')\n",
    "    \n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    "    )\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    # Ensure all features are float32 for training\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_val = X_val.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    \n",
    "    print(f'\\nðŸ“ˆ Data Split Summary:')\n",
    "    print(f'   Training samples: {len(X_train)}')\n",
    "    print(f'   Validation samples: {len(X_val)}')\n",
    "    print(f'   Test samples: {len(X_test)}')\n",
    "    print(f'   Total: {len(X_train) + len(X_val) + len(X_test)}')\n",
    "    print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ae99a",
   "metadata": {},
   "source": [
    "## Section 6: Efficient tf.data Pipeline Helper\n",
    "\n",
    "Create an optimized data pipeline function for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d14b85f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tf.data pipeline helper function created\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 6: EFFICIENT TF.DATA PIPELINE\n",
    "# ============================================\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(features, labels, batch_size, training=True):\n",
    "    \"\"\"\n",
    "    Create an efficient tf.data pipeline for training or evaluation.\n",
    "    \n",
    "    Args:\n",
    "        features: Input features (numpy array)\n",
    "        labels: Target labels (numpy array)\n",
    "        batch_size: Batch size for training\n",
    "        training: Whether to shuffle data (True for training, False for validation/test)\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset: Optimized dataset pipeline\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    \n",
    "    if training:\n",
    "        # Shuffle with a reasonable buffer size for reproducibility\n",
    "        buffer = min(len(features), 10000)\n",
    "        ds = ds.shuffle(buffer_size=buffer, reshuffle_each_iteration=True)\n",
    "    \n",
    "    # Batch and prefetch for efficient GPU feeding\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "print('âœ… tf.data pipeline helper function created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d86ca",
   "metadata": {},
   "source": [
    "## Section 7: Build and Train MLP Model (GPU-Optimized)\n",
    "\n",
    "Build, compile, and train a multi-layer perceptron model optimized for GPU training with all callbacks and advanced features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05d6d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ”¨ BUILDING AND TRAINING MLP MODEL\n",
      "============================================================\n",
      "\n",
      "ðŸ“‹ Model Configuration:\n",
      "   Input shape: 63 features\n",
      "   Output classes: 31 Arabic letters\n",
      "   Device: /GPU:0\n",
      "\n",
      "ðŸ“Š Model Summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_256 (Dense)           (None, 256)               16384     \n",
      "                                                                 \n",
      " bn_1 (BatchNormalization)   (None, 256)               1024      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " bn_2 (BatchNormalization)   (None, 128)               512       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 31)                2015      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,087\n",
      "Trainable params: 60,319\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "\n",
      "âš™ï¸  Training Configuration:\n",
      "   Batch size: 256\n",
      "   Epochs: 20 (early stopping may occur earlier)\n",
      "   Optimizer: Adam (learning rate: 0.001)\n",
      "   Loss: Categorical Crossentropy\n",
      "   Metrics: Accuracy\n",
      "   Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
      "\n",
      "ðŸš€ Starting training...\n",
      "============================================================\n",
      "Epoch 1/20\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 3.7628 - accuracy: 0.0881\n",
      "Epoch 1: val_accuracy improved from -inf to 0.08757, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 6s 62ms/step - loss: 3.7068 - accuracy: 0.0981 - val_loss: 3.4414 - val_accuracy: 0.0876 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 2.8162 - accuracy: 0.2471\n",
      "Epoch 2: val_accuracy improved from 0.08757 to 0.22242, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 2.7768 - accuracy: 0.2571 - val_loss: 3.2541 - val_accuracy: 0.2224 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 2.2975 - accuracy: 0.3571\n",
      "Epoch 3: val_accuracy did not improve from 0.22242\n",
      "18/18 [==============================] - 0s 22ms/step - loss: 2.2975 - accuracy: 0.3571 - val_loss: 3.0893 - val_accuracy: 0.2075 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.8671 - accuracy: 0.4668\n",
      "Epoch 4: val_accuracy improved from 0.22242 to 0.24168, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 61ms/step - loss: 1.8671 - accuracy: 0.4668 - val_loss: 2.8901 - val_accuracy: 0.2417 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 1.5644 - accuracy: 0.5568\n",
      "Epoch 5: val_accuracy improved from 0.24168 to 0.32399, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 33ms/step - loss: 1.5532 - accuracy: 0.5588 - val_loss: 2.6694 - val_accuracy: 0.3240 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.3170 - accuracy: 0.6135\n",
      "Epoch 6: val_accuracy improved from 0.32399 to 0.41419, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 48ms/step - loss: 1.3170 - accuracy: 0.6135 - val_loss: 2.4384 - val_accuracy: 0.4142 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 1.1051 - accuracy: 0.6740\n",
      "Epoch 7: val_accuracy did not improve from 0.41419\n",
      "18/18 [==============================] - 0s 21ms/step - loss: 1.1051 - accuracy: 0.6740 - val_loss: 2.2785 - val_accuracy: 0.3967 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.9664 - accuracy: 0.7153\n",
      "Epoch 8: val_accuracy improved from 0.41419 to 0.42207, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 40ms/step - loss: 0.9664 - accuracy: 0.7153 - val_loss: 2.1158 - val_accuracy: 0.4221 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "15/18 [========================>.....] - ETA: 0s - loss: 0.8798 - accuracy: 0.7398\n",
      "Epoch 9: val_accuracy improved from 0.42207 to 0.47811, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.8771 - accuracy: 0.7397 - val_loss: 1.9057 - val_accuracy: 0.4781 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.7983 - accuracy: 0.7530\n",
      "Epoch 10: val_accuracy improved from 0.47811 to 0.51751, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.7985 - accuracy: 0.7550 - val_loss: 1.7179 - val_accuracy: 0.5175 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.7277 - accuracy: 0.7786\n",
      "Epoch 11: val_accuracy did not improve from 0.51751\n",
      "18/18 [==============================] - 0s 24ms/step - loss: 0.7277 - accuracy: 0.7786 - val_loss: 1.7305 - val_accuracy: 0.4764 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.6534 - accuracy: 0.8063\n",
      "Epoch 12: val_accuracy improved from 0.51751 to 0.56918, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.6533 - accuracy: 0.8062 - val_loss: 1.4931 - val_accuracy: 0.5692 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.6213 - accuracy: 0.8153\n",
      "Epoch 13: val_accuracy improved from 0.56918 to 0.60333, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 0.6178 - accuracy: 0.8174 - val_loss: 1.3166 - val_accuracy: 0.6033 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5802 - accuracy: 0.8362\n",
      "Epoch 14: val_accuracy improved from 0.60333 to 0.66462, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 41ms/step - loss: 0.5775 - accuracy: 0.8362 - val_loss: 1.1404 - val_accuracy: 0.6646 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5317 - accuracy: 0.8483\n",
      "Epoch 15: val_accuracy improved from 0.66462 to 0.67863, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 42ms/step - loss: 0.5317 - accuracy: 0.8483 - val_loss: 1.0598 - val_accuracy: 0.6786 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.5204 - accuracy: 0.8527\n",
      "Epoch 16: val_accuracy improved from 0.67863 to 0.70753, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 43ms/step - loss: 0.5181 - accuracy: 0.8542 - val_loss: 0.9981 - val_accuracy: 0.7075 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.5011 - accuracy: 0.8660\n",
      "Epoch 17: val_accuracy improved from 0.70753 to 0.74869, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 46ms/step - loss: 0.5011 - accuracy: 0.8660 - val_loss: 0.8818 - val_accuracy: 0.7487 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "16/18 [=========================>....] - ETA: 0s - loss: 0.4719 - accuracy: 0.8677\n",
      "Epoch 18: val_accuracy improved from 0.74869 to 0.80210, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 53ms/step - loss: 0.4675 - accuracy: 0.8706 - val_loss: 0.7348 - val_accuracy: 0.8021 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.4440 - accuracy: 0.8787\n",
      "Epoch 19: val_accuracy did not improve from 0.80210\n",
      "18/18 [==============================] - 1s 28ms/step - loss: 0.4440 - accuracy: 0.8787 - val_loss: 0.7190 - val_accuracy: 0.7942 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "17/18 [===========================>..] - ETA: 0s - loss: 0.4177 - accuracy: 0.8897\n",
      "Epoch 20: val_accuracy improved from 0.80210 to 0.83713, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "18/18 [==============================] - 1s 51ms/step - loss: 0.4160 - accuracy: 0.8894 - val_loss: 0.5833 - val_accuracy: 0.8371 - lr: 0.0010\n",
      "\n",
      "============================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "============================================================\n",
      "â±ï¸  Training time: 20.64 seconds (0.34 minutes)\n",
      "ðŸ“ Best model saved: arsl_mediapipe_mlp_model_best.h5\n",
      "ðŸ“ Final model saved: arsl_mediapipe_mlp_model_final.h5\n",
      "\n",
      "ðŸ“Š Final Training Metrics:\n",
      "   Training Accuracy: 88.94%\n",
      "   Validation Accuracy: 83.71%\n",
      "   Training Loss: 0.4160\n",
      "   Validation Loss: 0.5833\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 7: BUILD AND TRAIN MLP MODEL\n",
    "# ============================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸ”¨ BUILDING AND TRAINING MLP MODEL')\n",
    "print('=' * 60)\n",
    "\n",
    "if df.empty:\n",
    "    print('âŒ ERROR: No data available. Run preprocessing cell first.')\n",
    "else:\n",
    "    # Clear session to free GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    print(f'\\nðŸ“‹ Model Configuration:')\n",
    "    print(f'   Input shape: {X_train.shape[1]} features')\n",
    "    print(f'   Output classes: {num_classes} Arabic letters')\n",
    "    print(f'   Device: {DEVICE}')\n",
    "    \n",
    "    # Build model with GPU optimization\n",
    "    with tf.device(DEVICE):\n",
    "        model = Sequential([\n",
    "            # Input layer: 256 neurons\n",
    "            Dense(\n",
    "                256,\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                input_shape=(X_train.shape[1],),\n",
    "                name='dense_256'\n",
    "            ),\n",
    "            BatchNormalization(name='bn_1'),\n",
    "            Dropout(0.3, name='dropout_1'),\n",
    "            \n",
    "            # Hidden layer: 128 neurons\n",
    "            Dense(\n",
    "                128,\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                name='dense_128'\n",
    "            ),\n",
    "            BatchNormalization(name='bn_2'),\n",
    "            Dropout(0.25, name='dropout_2'),\n",
    "            \n",
    "            # Hidden layer: 64 neurons\n",
    "            Dense(\n",
    "                64,\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                name='dense_64'\n",
    "            ),\n",
    "            Dropout(0.2, name='dropout_3'),\n",
    "            \n",
    "            # Output layer: softmax for multi-class classification\n",
    "            Dense(num_classes, activation='softmax', dtype='float32', name='output')\n",
    "        ])\n",
    "        \n",
    "        # Use legacy Adam optimizer (works better with mixed precision)\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    # Display model summary\n",
    "    print('\\nðŸ“Š Model Summary:')\n",
    "    model.summary()\n",
    "    \n",
    "    # Prepare data pipelines\n",
    "    if USE_GPU:\n",
    "        BATCH_SIZE = 256\n",
    "    else:\n",
    "        BATCH_SIZE = 64\n",
    "    \n",
    "    train_ds = make_dataset(X_train, y_train, BATCH_SIZE, training=True)\n",
    "    val_ds = make_dataset(X_val, y_val, BATCH_SIZE, training=False)\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            'arsl_mediapipe_mlp_model_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Print training configuration\n",
    "    print(f'\\nâš™ï¸  Training Configuration:')\n",
    "    print(f'   Batch size: {BATCH_SIZE}')\n",
    "    print(f'   Epochs: 20 (early stopping may occur earlier)')\n",
    "    print(f'   Optimizer: Adam (learning rate: 0.001)')\n",
    "    print(f'   Loss: Categorical Crossentropy')\n",
    "    print(f'   Metrics: Accuracy')\n",
    "    print(f'   Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau')\n",
    "    \n",
    "    # Train model\n",
    "    print('\\nðŸš€ Starting training...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.device(DEVICE):\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=20,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Save final model\n",
    "    model.save('arsl_mediapipe_mlp_model_final.h5')\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('âœ… TRAINING COMPLETE!')\n",
    "    print('=' * 60)\n",
    "    print(f'â±ï¸  Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)')\n",
    "    print(f'ðŸ“ Best model saved: arsl_mediapipe_mlp_model_best.h5')\n",
    "    print(f'ðŸ“ Final model saved: arsl_mediapipe_mlp_model_final.h5')\n",
    "    \n",
    "    # Display final metrics\n",
    "    if hasattr(history, 'history'):\n",
    "        final_train_acc = history.history['accuracy'][-1]\n",
    "        final_val_acc = history.history['val_accuracy'][-1]\n",
    "        final_train_loss = history.history['loss'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        print(f'\\nðŸ“Š Final Training Metrics:')\n",
    "        print(f'   Training Accuracy: {final_train_acc*100:.2f}%')\n",
    "        print(f'   Validation Accuracy: {final_val_acc*100:.2f}%')\n",
    "        print(f'   Training Loss: {final_train_loss:.4f}')\n",
    "        print(f'   Validation Loss: {final_val_loss:.4f}')\n",
    "    print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b7f39",
   "metadata": {},
   "source": [
    "## Section 8: Evaluate Model on Test Data\n",
    "\n",
    "Evaluate the best trained model on the held-out test set and report performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af3f8d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ§ª MODEL EVALUATION ON TEST DATA\n",
      "============================================================\n",
      "\n",
      "ðŸ“¦ Loading best model...\n",
      "\n",
      "ðŸ” Evaluating on 1428 test samples...\n",
      "   Batch size: 256\n",
      "   Device: /GPU:0\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.6026 - accuracy: 0.8207\n",
      "\n",
      "============================================================\n",
      "âœ… EVALUATION COMPLETE!\n",
      "============================================================\n",
      "â±ï¸  Evaluation time: 0.4056 seconds\n",
      "\n",
      "ðŸ“Š Test Performance Metrics:\n",
      "   Test Loss: 0.6026\n",
      "   Test Accuracy: 82.07%\n",
      "\n",
      "   ðŸ‘ Good performance. Consider more training data or fine-tuning.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 8: EVALUATE MODEL ON TEST DATA\n",
    "# ============================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸ§ª MODEL EVALUATION ON TEST DATA')\n",
    "print('=' * 60)\n",
    "\n",
    "if df.empty:\n",
    "    print('âŒ ERROR: No data available. Run preprocessing cell first.')\n",
    "else:\n",
    "    # Load the best model\n",
    "    print('\\nðŸ“¦ Loading best model...')\n",
    "    model_best = tf.keras.models.load_model('arsl_mediapipe_mlp_model_best.h5')\n",
    "    \n",
    "    # Create test dataset pipeline\n",
    "    eval_batch_size = 256 if USE_GPU else 128\n",
    "    test_ds = make_dataset(X_test, y_test, eval_batch_size, training=False)\n",
    "    \n",
    "    print(f'\\nðŸ” Evaluating on {len(X_test)} test samples...')\n",
    "    print(f'   Batch size: {eval_batch_size}')\n",
    "    print(f'   Device: {DEVICE}')\n",
    "    \n",
    "    # Evaluate\n",
    "    start_time = time.time()\n",
    "    with tf.device(DEVICE):\n",
    "        test_loss, test_accuracy = model_best.evaluate(test_ds, verbose=1)\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('âœ… EVALUATION COMPLETE!')\n",
    "    print('=' * 60)\n",
    "    print(f'â±ï¸  Evaluation time: {eval_time:.4f} seconds')\n",
    "    print(f'\\nðŸ“Š Test Performance Metrics:')\n",
    "    print(f'   Test Loss: {test_loss:.4f}')\n",
    "    print(f'   Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "    \n",
    "    # Performance interpretation\n",
    "    if test_accuracy >= 0.95:\n",
    "        print(f'\\n   ðŸŒŸ Excellent! Model is highly accurate.')\n",
    "    elif test_accuracy >= 0.90:\n",
    "        print(f'\\n   â­ Very good performance!')\n",
    "    elif test_accuracy >= 0.80:\n",
    "        print(f'\\n   ðŸ‘ Good performance. Consider more training data or fine-tuning.')\n",
    "    else:\n",
    "        print(f'\\n   âš ï¸  May need improvement. Check data quality or increase epochs.')\n",
    "    print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0f09c",
   "metadata": {},
   "source": [
    "## Section 9: Real-Time Inference (Webcam) with Arabic Letters\n",
    "\n",
    "Deploy the trained model for real-time Arabic letter recognition using your webcam. Press 'q' to quit.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Make sure your webcam is connected and working\n",
    "2. Position your hand clearly in front of the camera\n",
    "3. The system will display the predicted letter in the top-left corner\n",
    "4. Special gestures:\n",
    "   - **SPACE**: Add a space between words\n",
    "   - **DELETE**: Remove the last letter\n",
    "   - **NOTHING**: Ignore the prediction\n",
    "\n",
    "The predicted sentence will be displayed at the bottom of the video feed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c310455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸŽ¥ REAL-TIME ARABIC LETTER RECOGNITION\n",
      "============================================================\n",
      "\n",
      "ðŸ“¦ Loading model and encoder...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'arsl_mediapipe_keypoints_final.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ“¦ Loading model and encoder...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     14\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m---> 15\u001b[0m df_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m encoder\u001b[38;5;241m.\u001b[39mfit(df_labels[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load trained model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'arsl_mediapipe_keypoints_final.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 9: REAL-TIME INFERENCE (WEBCAM)\n",
    "# ============================================\n",
    "# Commit-once-then-wait strategy\n",
    "# Control labels match CSV: 'space', 'del', 'nothing' (lowercase)\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸŽ¥ REAL-TIME ARABIC LETTER RECOGNITION')\n",
    "print('=' * 60)\n",
    "\n",
    "if df.empty:\n",
    "    print('âŒ ERROR: No data available. Run preprocessing cell first.')\n",
    "else:\n",
    "    # Load encoder\n",
    "    print('\\nðŸ“¦ Loading model and encoder...')\n",
    "    encoder = LabelEncoder()\n",
    "    df_labels = pd.read_csv(CSV_PATH)\n",
    "    # Handle both possible column names ('label' or 'letter')\n",
    "    label_col = 'label' if 'label' in df_labels.columns else 'letter'\n",
    "    encoder.fit(df_labels[label_col])\n",
    "    print(f'   Encoder classes ({len(encoder.classes_)}): {list(encoder.classes_[:5])}...')\n",
    "    \n",
    "    # Load trained model\n",
    "    tf.keras.mixed_precision.set_global_policy('float32')\n",
    "    mlp_model = tf.keras.models.load_model('arsl_mediapipe_mlp_model_best.h5')\n",
    "    print('âœ… Model and encoder loaded!')\n",
    "    \n",
    "    # Initialize MediaPipe\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    hands = mp_hands.Hands(\n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.7\n",
    "    )\n",
    "    \n",
    "    # Stabilization settings\n",
    "    STABILIZATION_WINDOW_SIZE = 10\n",
    "    STABILIZATION_THRESHOLD = 7\n",
    "    MIN_CONFIDENCE = 0.70\n",
    "    HOLD_TIME_REQUIRED = 0.8\n",
    "    DISPLAY_WIDTH = 1280\n",
    "    DISPLAY_HEIGHT = 720\n",
    "    \n",
    "    # Open webcam\n",
    "    print('\\nðŸŽ¥ Initializing webcam...')\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print('âŒ ERROR: Could not open webcam!')\n",
    "    else:\n",
    "        print('âœ… Webcam opened successfully!')\n",
    "        print('\\nðŸ“ Instructions:')\n",
    "        print('   - Position your hand in front of the camera')\n",
    "        print('   - Hold a sign steady until it commits')\n",
    "        print('   - Change sign or remove hand for next letter')\n",
    "        print('   - Press \"q\" to quit, \"c\" to clear')\n",
    "        print('\\n' + '=' * 60)\n",
    "        print('ðŸ”´ Recording... Press \"q\" to stop')\n",
    "        print('=' * 60 + '\\n')\n",
    "        \n",
    "        # State variables\n",
    "        predicted_sentence = ''\n",
    "        stabilization_buffer = deque(maxlen=STABILIZATION_WINDOW_SIZE)\n",
    "        \n",
    "        # Commit-once-then-wait state\n",
    "        committed_label = None\n",
    "        current_sign_label = None\n",
    "        current_sign_start = None\n",
    "        waiting_for_change = False\n",
    "        \n",
    "        window_name = 'Arabic Sign Language Recognition (MediaPipe + MLP)'\n",
    "        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(window_name, DISPLAY_WIDTH, DISPLAY_HEIGHT)\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Process UNFLIPPED frame with MediaPipe (matches training data)\n",
    "                frame = cv2.resize(frame, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "                h, w, c = frame.shape\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                rgb_frame.flags.writeable = False\n",
    "                results = hands.process(rgb_frame)\n",
    "                rgb_frame.flags.writeable = True\n",
    "                \n",
    "                display_status = ''\n",
    "                status_color = (200, 200, 200)\n",
    "                \n",
    "                if results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(\n",
    "                        results.multi_hand_landmarks,\n",
    "                        results.multi_handedness\n",
    "                    ):\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            frame, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "                        )\n",
    "                        \n",
    "                        # Extract landmarks â€” NO mirroring (matches training)\n",
    "                        landmarks = np.array([\n",
    "                            [lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark\n",
    "                        ])\n",
    "                        input_data = landmarks.flatten().reshape(1, -1).astype('float32')\n",
    "                        \n",
    "                        try:\n",
    "                            prediction = mlp_model.predict(input_data, verbose=0)\n",
    "                        except Exception as e:\n",
    "                            display_status = f'Prediction error: {e}'\n",
    "                            status_color = (0, 0, 255)\n",
    "                            break\n",
    "                        \n",
    "                        pred_class = np.argmax(prediction)\n",
    "                        pred_confidence = float(np.max(prediction))\n",
    "                        pred_label = encoder.inverse_transform([pred_class])[0]\n",
    "                        \n",
    "                        # Skip low confidence\n",
    "                        if pred_confidence < MIN_CONFIDENCE:\n",
    "                            display_status = f'{pred_label} ({pred_confidence:.0%}) Low conf'\n",
    "                            status_color = (0, 100, 255)\n",
    "                            break\n",
    "                        \n",
    "                        # Stability buffer\n",
    "                        stabilization_buffer.append(pred_label)\n",
    "                        buffer_count = stabilization_buffer.count(pred_label)\n",
    "                        is_stable = (buffer_count >= STABILIZATION_THRESHOLD and \n",
    "                                     len(stabilization_buffer) == STABILIZATION_WINDOW_SIZE)\n",
    "                        \n",
    "                        if not is_stable:\n",
    "                            progress = buffer_count / STABILIZATION_THRESHOLD * 100\n",
    "                            display_status = f'{pred_label} ({pred_confidence:.0%}) Stabilizing {progress:.0f}%'\n",
    "                            status_color = (0, 255, 255)\n",
    "                            break\n",
    "                        \n",
    "                        now = time.time()\n",
    "                        \n",
    "                        # Check if waiting after a commit\n",
    "                        if waiting_for_change:\n",
    "                            if pred_label == committed_label:\n",
    "                                display_status = f'{pred_label} ({pred_confidence:.0%}) Committed - change sign'\n",
    "                                status_color = (255, 200, 0)\n",
    "                                break\n",
    "                            else:\n",
    "                                waiting_for_change = False\n",
    "                                committed_label = None\n",
    "                                current_sign_label = pred_label\n",
    "                                current_sign_start = now\n",
    "                        \n",
    "                        # Track hold time\n",
    "                        if pred_label != current_sign_label:\n",
    "                            current_sign_label = pred_label\n",
    "                            current_sign_start = now\n",
    "                        \n",
    "                        hold_duration = now - current_sign_start if current_sign_start else 0\n",
    "                        \n",
    "                        if hold_duration < HOLD_TIME_REQUIRED:\n",
    "                            hold_pct = hold_duration / HOLD_TIME_REQUIRED * 100\n",
    "                            display_status = f'{pred_label} ({pred_confidence:.0%}) Hold: {hold_pct:.0f}%'\n",
    "                            status_color = (0, 255, 255)\n",
    "                            break\n",
    "                        \n",
    "                        # COMMIT â€” control labels match CSV: 'space', 'del', 'nothing'\n",
    "                        if pred_label == 'space':\n",
    "                            if predicted_sentence and predicted_sentence[-1] != ' ':\n",
    "                                predicted_sentence += ' '\n",
    "                        elif pred_label == 'del':\n",
    "                            if predicted_sentence:\n",
    "                                predicted_sentence = predicted_sentence[:-1]\n",
    "                        elif pred_label != 'nothing':\n",
    "                            predicted_sentence += pred_label\n",
    "                        \n",
    "                        committed_label = pred_label\n",
    "                        waiting_for_change = True\n",
    "                        current_sign_label = None\n",
    "                        current_sign_start = None\n",
    "                        stabilization_buffer.clear()\n",
    "                        \n",
    "                        display_status = f'{pred_label} ({pred_confidence:.0%}) COMMITTED!'\n",
    "                        status_color = (0, 255, 0)\n",
    "                else:\n",
    "                    # No hand â†’ full reset\n",
    "                    committed_label = None\n",
    "                    waiting_for_change = False\n",
    "                    current_sign_label = None\n",
    "                    current_sign_start = None\n",
    "                    stabilization_buffer.clear()\n",
    "                    display_status = 'No hand detected'\n",
    "                    status_color = (150, 150, 150)\n",
    "                \n",
    "                # Flip for selfie-view display\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Draw info panel\n",
    "                cv2.rectangle(frame, (10, 10), (500, 100), (0, 0, 0), -1)\n",
    "                cv2.putText(frame, 'Arabic Letter Recognition', (20, 35),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)\n",
    "                cv2.putText(frame, display_status, (20, 75),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)\n",
    "                \n",
    "                # Draw sentence panel at bottom\n",
    "                cv2.rectangle(frame, (0, h - 80), (w, h), (0, 0, 0), -1)\n",
    "                cv2.putText(frame, 'Predicted:', (10, h - 55),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 1)\n",
    "                cv2.putText(frame, predicted_sentence[-50:], (10, h - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)\n",
    "                \n",
    "                cv2.imshow(window_name, frame)\n",
    "                \n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('c'):\n",
    "                    predicted_sentence = ''\n",
    "                    committed_label = None\n",
    "                    waiting_for_change = False\n",
    "                    stabilization_buffer.clear()\n",
    "                    print('Sentence cleared')\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nâš ï¸ Interrupted by user')\n",
    "        except Exception as e:\n",
    "            print(f'âŒ Error: {e}')\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            print('\\n' + '=' * 60)\n",
    "            print('Session ended')\n",
    "            if predicted_sentence:\n",
    "                print(f'Final sentence: {predicted_sentence}')\n",
    "            print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd06964",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully created a complete Arabic sign language recognition system using MediaPipe and neural networks.\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "âœ… Extracted MediaPipe hand keypoints from your Arabic dataset  \n",
    "âœ… Preprocessed and split data into training, validation, and test sets  \n",
    "âœ… Built and trained a GPU-optimized MLP model  \n",
    "âœ… Evaluated model performance on test data  \n",
    "âœ… Deployed real-time inference using webcam\n",
    "\n",
    "### Generated Files:\n",
    "\n",
    "- `arsl_mediapipe_keypoints_final.csv` - Extracted keypoints dataset\n",
    "- `arsl_mediapipe_mlp_model_best.h5` - Best trained model (highest validation accuracy)\n",
    "- `arsl_mediapipe_mlp_model_final.h5` - Final model after training\n",
    "\n",
    "### Troubleshooting Tips:\n",
    "\n",
    "**If extraction is slow:**\n",
    "\n",
    "- Reduce dataset size or use a subset for testing\n",
    "- GPU will not significantly speed up image reading, only model training\n",
    "\n",
    "**If training is slow or runs out of memory:**\n",
    "\n",
    "- Reduce `BATCH_SIZE` to 128 or 64\n",
    "- Close other applications\n",
    "- Check GPU memory with `nvidia-smi -l 1`\n",
    "\n",
    "**If real-time inference is slow:**\n",
    "\n",
    "- Ensure GPU is being used (check `DEVICE` variable)\n",
    "- Reduce preprocessing in the inference loop if needed\n",
    "\n",
    "**If accuracy is low:**\n",
    "\n",
    "- Check data quality (images should be clear, well-lit)\n",
    "- Ensure hands are visible in most training images\n",
    "- Increase training data\n",
    "- Train for more epochs (remove early stopping or increase patience)\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "- Use transfer learning with pre-trained models (ResNet, MobileNet)\n",
    "- Implement sequence modeling (LSTM, Transformers) for better temporal understanding\n",
    "- Add hand gesture smoothing for more stable predictions\n",
    "- Include two-hand detection for two-handed signs\n",
    "- Build a larger, more diverse dataset\n",
    "- Deploy as a web application or mobile app\n",
    "\n",
    "**Good luck with your Arabic sign language recognition project!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
