{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "570a337e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7dec73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment set: CPU-only. Next: run the Imports cell to validate protobuf.\n"
     ]
    }
   ],
   "source": [
    "# ---- Environment setup (CPU + MediaPipe/protobuf) ----\n",
    "# Run this cell FIRST, before importing TensorFlow/MediaPipe.\n",
    "# If you install/upgrade packages (protobuf/mediapipe), restart the kernel afterwards.\n",
    "\n",
    "import os\n",
    "\n",
    "# Force CPU-only TensorFlow (reduces crashes / avoids GPU issues)\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# On some Windows setups this can avoid protobuf binary issues,\n",
    "# but it will NOT fix an incompatible protobuf version.\n",
    "os.environ.setdefault(\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\", \"python\")\n",
    "\n",
    "# Optional: quieter logs\n",
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"2\")\n",
    "\n",
    "print(\"Environment set: CPU-only. Next: run the Imports cell to validate protobuf.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a55fb57",
   "metadata": {},
   "source": [
    "# Arabic Combined Architecture (GPU Optimized)\n",
    "\n",
    "This notebook provides the **same core functionality** as the original combined notebook: real-time sign recognition + sentence building — but adapted for **Arabic (ArSL)** and improved for **GPU + camera feedback**.\n",
    "\n",
    "**What it does**\n",
    "\n",
    "- Uses **two models** in real-time: an image model (MobileNetV2) + a landmark model (MediaPipe MLP).\n",
    "- Fuses their probabilities, smooths predictions, and builds a sentence using `space`, `del`, `nothing`.\n",
    "- Shows clearer camera overlay: FPS, GPU status, stable prediction, and a **two-hands warning**.\n",
    "\n",
    "**How to run**\n",
    "\n",
    "1. Run cells in order until model loading succeeds.\n",
    "2. Run the last \"Run camera\" cell (press `q` to quit).\n",
    "\n",
    "**Optional (better Arabic text rendering)**\n",
    "\n",
    "- Install: `pip install pillow arabic-reshaper python-bidi` to display properly-shaped Arabic in the overlay. Otherwise, it falls back to OpenCV text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd5c7f5",
   "metadata": {},
   "source": [
    "## Cell: Imports\n",
    "\n",
    "Loads Python/TensorFlow/OpenCV/MediaPipe dependencies used by the rest of the notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47ba5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protobuf version: 3.19.6\n",
      "[WARN] Protobuf not compatible for MediaPipe. MediaPipe will be disabled.\n",
      "Protobuf version incompatible for MediaPipe 0.10.x. Please install protobuf>=4.25.3,<5 and restart kernel.\n",
      "[WARN] MediaPipe disabled due to protobuf version check.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import deque, Counter\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ---- Protobuf / MediaPipe compatibility preflight ----\n",
    "import google.protobuf\n",
    "from google.protobuf import message_factory as _message_factory\n",
    "\n",
    "print(f\"Protobuf version: {google.protobuf.__version__}\")\n",
    "PROTOBUF_OK = True\n",
    "MP_OK = True\n",
    "MP_REASON = None\n",
    "\n",
    "# Mediapipe 0.10.x expects protobuf >=4.25.3 and <5\n",
    "# If version is lower, warn and mark MP_OK=False so we can skip gracefully.\n",
    "def _version_tuple(v):\n",
    "    return tuple(int(x) for x in str(v).split('.') if x.isdigit())\n",
    "\n",
    "pb_ver = _version_tuple(google.protobuf.__version__)\n",
    "if pb_ver < (4, 25, 3) or pb_ver >= (5, 0, 0):\n",
    "    PROTOBUF_OK = False\n",
    "    MP_OK = False\n",
    "    MP_REASON = (\"Protobuf version incompatible for MediaPipe 0.10.x. \"\n",
    "                 \"Please install protobuf>=4.25.3,<5 and restart kernel.\")\n",
    "\n",
    "if not hasattr(_message_factory, \"GetMessageClass\") and MP_OK:\n",
    "    PROTOBUF_OK = False\n",
    "    MP_OK = False\n",
    "    MP_REASON = (\"Incompatible protobuf: message_factory.GetMessageClass missing. \"\n",
    "                 \"Install protobuf>=4.25.3,<5 and restart kernel.\")\n",
    "\n",
    "if not PROTOBUF_OK:\n",
    "    print(\"[WARN] Protobuf not compatible for MediaPipe. MediaPipe will be disabled.\")\n",
    "    print(MP_REASON)\n",
    "\n",
    "# Try to import mediapipe only if protobuf looks OK\n",
    "if MP_OK:\n",
    "    try:\n",
    "        import mediapipe as mp\n",
    "        try:\n",
    "            mp_ver = getattr(mp, \"__version__\", None)\n",
    "        except Exception:\n",
    "            mp_ver = None\n",
    "        print(\"MediaPipe imported.\", \"version=\" + str(mp_ver) if mp_ver else \"\")\n",
    "    except Exception as e:\n",
    "        MP_OK = False\n",
    "        MP_REASON = f\"Failed to import mediapipe: {e}\"\n",
    "        print('[WARN] MediaPipe import failed; disabling hand tracking.')\n",
    "else:\n",
    "    mp = None\n",
    "    print('[WARN] MediaPipe disabled due to protobuf version check.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb9cde",
   "metadata": {},
   "source": [
    "## Cell: Project paths\n",
    "\n",
    "Finds the project root and defines `ARABIC_DIR` and `GUIDE_DIR` so the notebook works no matter where you run it from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6ddfad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\n",
      "ARABIC_DIR: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ArSL Letter (Arabic) exists= True\n",
      "GUIDE_DIR: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Arabic guide exists= True\n"
     ]
    }
   ],
   "source": [
    "# ---- Locate project folders (robust to different working directories) ----\n",
    "def find_sign_to_sentence_root() -> Path:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd, *cwd.parents]:\n",
    "        if p.name == \"Sign_to_Sentence Project Main\":\n",
    "            return p\n",
    "        if (p / \"Sign_to_Sentence Project Main\").exists():\n",
    "            return (p / \"Sign_to_Sentence Project Main\").resolve()\n",
    "    return cwd\n",
    "\n",
    "ROOT = find_sign_to_sentence_root()\n",
    "ARABIC_DIR = ROOT / \"ArSL Letter (Arabic)\"\n",
    "GUIDE_DIR = ROOT / \"Arabic guide\"\n",
    "\n",
    "print('ROOT:', ROOT)\n",
    "print('ARABIC_DIR:', ARABIC_DIR, 'exists=', ARABIC_DIR.exists())\n",
    "print('GUIDE_DIR:', GUIDE_DIR, 'exists=', GUIDE_DIR.exists())\n",
    "\n",
    "if not ARABIC_DIR.exists():\n",
    "    raise FileNotFoundError(f'Arabic folder not found at: {ARABIC_DIR}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915cd01",
   "metadata": {},
   "source": [
    "## Cell: GPU / performance setup\n",
    "\n",
    "Enables safe GPU options (memory-growth, optional XLA, optional mixed precision). If you see instability, set mixed precision off in that cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58acd04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs visible to TensorFlow: []\n",
      "Running in CPU-only mode.\n",
      "TensorFlow threads limited (intra/inter = 1).\n"
     ]
    }
   ],
   "source": [
    "# ---- CPU-only / performance setup ----\n",
    "# This notebook is configured for CPU stability + low CPU usage.\n",
    "\n",
    "# Confirm TensorFlow sees no GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('GPUs visible to TensorFlow:', gpus)\n",
    "print('Running in CPU-only mode.')\n",
    "\n",
    "# Reduce CPU usage by limiting TensorFlow thread pools (tune as needed)\n",
    "try:\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    print('TensorFlow threads limited (intra/inter = 1).')\n",
    "except Exception as e:\n",
    "    print('Could not set TF threading options:', e)\n",
    "\n",
    "USE_MIXED_PRECISION = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2344e7",
   "metadata": {},
   "source": [
    "## Cell: Arabic class labels\n",
    "\n",
    "Loads the Arabic class list used to interpret model outputs and to build the final sentence. This must match how your Arabic models were trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75e8740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic Sign Language Classes:\n",
      "  - Letters: 28\n",
      "  - Total classes: 31\n",
      "  - Classes: ['ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'space', 'del', 'nothing']\n",
      "Loaded ARABIC_CLASSES from arabic_class_labels.py: 31\n",
      "ARABIC_CLASSES: ['ا', 'ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق', 'ك', 'ل', 'م', 'ن', 'ه', 'و', 'ي', 'space', 'del', 'nothing']\n"
     ]
    }
   ],
   "source": [
    "# ---- Arabic class labels (prefer the repo's guide if available) ----\n",
    "ARABIC_CLASSES = None\n",
    "try:\n",
    "    import importlib.util\n",
    "    labels_path = GUIDE_DIR / 'arabic_class_labels.py'\n",
    "    spec = importlib.util.spec_from_file_location('arabic_class_labels', str(labels_path))\n",
    "    mod = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(mod)\n",
    "    ARABIC_CLASSES = list(mod.ARABIC_CLASSES)\n",
    "    print('Loaded ARABIC_CLASSES from arabic_class_labels.py:', len(ARABIC_CLASSES))\n",
    "except Exception as e:\n",
    "    print('Falling back to inline ARABIC_CLASSES (could not import guide):', e)\n",
    "    ARABIC_CLASSES = [\n",
    "        'ا','ب','ت','ث','ج','ح','خ','د','ذ','ر',\n",
    "        'ز','س','ش','ص','ض','ط','ظ','ع','غ','ف',\n",
    "        'ق','ك','ل','م','ن','ه','و','ي',\n",
    "        'space','del','nothing'\n",
    "    ]\n",
    "\n",
    "ARABIC_CLASSES = list(ARABIC_CLASSES)\n",
    "CLASS_TO_INDEX = {c: i for i, c in enumerate(ARABIC_CLASSES)}\n",
    "print('ARABIC_CLASSES:', ARABIC_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472dc93",
   "metadata": {},
   "source": [
    "## Cell: Load models + align labels\n",
    "\n",
    "Loads Arabic MobileNet + Arabic MediaPipe-MLP and builds a mapping so the MLP output indices match the `ARABIC_CLASSES` order used for fusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339dcf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNet: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ArSL Letter (Arabic)\\Final Notebooks\\mobilenet_arabic_final.h5 exists= True\n",
      "MLP: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ArSL Letter (Arabic)\\Final Notebooks\\arsl_mediapipe_mlp_model_final.h5 exists= True\n",
      "CSV: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ArSL Letter (Arabic)\\Final Notebooks\\FINAL_CLEAN_DATASET.csv exists= True\n",
      "MobileNet output classes: 35\n",
      "MLP output classes: 34\n",
      "CSV unique labels: 34\n",
      "MobileNet disabled for correctness/CPU: CSV labels=34 but MobileNet outputs=35.\n",
      "Fusion label count: 34\n"
     ]
    }
   ],
   "source": [
    "# ---- Load models (CPU) + labels from a single CSV ----\n",
    "# Requirement: use ONLY these three paths (no auto-search).\n",
    "# - MobileNet model\n",
    "# - MLP model\n",
    "# - Keypoints CSV for label order\n",
    "\n",
    "# IMPORTANT: For correctness, we do NOT create/pad \"dummy\" labels.\n",
    "# If MobileNet outputs a different number of classes than the CSV, MobileNet will be disabled automatically.\n",
    "\n",
    "FINAL_DIR = ARABIC_DIR / 'Final Notebooks'\n",
    "\n",
    "# 1) EXACT paths (edit here only if you move files)\n",
    "MOBILENET_PATH = FINAL_DIR / 'mobilenet_arabic_final.h5'\n",
    "MLP_PATH = FINAL_DIR / 'arsl_mediapipe_mlp_model_final.h5'\n",
    "KEYPOINTS_CSV = FINAL_DIR / 'FINAL_CLEAN_DATASET.csv'\n",
    "\n",
    "print('MobileNet:', MOBILENET_PATH, 'exists=', MOBILENET_PATH.exists())\n",
    "print('MLP:', MLP_PATH, 'exists=', MLP_PATH.exists())\n",
    "print('CSV:', KEYPOINTS_CSV, 'exists=', KEYPOINTS_CSV.exists())\n",
    "\n",
    "if not MOBILENET_PATH.exists():\n",
    "    raise FileNotFoundError(f'MobileNet missing: {MOBILENET_PATH}')\n",
    "if not MLP_PATH.exists():\n",
    "    raise FileNotFoundError(f'MLP missing: {MLP_PATH}')\n",
    "if not KEYPOINTS_CSV.exists():\n",
    "    raise FileNotFoundError(f'CSV missing: {KEYPOINTS_CSV}')\n",
    "\n",
    "# FIX: Force float32 policy BEFORE loading models.\n",
    "# The MLP model was trained with mixed_float16, which crashes on CPU-only systems.\n",
    "tf.keras.mixed_precision.set_global_policy('float32')\n",
    "\n",
    "# 2) Load models (inference only)\n",
    "mobilenet_model = tf.keras.models.load_model(str(MOBILENET_PATH), compile=False)\n",
    "mlp_model_raw = tf.keras.models.load_model(str(MLP_PATH), compile=False)\n",
    "\n",
    "# FIX: Convert MLP from mixed_float16 to float32 for CPU stability.\n",
    "# Clone the model architecture under float32 policy, then copy weights.\n",
    "try:\n",
    "    mlp_model = tf.keras.models.clone_model(mlp_model_raw)\n",
    "    mlp_model.set_weights(mlp_model_raw.get_weights())\n",
    "    del mlp_model_raw\n",
    "    print('✅ MLP model converted from mixed_float16 → float32 for CPU stability')\n",
    "except Exception as e:\n",
    "    print(f'⚠️  Could not clone model to float32, using original: {e}')\n",
    "    mlp_model = mlp_model_raw\n",
    "\n",
    "mn_dim = int(mobilenet_model.output_shape[-1])\n",
    "mlp_dim = int(mlp_model.output_shape[-1])\n",
    "print('MobileNet output classes:', mn_dim)\n",
    "print('MLP output classes:', mlp_dim)\n",
    "\n",
    "# 3) Labels from the CSV (single source of truth for label order)\n",
    "labels_df = pd.read_csv(str(KEYPOINTS_CSV), usecols=['label'])\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(labels_df['label'])\n",
    "CSV_LABELS = list(encoder.classes_)\n",
    "print('CSV unique labels:', len(CSV_LABELS))\n",
    "\n",
    "# MLP label order must match MLP outputs\n",
    "if len(CSV_LABELS) != mlp_dim:\n",
    "    raise RuntimeError(\n",
    "        f'CSV labels ({len(CSV_LABELS)}) != MLP output classes ({mlp_dim}). '\n",
    "        'This means the CSV is not the one used to train the MLP.'\n",
    "    )\n",
    "MLP_LABELS = list(CSV_LABELS)\n",
    "\n",
    "# MobileNet: only use if it matches CSV label count\n",
    "USE_MOBILENET = (len(CSV_LABELS) == mn_dim)\n",
    "if not USE_MOBILENET:\n",
    "    print(\n",
    "        'MobileNet disabled for correctness/CPU: '\n",
    "        f'CSV labels={len(CSV_LABELS)} but MobileNet outputs={mn_dim}.'\n",
    "    )\n",
    "    MN_LABELS = []\n",
    "else:\n",
    "    MN_LABELS = list(CSV_LABELS)\n",
    "\n",
    "# 4) Fusion label space\n",
    "FUSION_LABELS = list(dict.fromkeys((MN_LABELS if USE_MOBILENET else []) + MLP_LABELS))\n",
    "FUSION_INDEX = {lab: i for i, lab in enumerate(FUSION_LABELS)}\n",
    "print('Fusion label count:', len(FUSION_LABELS))\n",
    "\n",
    "# 5) Transliteration -> Arabic character mapping for display/sentence (optional)\n",
    "NAME_TO_ARABIC = {\n",
    "    'Alef': 'ا', 'Beh': 'ب', 'Teh': 'ت', 'Theh': 'ث', 'Jeem': 'ج', 'Hah': 'ح', 'Khah': 'خ',\n",
    "    'Dal': 'د', 'Thal': 'ذ', 'thal': 'ذ', 'Reh': 'ر', 'Zain': 'ز', 'Seen': 'س', 'Sheen': 'ش', 'Sad': 'ص',\n",
    "    'Dad': 'ض', 'Tah': 'ط', 'Zah': 'ظ', 'Ain': 'ع', 'Ghain': 'غ', 'Feh': 'ف', 'Qaf': 'ق',\n",
    "    'Kaf': 'ك', 'Lam': 'ل', 'Meem': 'م', 'Noon': 'ن', 'Heh': 'ه', 'Waw': 'و', 'Yeh': 'ي',\n",
    "    'space': 'space', 'del': 'del', 'nothing': 'nothing',\n",
    "    'Teh_Marbuta': None, 'Al': None, 'Laa': None,\n",
    "}\n",
    "\n",
    "def to_display_label(label: str) -> str:\n",
    "    mapped = NAME_TO_ARABIC.get(label, None)\n",
    "    if mapped is None:\n",
    "        return str(label)\n",
    "    return str(mapped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82803b4",
   "metadata": {},
   "source": [
    "## Cell: Report (what is loaded)\n",
    "\n",
    "This section prints a quick **verification report** showing:\n",
    "\n",
    "- Which MobileNet/MLP model files were loaded\n",
    "- Output class counts\n",
    "- Which CSV files were used to reconstruct the **exact label order** used in training\n",
    "- A small label distribution preview (first few rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a542b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- FILES ---\n",
      "MobileNet file: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ArSL Letter (Arabic)\\Final Notebooks\\mobilenet_arabic_final.h5\n",
      "MLP file: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ArSL Letter (Arabic)\\Final Notebooks\\arsl_mediapipe_mlp_model_final.h5\n",
      "CSV: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ArSL Letter (Arabic)\\Final Notebooks\\FINAL_CLEAN_DATASET.csv\n",
      "\n",
      "--- MODEL OUTPUT DIMS ---\n",
      "MobileNet classes: 35\n",
      "MLP classes: 34\n",
      "CSV label count: 34\n",
      "\n",
      "--- MODE SWITCHES ---\n",
      "USE_MOBILENET: False\n",
      "\n",
      "--- LABEL SAMPLES ---\n",
      "MLP_LABELS sample: ['Ain', 'Al', 'Alef', 'Beh', 'Dad', 'Dal', 'Feh', 'Ghain', 'Hah', 'Heh']\n",
      "FUSION_LABELS sample: ['Ain', 'Al', 'Alef', 'Beh', 'Dad', 'Dal', 'Feh', 'Ghain', 'Hah', 'Heh']\n",
      "\n",
      "--- LABEL DISTRIBUTION (top 10) ---\n",
      "label\n",
      "del        300\n",
      "nothing    300\n",
      "space      300\n",
      "Teh        278\n",
      "Sheen      274\n",
      "Beh        274\n",
      "Theh       271\n",
      "Al         269\n",
      "Seen       262\n",
      "Sad        261\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- MODEL PARAMS ---\n",
      "MLP params: 185826\n"
     ]
    }
   ],
   "source": [
    "# ---- Verification report (CPU) ----\n",
    "print('--- FILES ---')\n",
    "print('MobileNet file:', MOBILENET_PATH)\n",
    "print('MLP file:', MLP_PATH)\n",
    "print('CSV:', KEYPOINTS_CSV)\n",
    "\n",
    "print('\\n--- MODEL OUTPUT DIMS ---')\n",
    "print('MobileNet classes:', mn_dim)\n",
    "print('MLP classes:', mlp_dim)\n",
    "print('CSV label count:', len(CSV_LABELS))\n",
    "\n",
    "print('\\n--- MODE SWITCHES ---')\n",
    "print('USE_MOBILENET:', USE_MOBILENET)\n",
    "\n",
    "print('\\n--- LABEL SAMPLES ---')\n",
    "print('MLP_LABELS sample:', MLP_LABELS[:10])\n",
    "print('FUSION_LABELS sample:', FUSION_LABELS[:10])\n",
    "\n",
    "print('\\n--- LABEL DISTRIBUTION (top 10) ---')\n",
    "try:\n",
    "    print(labels_df['label'].value_counts().head(10))\n",
    "except Exception as e:\n",
    "    print('Could not compute label counts:', e)\n",
    "\n",
    "print('\\n--- MODEL PARAMS ---')\n",
    "print('MLP params:', mlp_model.count_params())\n",
    "if USE_MOBILENET:\n",
    "    print('MobileNet params:', mobilenet_model.count_params())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af8198",
   "metadata": {},
   "source": [
    "## Cell: MediaPipe hand tracking helpers\n",
    "\n",
    "Initializes MediaPipe Hands and defines helper functions for landmark extraction and bounding-box cropping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49373b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Protobuf version incompatible for MediaPipe 0.10.x. Please install protobuf>=4.25.3,<5 and restart kernel.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MP_OK:\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(MP_REASON \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMediaPipe not available; fix protobuf and restart kernel.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n\u001b[0;32m      5\u001b[0m mp_drawing \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mdrawing_utils\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Protobuf version incompatible for MediaPipe 0.10.x. Please install protobuf>=4.25.3,<5 and restart kernel."
     ]
    }
   ],
   "source": [
    "if not MP_OK:\n",
    "    raise RuntimeError(MP_REASON or 'MediaPipe not available; fix protobuf and restart kernel.')\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# CPU-friendly settings: model_complexity=0 is fastest.\n",
    "# Keep max_num_hands=2 so we can warn when two hands appear.\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6,\n",
    "    max_num_hands=2,\n",
    " )\n",
    "\n",
    "def extract_landmark_features(hand_landmarks, handedness=None):\n",
    "    \"\"\"Flatten 21 hand landmarks into shape (1, 63).\n",
    "    FIX: No mirroring — training data was extracted without any mirroring,\n",
    "    so inference must match exactly.\"\"\"\n",
    "    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark], dtype=np.float32)\n",
    "    # NOTE: Removed right-hand mirroring. Training data did NOT mirror landmarks,\n",
    "    # so mirroring at inference time causes wrong predictions.\n",
    "    return landmarks.flatten()[None, :]\n",
    "\n",
    "def landmarks_bbox_px(hand_landmarks, frame_shape, pad_w=150, pad_h=220):\n",
    "    h, w = frame_shape[:2]\n",
    "    xs = [lm.x for lm in hand_landmarks.landmark]\n",
    "    ys = [lm.y for lm in hand_landmarks.landmark]\n",
    "    x_min = max(0, int(min(xs) * w - pad_w))\n",
    "    y_min = max(0, int(min(ys) * h - pad_h))\n",
    "    x_max = min(w, int(max(xs) * w + pad_w))\n",
    "    y_max = min(h, int(max(ys) * h + pad_h))\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "def softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    ex = np.exp(x)\n",
    "    return ex / (np.sum(ex) + 1e-9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468591dd",
   "metadata": {},
   "source": [
    "## Optional: Better Arabic text rendering (recommended)\n",
    "\n",
    "OpenCV’s built-in fonts don’t shape Arabic well. This block enables a Pillow-based overlay when available.\n",
    "If these packages are missing, the notebook will fall back to `cv2.putText`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bf9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARABIC_TEXT_OK = False\n",
    "try:\n",
    "    from PIL import Image, ImageDraw, ImageFont\n",
    "    import arabic_reshaper\n",
    "    from bidi.algorithm import get_display\n",
    "    ARABIC_TEXT_OK = True\n",
    "except Exception:\n",
    "    ARABIC_TEXT_OK = False\n",
    "\n",
    "def put_text(frame_bgr, text, org, color=(255,255,255), font_scale=1.0, thickness=2):\n",
    "    \"\"\"Draw text. Uses Arabic shaping via PIL if available, else falls back to cv2.putText.\"\"\"\n",
    "    x, y = org\n",
    "    if not ARABIC_TEXT_OK:\n",
    "        cv2.putText(frame_bgr, str(text), (x, y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, thickness, cv2.LINE_AA)\n",
    "        return frame_bgr\n",
    "\n",
    "    # PIL path (supports Arabic shaping)\n",
    "    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(rgb)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    shaped = get_display(arabic_reshaper.reshape(str(text)))\n",
    "\n",
    "    # Try a common Windows Arabic-capable font; fallback to default if missing\n",
    "    font = None\n",
    "    for fp in [\n",
    "        r'C:\\\\Windows\\\\Fonts\\\\arial.ttf',\n",
    "        r'C:\\\\Windows\\\\Fonts\\\\tahoma.ttf'\n",
    "    ]:\n",
    "        if Path(fp).exists():\n",
    "            try:\n",
    "                font = ImageFont.truetype(fp, int(24 * font_scale))\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if font is None:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    draw.text((x, y), shaped, fill=(color[2], color[1], color[0]), font=font)\n",
    "    out = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
    "    return out\n",
    "\n",
    "print('Arabic PIL overlay enabled:', ARABIC_TEXT_OK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88903fe8",
   "metadata": {},
   "source": [
    "## Cell: Camera config + fusion helpers\n",
    "\n",
    "Defines the fusion/smoothing/stabilization settings and helper functions used by the camera loop. Tune these first if the camera feels too slow or too sensitive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2250a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CPU-friendly camera knobs ----\n",
    "CAMERA_INDEX = 0\n",
    "\n",
    "# Lower resolution reduces CPU usage a lot\n",
    "FRAME_W, FRAME_H = 640, 480\n",
    "\n",
    "# FIX: MobileNet model expects (96, 96, 3) input — was incorrectly set to (128, 128)\n",
    "MOBILENET_INPUT = (96, 96)\n",
    "\n",
    "# Smaller padding = smaller crop\n",
    "PAD_W, PAD_H = 80, 120\n",
    "\n",
    "# Run inference less often to reduce CPU load (still stable due to smoothing)\n",
    "PROCESS_EVERY_N_FRAMES = 2  # 1 = every frame (highest CPU), 2 = half CPU\n",
    "RUN_MOBILENET_EVERY_N = 4   # only used when USE_MOBILENET=True\n",
    "\n",
    "EMA_ALPHA = 0.8\n",
    "STABLE_WINDOW = 7\n",
    "STABLE_MIN_COUNT = 5\n",
    "CONF_THRESHOLD = 0.55\n",
    "\n",
    "# ---- Commit-once-then-wait strategy ----\n",
    "# After committing a letter, the system LOCKS that label.\n",
    "# It won't accept the same letter again until:\n",
    "#   a) The hand leaves the frame, OR\n",
    "#   b) A genuinely DIFFERENT sign becomes stable.\n",
    "# This prevents \"mmmmmooooccc\" repetition.\n",
    "HOLD_TIME_REQUIRED = 0.8  # seconds to hold a sign before committing\n",
    "\n",
    "# ---- State ----\n",
    "predicted_sentence = ''\n",
    "committed_label = None       # The label we already committed (locked)\n",
    "waiting_for_change = False   # True after committing → blocking re-commit\n",
    "current_sign_label = None    # The current sign being tracked for hold time\n",
    "current_sign_start = None    # When the current sign was first seen\n",
    "label_history = deque(maxlen=STABLE_WINDOW)\n",
    "fps_times = deque(maxlen=30)\n",
    "ema_probs = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1980a69",
   "metadata": {},
   "source": [
    "## Cell: Run camera (combined fusion)\n",
    "\n",
    "Starts the real-time camera loop. Shows FPS, stable prediction, and builds the sentence. Press `q` to quit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffbea31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Run camera (CPU, stable, low usage)\n",
    "# ========================================================\n",
    "# Notes:\n",
    "# - Uses ThreadedCamera to reduce lag.\n",
    "# - Runs MediaPipe/MLP every N frames to reduce CPU.\n",
    "# - MobileNet is optional and disabled automatically if label counts mismatch.\n",
    "# - FIX: Frame is NOT flipped before MediaPipe (matches training data).\n",
    "#   Flip is applied AFTER processing for selfie-view display only.\n",
    "# - Uses commit-once-then-wait: each sign commits ONCE, then waits\n",
    "#   for hand-drop or a different sign before committing again.\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "class ThreadedCamera:\n",
    "    def __init__(self, src=0, width=640, height=480):\n",
    "        backend = cv2.CAP_DSHOW if os.name == 'nt' else 0\n",
    "        self.capture = cv2.VideoCapture(src, backend)\n",
    "        if not self.capture.isOpened():\n",
    "            self.capture = cv2.VideoCapture(src)\n",
    "        try:\n",
    "            self.capture.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*'MJPG'))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            self.capture.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.capture.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "        self.capture.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "        self.status, self.frame = self.capture.read()\n",
    "        self.stopped = False\n",
    "\n",
    "    def start(self):\n",
    "        Thread(target=self.update, args=(), daemon=True).start()\n",
    "        return self\n",
    "\n",
    "    def update(self):\n",
    "        while True:\n",
    "            if self.stopped:\n",
    "                return\n",
    "            status, frame = self.capture.read()\n",
    "            if status and frame is not None:\n",
    "                self.status, self.frame = status, frame\n",
    "\n",
    "    def read(self):\n",
    "        return self.status, self.frame\n",
    "\n",
    "    def release(self):\n",
    "        self.stopped = True\n",
    "        try:\n",
    "            self.capture.release()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def _as_probs(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x).astype(np.float32).reshape(-1)\n",
    "    if np.any(x < 0) or abs(float(np.sum(x)) - 1.0) > 0.05:\n",
    "        x = softmax(x)\n",
    "    return x\n",
    "\n",
    "# Fusion mapping (MLP always available; MobileNet optional)\n",
    "FUSION_CLASS_TO_INDEX = {c: i for i, c in enumerate(FUSION_LABELS)}\n",
    "MLP_TO_FUSION = np.asarray([FUSION_CLASS_TO_INDEX.get(c, -1) for c in MLP_LABELS], dtype=np.int32)\n",
    "MN_TO_FUSION = np.asarray([FUSION_CLASS_TO_INDEX.get(c, -1) for c in MN_LABELS], dtype=np.int32) if USE_MOBILENET else None\n",
    "\n",
    "def align_probs_to_fusion(probs_1d: np.ndarray, index_map: np.ndarray) -> np.ndarray:\n",
    "    out = np.zeros((len(FUSION_LABELS),), dtype=np.float32)\n",
    "    if probs_1d is None or index_map is None:\n",
    "        return out\n",
    "    n = min(int(probs_1d.shape[0]), int(index_map.shape[0]))\n",
    "    for src_i in range(n):\n",
    "        dst_i = int(index_map[src_i])\n",
    "        if dst_i >= 0:\n",
    "            out[dst_i] = float(probs_1d[src_i])\n",
    "    s = float(out.sum())\n",
    "    if s > 0:\n",
    "        out /= s\n",
    "    return out\n",
    "\n",
    "def fuse_probs(mn_probs_fusion: np.ndarray, mlp_probs_fusion: np.ndarray) -> np.ndarray:\n",
    "    if mn_probs_fusion is None or float(np.sum(mn_probs_fusion)) <= 0:\n",
    "        return mlp_probs_fusion.copy()\n",
    "    mn_max = float(np.max(mn_probs_fusion))\n",
    "    mlp_max = float(np.max(mlp_probs_fusion))\n",
    "    w_mn, w_mlp = mn_max, mlp_max\n",
    "    denom = w_mn + w_mlp\n",
    "    if denom <= 1e-9:\n",
    "        return mlp_probs_fusion.copy()\n",
    "    return (w_mn * mn_probs_fusion + w_mlp * mlp_probs_fusion) / denom\n",
    "\n",
    "def compute_fps():\n",
    "    if len(fps_times) < 2:\n",
    "        return 0.0\n",
    "    dt = fps_times[-1] - fps_times[0]\n",
    "    if dt <= 1e-9:\n",
    "        return 0.0\n",
    "    return (len(fps_times) - 1) / dt\n",
    "\n",
    "gpu_status = 'No (CPU mode)'\n",
    "cap = ThreadedCamera(CAMERA_INDEX, width=FRAME_W, height=FRAME_H).start()\n",
    "time.sleep(0.4)\n",
    "if not cap.status or cap.frame is None:\n",
    "    raise RuntimeError('Camera could not start. Try changing CAMERA_INDEX.')\n",
    "print('Camera opened. Press q to quit, c to clear sentence.')\n",
    "\n",
    "frame_i = 0\n",
    "last_mlp_probs = np.zeros((len(FUSION_LABELS),), dtype=np.float32)\n",
    "last_mn_probs = np.zeros((len(FUSION_LABELS),), dtype=np.float32)\n",
    "last_two_hands = False\n",
    "last_bbox = None\n",
    "status_text = ''\n",
    "status_color = (200, 200, 200)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret or frame is None:\n",
    "        continue\n",
    "\n",
    "    # FIX: Do NOT flip the frame before MediaPipe processing.\n",
    "    fps_times.append(time.time())\n",
    "    frame_i += 1\n",
    "\n",
    "    do_process = (frame_i % PROCESS_EVERY_N_FRAMES == 0)\n",
    "\n",
    "    stable_label = None\n",
    "    stable_conf = 0.0\n",
    "    two_hands = False\n",
    "\n",
    "    if do_process:\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb)\n",
    "        two_hands = bool(results.multi_hand_landmarks and len(results.multi_hand_landmarks) > 1)\n",
    "        last_two_hands = two_hands\n",
    "\n",
    "        if results.multi_hand_landmarks and not two_hands:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            try:\n",
    "                feats = extract_landmark_features(hand_landmarks)\n",
    "                mlp_raw = _as_probs(mlp_model.predict(feats, verbose=0)[0])\n",
    "                last_mlp_probs = align_probs_to_fusion(mlp_raw, MLP_TO_FUSION)\n",
    "            except Exception as e:\n",
    "                print(f'⚠️  MLP predict error: {e}')\n",
    "                last_mlp_probs[:] = 0.0\n",
    "\n",
    "            if USE_MOBILENET and (frame_i % RUN_MOBILENET_EVERY_N == 0):\n",
    "                try:\n",
    "                    x_min, y_min, x_max, y_max = landmarks_bbox_px(hand_landmarks, frame.shape, pad_w=PAD_W, pad_h=PAD_H)\n",
    "                    crop = frame[y_min:y_max, x_min:x_max]\n",
    "                    last_bbox = (x_min, y_min, x_max, y_max)\n",
    "                    if crop.shape[0] > 0 and crop.shape[1] > 0:\n",
    "                        resized = cv2.resize(crop, MOBILENET_INPUT)\n",
    "                        inp = (resized.astype(np.float32) / 255.0)[None, ...]\n",
    "                        mn_raw = _as_probs(mobilenet_model.predict(inp, verbose=0)[0])\n",
    "                        last_mn_probs = align_probs_to_fusion(mn_raw, MN_TO_FUSION)\n",
    "                except Exception as e:\n",
    "                    print(f'⚠️  MobileNet predict error: {e}')\n",
    "                    last_mn_probs[:] = 0.0\n",
    "            else:\n",
    "                last_mn_probs[:] = 0.0\n",
    "\n",
    "            # Fuse + smooth\n",
    "            fused = fuse_probs(last_mn_probs if USE_MOBILENET else None, last_mlp_probs)\n",
    "            if ema_probs is None:\n",
    "                ema_probs = fused\n",
    "            else:\n",
    "                ema_probs = (EMA_ALPHA * ema_probs + (1.0 - EMA_ALPHA) * fused).astype(np.float32)\n",
    "                s = float(ema_probs.sum())\n",
    "                if s > 0:\n",
    "                    ema_probs /= s\n",
    "\n",
    "            idx = int(np.argmax(ema_probs))\n",
    "            label = FUSION_LABELS[idx]\n",
    "            conf = float(ema_probs[idx])\n",
    "\n",
    "            label_history.append(label)\n",
    "            if len(label_history) == STABLE_WINDOW:\n",
    "                most, cnt = Counter(label_history).most_common(1)[0]\n",
    "                if cnt >= STABLE_MIN_COUNT:\n",
    "                    stable_label = most\n",
    "                    stable_idx = int(FUSION_INDEX.get(most, idx))\n",
    "                    stable_conf = float(ema_probs[stable_idx])\n",
    "\n",
    "            # --- COMMIT-ONCE-THEN-WAIT LOGIC ---\n",
    "            now = time.time()\n",
    "            if stable_label and stable_conf >= CONF_THRESHOLD:\n",
    "                # If waiting after a commit, check whether sign changed\n",
    "                if waiting_for_change:\n",
    "                    if stable_label == committed_label:\n",
    "                        # Same sign still held — keep waiting\n",
    "                        status_text = f'{to_display_label(stable_label)} ({stable_conf:.2f}) Committed - change sign'\n",
    "                        status_color = (255, 200, 0)\n",
    "                    else:\n",
    "                        # Different sign! Unlock\n",
    "                        waiting_for_change = False\n",
    "                        committed_label = None\n",
    "                        current_sign_label = stable_label\n",
    "                        current_sign_start = now\n",
    "                        status_text = f'{to_display_label(stable_label)} ({stable_conf:.2f}) New sign detected'\n",
    "                        status_color = (0, 255, 255)\n",
    "\n",
    "                if not waiting_for_change:\n",
    "                    # Track hold time for current sign\n",
    "                    if stable_label != current_sign_label:\n",
    "                        current_sign_label = stable_label\n",
    "                        current_sign_start = now\n",
    "\n",
    "                    hold_duration = now - current_sign_start if current_sign_start else 0\n",
    "\n",
    "                    if hold_duration < HOLD_TIME_REQUIRED:\n",
    "                        hold_pct = hold_duration / HOLD_TIME_REQUIRED * 100\n",
    "                        status_text = f'{to_display_label(stable_label)} ({stable_conf:.2f}) Hold: {hold_pct:.0f}%'\n",
    "                        status_color = (0, 255, 255)\n",
    "                    else:\n",
    "                        # COMMIT\n",
    "                        commit = NAME_TO_ARABIC.get(stable_label, stable_label)\n",
    "                        if commit is None:\n",
    "                            pass\n",
    "                        elif commit not in ['nothing', 'del', 'space']:\n",
    "                            predicted_sentence += str(commit)\n",
    "                        elif commit == 'space':\n",
    "                            predicted_sentence += ' '\n",
    "                        elif commit == 'del':\n",
    "                            predicted_sentence = predicted_sentence[:-1]\n",
    "                        \n",
    "                        committed_label = stable_label\n",
    "                        waiting_for_change = True\n",
    "                        current_sign_label = None\n",
    "                        current_sign_start = None\n",
    "                        label_history.clear()\n",
    "                        ema_probs = None\n",
    "                        \n",
    "                        status_text = f'{to_display_label(stable_label)} ({stable_conf:.2f}) COMMITTED!'\n",
    "                        status_color = (0, 255, 0)\n",
    "            elif stable_label:\n",
    "                status_text = f'{to_display_label(stable_label)} ({stable_conf:.2f}) Low conf'\n",
    "                status_color = (0, 100, 255)\n",
    "            else:\n",
    "                status_text = 'Stabilizing...'\n",
    "                status_color = (200, 200, 200)\n",
    "\n",
    "        elif not (results.multi_hand_landmarks and two_hands):\n",
    "            # No hand detected → FULL RESET (allows re-doing same letter)\n",
    "            committed_label = None\n",
    "            waiting_for_change = False\n",
    "            current_sign_label = None\n",
    "            current_sign_start = None\n",
    "            label_history.clear()\n",
    "            ema_probs = None\n",
    "            status_text = 'No hand'\n",
    "            status_color = (150, 150, 150)\n",
    "\n",
    "    # FIX: Flip the frame AFTER MediaPipe processing for selfie-view display\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Draw bbox if we have one\n",
    "    if last_bbox is not None:\n",
    "        x_min, y_min, x_max, y_max = last_bbox\n",
    "        h_frame, w_frame = frame.shape[:2]\n",
    "        flipped_x_min = w_frame - x_max\n",
    "        flipped_x_max = w_frame - x_min\n",
    "        cv2.rectangle(frame, (flipped_x_min, y_min), (flipped_x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "    # UI\n",
    "    fps = compute_fps()\n",
    "    hud_y = 28\n",
    "    frame = put_text(frame, f'FPS: {fps:.1f}', (12, hud_y), color=(255,255,255), font_scale=0.8, thickness=2)\n",
    "    hud_y += 26\n",
    "    frame = put_text(frame, f'Mode: CPU', (12, hud_y), color=(255,255,255), font_scale=0.8, thickness=2)\n",
    "    hud_y += 26\n",
    "    if last_two_hands:\n",
    "        frame = put_text(frame, 'Only one hand allowed', (12, hud_y), color=(0,0,255), font_scale=0.9, thickness=3)\n",
    "    else:\n",
    "        frame = put_text(frame, status_text, (12, hud_y), color=status_color, font_scale=0.9, thickness=3)\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    bar_h = 60\n",
    "    cv2.rectangle(frame, (0, h - bar_h), (w, h), (0, 0, 0), -1)\n",
    "    frame = put_text(frame, predicted_sentence[-50:], (12, h - 22), color=(255,255,255), font_scale=1.0, thickness=2)\n",
    "\n",
    "    cv2.imshow('Arabic Sign Recognition (CPU)', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('c'):\n",
    "        predicted_sentence = ''\n",
    "        committed_label = None\n",
    "        waiting_for_change = False\n",
    "        current_sign_label = None\n",
    "        current_sign_start = None\n",
    "        label_history.clear()\n",
    "        ema_probs = None\n",
    "        print('Sentence cleared')\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
