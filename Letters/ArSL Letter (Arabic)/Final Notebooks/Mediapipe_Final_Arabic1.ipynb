{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef53c74d",
   "metadata": {},
   "source": [
    "# Final MediaPipe Notebook for Arabic Sign Language Letter Recognition\n",
    "\n",
    "This comprehensive notebook combines the best features from all previous notebooks for training and deploying a MediaPipe-based Arabic sign language recognition system.\n",
    "\n",
    "## Key Features:\n",
    "\n",
    "- **GPU Optimization**: Automatic GPU detection and configuration\n",
    "- **Mixed Precision Training**: Faster training on supported GPUs\n",
    "- **Data Extraction**: MediaPipe keypoint extraction from your Arabic dataset\n",
    "- **Model Training**: GPU-optimized MLP model with early stopping and learning rate scheduling\n",
    "- **Real-Time Inference**: Webcam-based live prediction with Arabic letter recognition\n",
    "- **Memory Management**: Efficient data pipelines and memory-conscious batch sizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a5891a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.13) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "âœ… All libraries imported successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 1: Import Required Libraries\n",
    "# ============================================\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "import mediapipe as mp\n",
    "\n",
    "print('=' * 60)\n",
    "print('âœ… All libraries imported successfully!')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104300b5",
   "metadata": {},
   "source": [
    "## Section 2: GPU Detection and Configuration\n",
    "\n",
    "This section automatically detects your GPU, configures TensorFlow for optimal GPU usage, and enables mixed precision training if supported.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6206cc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ” GPU DETECTION & CONFIGURATION\n",
      "============================================================\n",
      "\n",
      "TensorFlow version: 2.10.0\n",
      "Found GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "âœ… GPU configured: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  NVIDIA GeForce MX150, compute capability 6.1\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "âš¡ Mixed precision enabled: mixed_float16\n",
      "\n",
      "âœ… Configuration complete. Using device: /GPU:0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 2: GPU DETECTION & CONFIGURATION\n",
    "# ============================================\n",
    "print('=' * 60)\n",
    "print('ðŸ” GPU DETECTION & CONFIGURATION')\n",
    "print('=' * 60)\n",
    "print(f'\\nTensorFlow version: {tf.__version__}')\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'Found GPUs: {gpus}')\n",
    "\n",
    "USE_GPU = False\n",
    "DEVICE = '/CPU:0'\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        USE_GPU = True\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(f'âœ… GPU configured: {gpus[0]}')\n",
    "    except RuntimeError as e:\n",
    "        print(f'âš ï¸  GPU config error: {e}')\n",
    "\n",
    "# Mixed precision (optional and beneficial on modern GPUs)\n",
    "try:\n",
    "    if USE_GPU:\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print(f'âš¡ Mixed precision enabled: {policy.name}')\n",
    "except Exception as e:\n",
    "    print(f'âš ï¸  Mixed precision not enabled: {e}')\n",
    "\n",
    "print(f'\\nâœ… Configuration complete. Using device: {DEVICE}')\n",
    "print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b921bf60",
   "metadata": {},
   "source": [
    "## Section 3: Batch Size and Memory Tips\n",
    "\n",
    "**Important Memory Management:**\n",
    "\n",
    "- If you get 'Out of Memory' errors, reduce `BATCH_SIZE` (try 128 or 64)\n",
    "- Or disable mixed precision by setting the policy to 'float32'\n",
    "- Monitor GPU usage with `nvidia-smi -l 1` in a separate PowerShell terminal\n",
    "- Close other GPU-intensive applications (browser, video software, etc.) during training\n",
    "- MLP models are memory-efficient - batch size 256 typically requires ~1.5-2.5 GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9164fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 4: EXTRACT KEYPOINTS FROM IMAGES\n",
    "# ============================================\n",
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm # Progress bar\n",
    "\n",
    "# âš ï¸ 1. UPDATE THIS PATH TO YOUR IMAGE FOLDER\n",
    "# It should be the folder containing subfolders like 'Ain', 'Alif', 'Baa'...\n",
    "DATASET_DIR = r\"M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database\\ArASL_Database\"\n",
    "\n",
    "# 2. Output File Name\n",
    "CSV_PATH = 'my_custom_arabic_keypoints.csv'\n",
    "\n",
    "# Setup MediaPipe\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "print(f\"ðŸš€ Starting Extraction from: {DATASET_DIR}\")\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "classes = sorted(os.listdir(DATASET_DIR)) # Get folder names (labels)\n",
    "\n",
    "if not classes:\n",
    "    print(\"âŒ ERROR: No folders found! Check your path.\")\n",
    "else:\n",
    "    print(f\"ðŸ“‚ Found {len(classes)} classes: {classes[:5]}...\")\n",
    "\n",
    "    # Loop through every folder (Class)\n",
    "    for folder_name in tqdm(classes, desc=\"Processing Classes\"):\n",
    "        folder_path = os.path.join(DATASET_DIR, folder_name)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            # Loop through every image in the folder\n",
    "            for img_name in os.listdir(folder_path):\n",
    "                img_path = os.path.join(folder_path, img_name)\n",
    "                \n",
    "                # Check file type\n",
    "                if not img_name.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # 1. Read Image\n",
    "                    # (Safe read for Windows paths with special chars)\n",
    "                    stream = open(img_path, \"rb\")\n",
    "                    bytes = bytearray(stream.read())\n",
    "                    numpyarray = np.asarray(bytes, dtype=np.uint8)\n",
    "                    img = cv2.imdecode(numpyarray, cv2.IMREAD_UNCHANGED)\n",
    "                    \n",
    "                    if img is None: continue\n",
    "                    \n",
    "                    # 2. Convert to RGB\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # 3. Extract Landmarks\n",
    "                    results = hands.process(img_rgb)\n",
    "                    \n",
    "                    if results.multi_hand_landmarks:\n",
    "                        for hand_landmarks in results.multi_hand_landmarks:\n",
    "                            row = []\n",
    "                            # Extract 21 points (x, y, z)\n",
    "                            for landmark in hand_landmarks.landmark:\n",
    "                                row.extend([landmark.x, landmark.y, landmark.z])\n",
    "                            \n",
    "                            data.append(row)\n",
    "                            labels.append(folder_name)\n",
    "                            \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "    # --- SAVE TO CSV ---\n",
    "    if len(data) > 0:\n",
    "        # Create column names\n",
    "        columns = []\n",
    "        for i in range(21):\n",
    "            columns.extend([f'x{i}', f'y{i}', f'z{i}'])\n",
    "        \n",
    "        df = pd.DataFrame(data, columns=columns)\n",
    "        df['label'] = labels # Add label column\n",
    "        \n",
    "        # Save it\n",
    "        df.to_csv(CSV_PATH, index=False)\n",
    "        print(f\"\\nâœ… SUCCESS! Extracted {len(df)} samples.\")\n",
    "        print(f\"ðŸ’¾ Saved to: {CSV_PATH}\")\n",
    "        print(\"ðŸ‘‰ You can now run Section 5 to train.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ FAILED: No hands were detected in any images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e4fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ Debugging Image: M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database\\ArASL_Database\\ain\\AIN (1).JPG\n",
      "\n",
      "Testing: Confidence=0.5, StaticMode=True...\n",
      "âŒ FAILED. No hand seen.\n",
      "\n",
      "Testing: Confidence=0.3, StaticMode=True...\n",
      "âŒ FAILED. No hand seen.\n",
      "\n",
      "Testing: Confidence=0.5, StaticMode=False...\n",
      "âŒ FAILED. No hand seen.\n",
      "\n",
      "ðŸš¨ CRITICAL FAILURE: MediaPipe cannot see the hand in this image at all.\n",
      "   1. Are the images blank?\n",
      "   2. Is the hand cut off (no wrist)?\n",
      "   3. Please open the image manually to check it.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Paste your path here again\n",
    "DATASET_DIR = r\"M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database\\ArASL_Database\"\n",
    "\n",
    "# We will pick the first letter folder we find (e.g., 'ain')\n",
    "target_class = os.listdir(DATASET_DIR)[0]\n",
    "class_path = os.path.join(DATASET_DIR, target_class)\n",
    "image_name = os.listdir(class_path)[0]\n",
    "img_path = os.path.join(class_path, image_name)\n",
    "\n",
    "print(f\"ðŸ§ Debugging Image: {img_path}\")\n",
    "\n",
    "# --- SETUP MEDIAPIPE ---\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_draw = mp.solutions.drawing_utils\n",
    "\n",
    "def try_detect(confidence, static_mode):\n",
    "    print(f\"\\nTesting: Confidence={confidence}, StaticMode={static_mode}...\")\n",
    "    \n",
    "    with mp_hands.Hands(\n",
    "        static_image_mode=static_mode,\n",
    "        max_num_hands=1,\n",
    "        min_detection_confidence=confidence\n",
    "    ) as hands:\n",
    "        \n",
    "        # 1. Read Image\n",
    "        # Use binary read to handle Windows path issues\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            bytes = bytearray(f.read())\n",
    "            numpyarray = np.asarray(bytes, dtype=np.uint8)\n",
    "            img = cv2.imdecode(numpyarray, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        if img is None:\n",
    "            print(\"âŒ Error: Could not read image file.\")\n",
    "            return False\n",
    "\n",
    "        # 2. Convert to RGB\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # 3. Process\n",
    "        results = hands.process(img_rgb)\n",
    "        \n",
    "        if results.multi_hand_landmarks:\n",
    "            print(\"âœ… SUCCESS! Hand Detected.\")\n",
    "            # Draw landmarks to prove it\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_draw.draw_landmarks(img, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            \n",
    "            # Show the success\n",
    "            cv2.imshow(f\"Success! Conf={confidence}\", img)\n",
    "            cv2.waitKey(0)\n",
    "            cv2.destroyAllWindows()\n",
    "            return True\n",
    "        else:\n",
    "            print(\"âŒ FAILED. No hand seen.\")\n",
    "            return False\n",
    "\n",
    "# --- RUN TESTS ---\n",
    "# Test 1: Default Settings\n",
    "if try_detect(confidence=0.5, static_mode=True):\n",
    "    print(\">> Recommendation: Your images are fine. Maybe the loop logic was wrong.\")\n",
    "    \n",
    "# Test 2: Lower Confidence (For bad lighting)\n",
    "elif try_detect(confidence=0.3, static_mode=True):\n",
    "    print(\">> Recommendation: Change 'min_detection_confidence' to 0.3 in your script.\")\n",
    "\n",
    "# Test 3: Video Mode (Sometimes works better for blurry pics)\n",
    "elif try_detect(confidence=0.5, static_mode=False):\n",
    "    print(\">> Recommendation: Change 'static_image_mode' to False in your script.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nðŸš¨ CRITICAL FAILURE: MediaPipe cannot see the hand in this image at all.\")\n",
    "    print(\"   1. Are the images blank?\")\n",
    "    print(\"   2. Is the hand cut off (no wrist)?\")\n",
    "    print(\"   3. Please open the image manually to check it.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Found Ready-Made CSV: FINAL_CLEAN_DATASET.csv\n",
      "ðŸŽ‰ SUCCESS! Loaded 8037 training samples.\n",
      "   You can now skip to Section 5/6 to train the model!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# REPLACEMENT FOR SECTION 4: LOAD CSV DIRECTLY\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Point to the file you uploaded\n",
    "CSV_PATH = r'FINAL_CLEAN_DATASET.csv'\n",
    "\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print(f\"âœ… Found Ready-Made CSV: {CSV_PATH}\")\n",
    "    \n",
    "    # Load it\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    \n",
    "    # 2. Fix the column names to match what the training code expects\n",
    "    # (The file has 'letter', but our code wants 'label')\n",
    "    if 'letter' in df.columns:\n",
    "        df = df.rename(columns={'letter': 'label'})\n",
    "        print(\"   -> Renamed column 'letter' to 'label'\")\n",
    "        \n",
    "    print(f\"ðŸŽ‰ SUCCESS! Loaded {len(df)} training samples.\")\n",
    "    print(\"   You can now skip to Section 5/6 to train the model!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âŒ Error: Could not find '{CSV_PATH}'\")\n",
    "    print(\"   Make sure you dragged and dropped the CSV file into the notebook folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b80dc0",
   "metadata": {},
   "source": [
    "## Section 5: Preprocess and Split Data\n",
    "\n",
    "Load the extracted keypoints, prepare features and labels, and split into training, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfb467e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ”„ DATA PREPROCESSING AND SPLITTING\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Dataset Statistics:\n",
      "   Total samples: 8037\n",
      "   Features per sample: 63 (Should be 63)\n",
      "   Unique classes: 34\n",
      "\n",
      "ðŸ”¤ Arabic Letters in Dataset:\n",
      "   Ain: 209 samples\n",
      "   Al: 269 samples\n",
      "   Alef: 258 samples\n",
      "   Beh: 274 samples\n",
      "   Dad: 256 samples\n",
      "   Dal: 219 samples\n",
      "   Feh: 236 samples\n",
      "   Ghain: 216 samples\n",
      "   Hah: 188 samples\n",
      "   Heh: 203 samples\n",
      "   Jeem: 192 samples\n",
      "   Kaf: 252 samples\n",
      "   Khah: 196 samples\n",
      "   Laa: 240 samples\n",
      "   Lam: 253 samples\n",
      "   Meem: 240 samples\n",
      "   Noon: 214 samples\n",
      "   Qaf: 193 samples\n",
      "   Reh: 206 samples\n",
      "   Sad: 261 samples\n",
      "   Seen: 262 samples\n",
      "   Sheen: 274 samples\n",
      "   Tah: 196 samples\n",
      "   Teh: 278 samples\n",
      "   Teh_Marbuta: 230 samples\n",
      "   Theh: 271 samples\n",
      "   Waw: 201 samples\n",
      "   Yeh: 261 samples\n",
      "   Zah: 210 samples\n",
      "   Zain: 191 samples\n",
      "   del: 300 samples\n",
      "   nothing: 300 samples\n",
      "   space: 300 samples\n",
      "   thal: 188 samples\n",
      "\n",
      "âœ‚ï¸  Splitting data (60% train, 20% val, 20% test)...\n",
      "\n",
      "ðŸ“ˆ Data Split Summary:\n",
      "   Training samples: 5143\n",
      "   Validation samples: 1286\n",
      "   Test samples: 1608\n",
      "   Total: 8037\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 5: PREPROCESS AND SPLIT DATA\n",
    "# ============================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸ”„ DATA PREPROCESSING AND SPLITTING')\n",
    "print('=' * 60)\n",
    "\n",
    "if df.empty:\n",
    "    print('\\nâŒ ERROR: No dataset loaded. Please run the extraction cell first.')\n",
    "else:\n",
    "    # --- CORRECTED SLICING FOR YOUR CSV ---\n",
    "    # Your label is in the FIRST column (index 0)\n",
    "    # Your data (x,y,z) starts from the SECOND column (index 1 to end)\n",
    "    \n",
    "    # 1. Extract Features (X): Skip the first column (label)\n",
    "    X = df.iloc[:, 1:].astype('float32').values\n",
    "    \n",
    "    # 2. Extract Labels (y): Take ONLY the first column\n",
    "    y = df.iloc[:, 0].values\n",
    "    \n",
    "    print(f'\\nðŸ“Š Dataset Statistics:')\n",
    "    print(f'   Total samples: {len(df)}')\n",
    "    print(f'   Features per sample: {X.shape[1]} (Should be 63)')\n",
    "    print(f'   Unique classes: {len(np.unique(y))}')\n",
    "    \n",
    "    # Encode labels\n",
    "    encoder = LabelEncoder()\n",
    "    y_encoded = encoder.fit_transform(y)\n",
    "    num_classes = len(encoder.classes_)\n",
    "    \n",
    "    print(f'\\nðŸ”¤ Arabic Letters in Dataset:')\n",
    "    for i, letter in enumerate(encoder.classes_):\n",
    "        count = np.sum(y_encoded == i)\n",
    "        print(f'   {letter}: {count} samples')\n",
    "    \n",
    "    # Stratified split: Train (60%), Validation (20%), Test (20%)\n",
    "    print(f'\\nâœ‚ï¸  Splitting data (60% train, 20% val, 20% test)...')\n",
    "    \n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    "    )\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    \n",
    "    # Ensure all features are float32 for training\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_val = X_val.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    \n",
    "    print(f'\\nðŸ“ˆ Data Split Summary:')\n",
    "    print(f'   Training samples: {len(X_train)}')\n",
    "    print(f'   Validation samples: {len(X_val)}')\n",
    "    print(f'   Test samples: {len(X_test)}')\n",
    "    print(f'   Total: {len(X_train) + len(X_val) + len(X_test)}')\n",
    "    print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636ae99a",
   "metadata": {},
   "source": [
    "## Section 6: Efficient tf.data Pipeline Helper\n",
    "\n",
    "Create an optimized data pipeline function for training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d14b85f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… tf.data pipeline helper function created\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 6: EFFICIENT TF.DATA PIPELINE\n",
    "# ============================================\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(features, labels, batch_size, training=True):\n",
    "    \"\"\"\n",
    "    Create an efficient tf.data pipeline for training or evaluation.\n",
    "    \n",
    "    Args:\n",
    "        features: Input features (numpy array)\n",
    "        labels: Target labels (numpy array)\n",
    "        batch_size: Batch size for training\n",
    "        training: Whether to shuffle data (True for training, False for validation/test)\n",
    "    \n",
    "    Returns:\n",
    "        tf.data.Dataset: Optimized dataset pipeline\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    \n",
    "    if training:\n",
    "        # Shuffle with a reasonable buffer size for reproducibility\n",
    "        buffer = min(len(features), 10000)\n",
    "        ds = ds.shuffle(buffer_size=buffer, reshuffle_each_iteration=True)\n",
    "    \n",
    "    # Batch and prefetch for efficient GPU feeding\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "print('âœ… tf.data pipeline helper function created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147d86ca",
   "metadata": {},
   "source": [
    "## Section 7: Build and Train MLP Model (GPU-Optimized)\n",
    "\n",
    "Build, compile, and train a multi-layer perceptron model optimized for GPU training with all callbacks and advanced features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05d6d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ”¨ BUILDING AND TRAINING MLP MODEL\n",
      "============================================================\n",
      "\n",
      "ðŸ“‹ Model Configuration:\n",
      "   Input shape: 63 features\n",
      "   Output classes: 34 Arabic letters\n",
      "   Device: /GPU:0\n",
      "\n",
      "ðŸ“Š Model Summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_512 (Dense)           (None, 512)               32768     \n",
      "                                                                 \n",
      " bn_1 (BatchNormalization)   (None, 512)               2048      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_256 (Dense)           (None, 256)               131328    \n",
      "                                                                 \n",
      " bn_2 (BatchNormalization)   (None, 256)               1024      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 34)                2210      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 185,826\n",
      "Trainable params: 184,290\n",
      "Non-trainable params: 1,536\n",
      "_________________________________________________________________\n",
      "\n",
      "âš™ï¸  Training Configuration:\n",
      "   Batch size: 256\n",
      "   Epochs: 20 (early stopping may occur earlier)\n",
      "   Optimizer: Adam (learning rate: 0.001)\n",
      "   Loss: Categorical Crossentropy\n",
      "   Metrics: Accuracy\n",
      "   Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
      "\n",
      "ðŸš€ Starting training...\n",
      "============================================================\n",
      "Epoch 1/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 3.5802 - accuracy: 0.1375\n",
      "Epoch 1: val_accuracy improved from -inf to 0.14308, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 5s 54ms/step - loss: 3.5423 - accuracy: 0.1435 - val_loss: 3.5192 - val_accuracy: 0.1431 - lr: 5.0000e-04\n",
      "Epoch 2/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 2.4511 - accuracy: 0.3678\n",
      "Epoch 2: val_accuracy improved from 0.14308 to 0.17885, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 46ms/step - loss: 2.4235 - accuracy: 0.3755 - val_loss: 3.3487 - val_accuracy: 0.1788 - lr: 5.0000e-04\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 1.9034 - accuracy: 0.5182\n",
      "Epoch 3: val_accuracy improved from 0.17885 to 0.31415, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 1.9034 - accuracy: 0.5182 - val_loss: 3.1299 - val_accuracy: 0.3142 - lr: 5.0000e-04\n",
      "Epoch 4/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 1.5537 - accuracy: 0.6002\n",
      "Epoch 4: val_accuracy improved from 0.31415 to 0.45645, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 1.5535 - accuracy: 0.6004 - val_loss: 2.8886 - val_accuracy: 0.4565 - lr: 5.0000e-04\n",
      "Epoch 5/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 1.2825 - accuracy: 0.6793\n",
      "Epoch 5: val_accuracy improved from 0.45645 to 0.50622, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 1.2766 - accuracy: 0.6800 - val_loss: 2.6626 - val_accuracy: 0.5062 - lr: 5.0000e-04\n",
      "Epoch 6/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 1.0982 - accuracy: 0.7303\n",
      "Epoch 6: val_accuracy improved from 0.50622 to 0.57154, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 1.0925 - accuracy: 0.7295 - val_loss: 2.4013 - val_accuracy: 0.5715 - lr: 5.0000e-04\n",
      "Epoch 7/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.9617 - accuracy: 0.7586\n",
      "Epoch 7: val_accuracy improved from 0.57154 to 0.59642, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.9627 - accuracy: 0.7579 - val_loss: 2.1924 - val_accuracy: 0.5964 - lr: 5.0000e-04\n",
      "Epoch 8/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.8745 - accuracy: 0.7934\n",
      "Epoch 8: val_accuracy improved from 0.59642 to 0.60498, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 0.8713 - accuracy: 0.7937 - val_loss: 1.9939 - val_accuracy: 0.6050 - lr: 5.0000e-04\n",
      "Epoch 9/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.7673 - accuracy: 0.8098\n",
      "Epoch 9: val_accuracy improved from 0.60498 to 0.69907, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.7675 - accuracy: 0.8096 - val_loss: 1.7373 - val_accuracy: 0.6991 - lr: 5.0000e-04\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.7183 - accuracy: 0.8254\n",
      "Epoch 10: val_accuracy improved from 0.69907 to 0.71229, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 38ms/step - loss: 0.7183 - accuracy: 0.8254 - val_loss: 1.5667 - val_accuracy: 0.7123 - lr: 5.0000e-04\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6386 - accuracy: 0.8557\n",
      "Epoch 11: val_accuracy improved from 0.71229 to 0.74961, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 36ms/step - loss: 0.6386 - accuracy: 0.8557 - val_loss: 1.3695 - val_accuracy: 0.7496 - lr: 5.0000e-04\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5950 - accuracy: 0.8680\n",
      "Epoch 12: val_accuracy improved from 0.74961 to 0.80949, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 44ms/step - loss: 0.5950 - accuracy: 0.8680 - val_loss: 1.1922 - val_accuracy: 0.8095 - lr: 5.0000e-04\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.5614 - accuracy: 0.8717\n",
      "Epoch 13: val_accuracy improved from 0.80949 to 0.82737, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 34ms/step - loss: 0.5614 - accuracy: 0.8717 - val_loss: 1.0630 - val_accuracy: 0.8274 - lr: 5.0000e-04\n",
      "Epoch 14/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.5476 - accuracy: 0.8762\n",
      "Epoch 14: val_accuracy improved from 0.82737 to 0.84448, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.5476 - accuracy: 0.8771 - val_loss: 0.9397 - val_accuracy: 0.8445 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.5250 - accuracy: 0.8826\n",
      "Epoch 15: val_accuracy improved from 0.84448 to 0.87792, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 33ms/step - loss: 0.5250 - accuracy: 0.8826 - val_loss: 0.7893 - val_accuracy: 0.8779 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.4984 - accuracy: 0.8936\n",
      "Epoch 16: val_accuracy improved from 0.87792 to 0.88958, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.4984 - accuracy: 0.8936 - val_loss: 0.7028 - val_accuracy: 0.8896 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4663 - accuracy: 0.8964\n",
      "Epoch 17: val_accuracy did not improve from 0.88958\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.4674 - accuracy: 0.8958 - val_loss: 0.6336 - val_accuracy: 0.8880 - lr: 5.0000e-04\n",
      "Epoch 18/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4558 - accuracy: 0.9043\n",
      "Epoch 18: val_accuracy improved from 0.88958 to 0.91135, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 0.4569 - accuracy: 0.9038 - val_loss: 0.5653 - val_accuracy: 0.9114 - lr: 5.0000e-04\n",
      "Epoch 19/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4341 - accuracy: 0.9070\n",
      "Epoch 19: val_accuracy improved from 0.91135 to 0.91835, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 32ms/step - loss: 0.4355 - accuracy: 0.9065 - val_loss: 0.5167 - val_accuracy: 0.9184 - lr: 5.0000e-04\n",
      "Epoch 20/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4143 - accuracy: 0.9186\n",
      "Epoch 20: val_accuracy improved from 0.91835 to 0.93390, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 35ms/step - loss: 0.4170 - accuracy: 0.9178 - val_loss: 0.4716 - val_accuracy: 0.9339 - lr: 5.0000e-04\n",
      "Epoch 21/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.4177 - accuracy: 0.9132\n",
      "Epoch 21: val_accuracy improved from 0.93390 to 0.93779, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 37ms/step - loss: 0.4186 - accuracy: 0.9123 - val_loss: 0.4330 - val_accuracy: 0.9378 - lr: 5.0000e-04\n",
      "Epoch 22/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.4012 - accuracy: 0.9178\n",
      "Epoch 22: val_accuracy did not improve from 0.93779\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.4010 - accuracy: 0.9179 - val_loss: 0.3938 - val_accuracy: 0.9378 - lr: 5.0000e-04\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3783 - accuracy: 0.9267\n",
      "Epoch 23: val_accuracy improved from 0.93779 to 0.94479, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.3783 - accuracy: 0.9267 - val_loss: 0.3694 - val_accuracy: 0.9448 - lr: 5.0000e-04\n",
      "Epoch 24/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3814 - accuracy: 0.9235\n",
      "Epoch 24: val_accuracy improved from 0.94479 to 0.95490, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.3809 - accuracy: 0.9236 - val_loss: 0.3508 - val_accuracy: 0.9549 - lr: 5.0000e-04\n",
      "Epoch 25/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3625 - accuracy: 0.9310\n",
      "Epoch 25: val_accuracy did not improve from 0.95490\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.3645 - accuracy: 0.9298 - val_loss: 0.3437 - val_accuracy: 0.9518 - lr: 5.0000e-04\n",
      "Epoch 26/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3701 - accuracy: 0.9250\n",
      "Epoch 26: val_accuracy did not improve from 0.95490\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.3679 - accuracy: 0.9255 - val_loss: 0.3478 - val_accuracy: 0.9440 - lr: 5.0000e-04\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3496 - accuracy: 0.9362\n",
      "Epoch 27: val_accuracy did not improve from 0.95490\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.3496 - accuracy: 0.9362 - val_loss: 0.3310 - val_accuracy: 0.9518 - lr: 5.0000e-04\n",
      "Epoch 28/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3388 - accuracy: 0.9385\n",
      "Epoch 28: val_accuracy did not improve from 0.95490\n",
      "21/21 [==============================] - 1s 23ms/step - loss: 0.3381 - accuracy: 0.9388 - val_loss: 0.3285 - val_accuracy: 0.9495 - lr: 5.0000e-04\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3474 - accuracy: 0.9366\n",
      "Epoch 29: val_accuracy did not improve from 0.95490\n",
      "21/21 [==============================] - 1s 29ms/step - loss: 0.3474 - accuracy: 0.9366 - val_loss: 0.3166 - val_accuracy: 0.9549 - lr: 5.0000e-04\n",
      "Epoch 30/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3285 - accuracy: 0.9406\n",
      "Epoch 30: val_accuracy did not improve from 0.95490\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.3269 - accuracy: 0.9421 - val_loss: 0.3173 - val_accuracy: 0.9533 - lr: 5.0000e-04\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.9417\n",
      "Epoch 31: val_accuracy did not improve from 0.95490\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.3239 - accuracy: 0.9417 - val_loss: 0.3134 - val_accuracy: 0.9526 - lr: 5.0000e-04\n",
      "Epoch 32/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3097 - accuracy: 0.9457\n",
      "Epoch 32: val_accuracy did not improve from 0.95490\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.3119 - accuracy: 0.9446 - val_loss: 0.3053 - val_accuracy: 0.9510 - lr: 5.0000e-04\n",
      "Epoch 33/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.3012 - accuracy: 0.9465\n",
      "Epoch 33: val_accuracy improved from 0.95490 to 0.95568, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.3018 - accuracy: 0.9461 - val_loss: 0.2994 - val_accuracy: 0.9557 - lr: 5.0000e-04\n",
      "Epoch 34/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.3015 - accuracy: 0.9465\n",
      "Epoch 34: val_accuracy did not improve from 0.95568\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.3024 - accuracy: 0.9467 - val_loss: 0.2994 - val_accuracy: 0.9526 - lr: 5.0000e-04\n",
      "Epoch 35/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.3024 - accuracy: 0.9514\n",
      "Epoch 35: val_accuracy did not improve from 0.95568\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.3038 - accuracy: 0.9504 - val_loss: 0.3162 - val_accuracy: 0.9463 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2844 - accuracy: 0.9505\n",
      "Epoch 36: val_accuracy did not improve from 0.95568\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.2849 - accuracy: 0.9496 - val_loss: 0.3180 - val_accuracy: 0.9502 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2911 - accuracy: 0.9523\n",
      "Epoch 37: val_accuracy did not improve from 0.95568\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.2918 - accuracy: 0.9512 - val_loss: 0.3205 - val_accuracy: 0.9463 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2850 - accuracy: 0.9531\n",
      "Epoch 38: val_accuracy improved from 0.95568 to 0.95801, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 47ms/step - loss: 0.2850 - accuracy: 0.9529 - val_loss: 0.3012 - val_accuracy: 0.9580 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2740 - accuracy: 0.9566\n",
      "Epoch 39: val_accuracy did not improve from 0.95801\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.2745 - accuracy: 0.9563 - val_loss: 0.3115 - val_accuracy: 0.9533 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2778 - accuracy: 0.9556\n",
      "Epoch 40: val_accuracy did not improve from 0.95801\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2758 - accuracy: 0.9559 - val_loss: 0.2985 - val_accuracy: 0.9565 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2708 - accuracy: 0.9579\n",
      "Epoch 41: val_accuracy improved from 0.95801 to 0.95956, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.2709 - accuracy: 0.9586 - val_loss: 0.2896 - val_accuracy: 0.9596 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2622 - accuracy: 0.9609\n",
      "Epoch 42: val_accuracy improved from 0.95956 to 0.96112, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.2623 - accuracy: 0.9607 - val_loss: 0.2866 - val_accuracy: 0.9611 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2730 - accuracy: 0.9572\n",
      "Epoch 43: val_accuracy did not improve from 0.96112\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.2708 - accuracy: 0.9572 - val_loss: 0.3029 - val_accuracy: 0.9580 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2618 - accuracy: 0.9595\n",
      "Epoch 44: val_accuracy did not improve from 0.96112\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.2631 - accuracy: 0.9592 - val_loss: 0.3015 - val_accuracy: 0.9572 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2646 - accuracy: 0.9545\n",
      "Epoch 45: val_accuracy did not improve from 0.96112\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.2646 - accuracy: 0.9545 - val_loss: 0.3171 - val_accuracy: 0.9526 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2589 - accuracy: 0.9614\n",
      "Epoch 46: val_accuracy improved from 0.96112 to 0.96345, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.2594 - accuracy: 0.9611 - val_loss: 0.2855 - val_accuracy: 0.9635 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2635 - accuracy: 0.9605\n",
      "Epoch 47: val_accuracy did not improve from 0.96345\n",
      "21/21 [==============================] - 0s 19ms/step - loss: 0.2651 - accuracy: 0.9596 - val_loss: 0.3094 - val_accuracy: 0.9541 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.9551\n",
      "Epoch 48: val_accuracy did not improve from 0.96345\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2641 - accuracy: 0.9551 - val_loss: 0.3180 - val_accuracy: 0.9510 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2512 - accuracy: 0.9601\n",
      "Epoch 49: val_accuracy did not improve from 0.96345\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2537 - accuracy: 0.9592 - val_loss: 0.2925 - val_accuracy: 0.9635 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2534 - accuracy: 0.9626\n",
      "Epoch 50: val_accuracy improved from 0.96345 to 0.96423, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 0s 23ms/step - loss: 0.2528 - accuracy: 0.9625 - val_loss: 0.2937 - val_accuracy: 0.9642 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2487 - accuracy: 0.9614\n",
      "Epoch 51: val_accuracy improved from 0.96423 to 0.96501, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 42ms/step - loss: 0.2503 - accuracy: 0.9611 - val_loss: 0.2960 - val_accuracy: 0.9650 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2495 - accuracy: 0.9645\n",
      "Epoch 52: val_accuracy did not improve from 0.96501\n",
      "21/21 [==============================] - 0s 18ms/step - loss: 0.2501 - accuracy: 0.9642 - val_loss: 0.2923 - val_accuracy: 0.9619 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2358 - accuracy: 0.9660\n",
      "Epoch 53: val_accuracy did not improve from 0.96501\n",
      "21/21 [==============================] - 1s 25ms/step - loss: 0.2358 - accuracy: 0.9660 - val_loss: 0.2860 - val_accuracy: 0.9650 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2395 - accuracy: 0.9638\n",
      "Epoch 54: val_accuracy improved from 0.96501 to 0.96890, saving model to arsl_mediapipe_mlp_model_best.h5\n",
      "21/21 [==============================] - 1s 28ms/step - loss: 0.2395 - accuracy: 0.9638 - val_loss: 0.2775 - val_accuracy: 0.9689 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2419 - accuracy: 0.9635\n",
      "Epoch 55: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.2433 - accuracy: 0.9636 - val_loss: 0.2884 - val_accuracy: 0.9588 - lr: 5.0000e-04\n",
      "Epoch 56/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2339 - accuracy: 0.9673\n",
      "Epoch 56: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.2347 - accuracy: 0.9669 - val_loss: 0.2820 - val_accuracy: 0.9658 - lr: 5.0000e-04\n",
      "Epoch 57/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9654\n",
      "Epoch 57: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2347 - accuracy: 0.9654 - val_loss: 0.2786 - val_accuracy: 0.9650 - lr: 5.0000e-04\n",
      "Epoch 58/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2256 - accuracy: 0.9699\n",
      "Epoch 58: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.2260 - accuracy: 0.9697 - val_loss: 0.2845 - val_accuracy: 0.9627 - lr: 5.0000e-04\n",
      "Epoch 59/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2303 - accuracy: 0.9659\n",
      "Epoch 59: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.2309 - accuracy: 0.9656 - val_loss: 0.2788 - val_accuracy: 0.9681 - lr: 5.0000e-04\n",
      "Epoch 60/100\n",
      "18/21 [========================>.....] - ETA: 0s - loss: 0.2299 - accuracy: 0.9638\n",
      "Epoch 60: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 1s 27ms/step - loss: 0.2255 - accuracy: 0.9656 - val_loss: 0.3025 - val_accuracy: 0.9603 - lr: 5.0000e-04\n",
      "Epoch 61/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2253 - accuracy: 0.9703\n",
      "Epoch 61: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 0s 21ms/step - loss: 0.2253 - accuracy: 0.9703 - val_loss: 0.2832 - val_accuracy: 0.9642 - lr: 5.0000e-04\n",
      "Epoch 62/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2234 - accuracy: 0.9675\n",
      "Epoch 62: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 0s 20ms/step - loss: 0.2238 - accuracy: 0.9669 - val_loss: 0.2908 - val_accuracy: 0.9611 - lr: 5.0000e-04\n",
      "Epoch 63/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2139 - accuracy: 0.9738\n",
      "Epoch 63: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 0s 22ms/step - loss: 0.2159 - accuracy: 0.9732 - val_loss: 0.2953 - val_accuracy: 0.9596 - lr: 5.0000e-04\n",
      "Epoch 64/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2303 - accuracy: 0.9661\n",
      "Epoch 64: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2315 - accuracy: 0.9652 - val_loss: 0.2927 - val_accuracy: 0.9611 - lr: 5.0000e-04\n",
      "Epoch 65/100\n",
      "20/21 [===========================>..] - ETA: 0s - loss: 0.2211 - accuracy: 0.9697\n",
      "Epoch 65: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 1s 23ms/step - loss: 0.2211 - accuracy: 0.9699 - val_loss: 0.2918 - val_accuracy: 0.9650 - lr: 5.0000e-04\n",
      "Epoch 66/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2183 - accuracy: 0.9691\n",
      "Epoch 66: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 1s 31ms/step - loss: 0.2183 - accuracy: 0.9691 - val_loss: 0.2944 - val_accuracy: 0.9627 - lr: 5.0000e-04\n",
      "Epoch 67/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2139 - accuracy: 0.9729\n",
      "Epoch 67: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 1s 24ms/step - loss: 0.2150 - accuracy: 0.9726 - val_loss: 0.2847 - val_accuracy: 0.9666 - lr: 5.0000e-04\n",
      "Epoch 68/100\n",
      "19/21 [==========================>...] - ETA: 0s - loss: 0.2083 - accuracy: 0.9727\n",
      "Epoch 68: val_accuracy did not improve from 0.96890\n",
      "21/21 [==============================] - 1s 30ms/step - loss: 0.2076 - accuracy: 0.9734 - val_loss: 0.2813 - val_accuracy: 0.9611 - lr: 5.0000e-04\n",
      "Epoch 69/100\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.2049 - accuracy: 0.9757\n",
      "Epoch 69: val_accuracy did not improve from 0.96890\n",
      "Restoring model weights from the end of the best epoch: 54.\n",
      "\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "21/21 [==============================] - 1s 26ms/step - loss: 0.2049 - accuracy: 0.9757 - val_loss: 0.2928 - val_accuracy: 0.9658 - lr: 5.0000e-04\n",
      "Epoch 69: early stopping\n",
      "\n",
      "============================================================\n",
      "âœ… TRAINING COMPLETE!\n",
      "============================================================\n",
      "â±ï¸  Training time: 45.96 seconds (0.77 minutes)\n",
      "ðŸ“ Best model saved: arsl_mediapipe_mlp_model_best.h5\n",
      "ðŸ“ Final model saved: arsl_mediapipe_mlp_model_final.h5\n",
      "\n",
      "ðŸ“Š Final Training Metrics:\n",
      "   Training Accuracy: 97.57%\n",
      "   Validation Accuracy: 96.58%\n",
      "   Training Loss: 0.2049\n",
      "   Validation Loss: 0.2928\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 7: BUILD AND TRAIN MLP MODEL\n",
    "# ============================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸ”¨ BUILDING AND TRAINING MLP MODEL')\n",
    "print('=' * 60)\n",
    "\n",
    "if df.empty:\n",
    "    print('âŒ ERROR: No data available. Run preprocessing cell first.')\n",
    "else:\n",
    "    # Clear session to free GPU memory\n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    print(f'\\nðŸ“‹ Model Configuration:')\n",
    "    print(f'   Input shape: {X_train.shape[1]} features')\n",
    "    print(f'   Output classes: {num_classes} Arabic letters')\n",
    "    print(f'   Device: {DEVICE}')\n",
    "    \n",
    "    # Build model with GPU optimization\n",
    "    with tf.device(DEVICE):\n",
    "        model = Sequential([\n",
    "            # Input layer: 256 neurons\n",
    "            Dense(\n",
    "                512,\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                input_shape=(X_train.shape[1],),\n",
    "                name='dense_512'\n",
    "            ),\n",
    "            BatchNormalization(name='bn_1'),\n",
    "            Dropout(0.2, name='dropout_1'),\n",
    "            \n",
    "            # Hidden layer: 128 neurons\n",
    "            Dense(\n",
    "                256,\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                name='dense_256'\n",
    "            ),\n",
    "            BatchNormalization(name='bn_2'),\n",
    "            Dropout(0.2, name='dropout_2'),\n",
    "            \n",
    "            # Hidden layer: 64 neurons\n",
    "            Dense(\n",
    "                64,\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                name='dense_64'\n",
    "            ),\n",
    "            Dropout(0.2, name='dropout_3'),\n",
    "            \n",
    "            # Output layer: softmax for multi-class classification\n",
    "            Dense(num_classes, activation='softmax', dtype='float32', name='output')\n",
    "        ])\n",
    "        \n",
    "        # Use legacy Adam optimizer (works better with mixed precision)\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.0005)\n",
    "        \n",
    "        # Compile model\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    # Display model summary\n",
    "    print('\\nðŸ“Š Model Summary:')\n",
    "    model.summary()\n",
    "    \n",
    "    # Prepare data pipelines\n",
    "    if USE_GPU:\n",
    "        BATCH_SIZE = 256\n",
    "    else:\n",
    "        BATCH_SIZE = 64\n",
    "    \n",
    "    train_ds = make_dataset(X_train, y_train, BATCH_SIZE, training=True)\n",
    "    val_ds = make_dataset(X_val, y_val, BATCH_SIZE, training=False)\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            'arsl_mediapipe_mlp_model_best.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=15,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Print training configuration\n",
    "    print(f'\\nâš™ï¸  Training Configuration:')\n",
    "    print(f'   Batch size: {BATCH_SIZE}')\n",
    "    print(f'   Epochs: 20 (early stopping may occur earlier)')\n",
    "    print(f'   Optimizer: Adam (learning rate: 0.001)')\n",
    "    print(f'   Loss: Categorical Crossentropy')\n",
    "    print(f'   Metrics: Accuracy')\n",
    "    print(f'   Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau')\n",
    "    \n",
    "    # Train model\n",
    "    print('\\nðŸš€ Starting training...')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tf.device(DEVICE):\n",
    "        history = model.fit(\n",
    "            train_ds,\n",
    "            validation_data=val_ds,\n",
    "            epochs=100,\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Save final model\n",
    "    model.save('arsl_mediapipe_mlp_model_final.h5')\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('âœ… TRAINING COMPLETE!')\n",
    "    print('=' * 60)\n",
    "    print(f'â±ï¸  Training time: {training_time:.2f} seconds ({training_time/60:.2f} minutes)')\n",
    "    print(f'ðŸ“ Best model saved: arsl_mediapipe_mlp_model_best.h5')\n",
    "    print(f'ðŸ“ Final model saved: arsl_mediapipe_mlp_model_final.h5')\n",
    "    \n",
    "    # Display final metrics\n",
    "    if hasattr(history, 'history'):\n",
    "        final_train_acc = history.history['accuracy'][-1]\n",
    "        final_val_acc = history.history['val_accuracy'][-1]\n",
    "        final_train_loss = history.history['loss'][-1]\n",
    "        final_val_loss = history.history['val_loss'][-1]\n",
    "        \n",
    "        print(f'\\nðŸ“Š Final Training Metrics:')\n",
    "        print(f'   Training Accuracy: {final_train_acc*100:.2f}%')\n",
    "        print(f'   Validation Accuracy: {final_val_acc*100:.2f}%')\n",
    "        print(f'   Training Loss: {final_train_loss:.4f}')\n",
    "        print(f'   Validation Loss: {final_val_loss:.4f}')\n",
    "    print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b7f39",
   "metadata": {},
   "source": [
    "## Section 8: Evaluate Model on Test Data\n",
    "\n",
    "Evaluate the best trained model on the held-out test set and report performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3f8d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ§ª MODEL EVALUATION ON TEST DATA\n",
      "============================================================\n",
      "\n",
      "ðŸ“¦ Loading best model...\n",
      "\n",
      "ðŸ” Evaluating on 1608 test samples...\n",
      "   Batch size: 256\n",
      "   Device: /GPU:0\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2402 - accuracy: 0.9745\n",
      "\n",
      "============================================================\n",
      "âœ… EVALUATION COMPLETE!\n",
      "============================================================\n",
      "â±ï¸  Evaluation time: 0.3225 seconds\n",
      "\n",
      "ðŸ“Š Test Performance Metrics:\n",
      "   Test Loss: 0.2402\n",
      "   Test Accuracy: 97.45%\n",
      "\n",
      "   ðŸŒŸ Excellent! Model is highly accurate.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Section 8: EVALUATE MODEL ON TEST DATA\n",
    "# ============================================\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸ§ª MODEL EVALUATION ON TEST DATA')\n",
    "print('=' * 60)\n",
    "\n",
    "if df.empty:\n",
    "    print('âŒ ERROR: No data available. Run preprocessing cell first.')\n",
    "else:\n",
    "    # Load the best model\n",
    "    print('\\nðŸ“¦ Loading best model...')\n",
    "    model_best = tf.keras.models.load_model('arsl_mediapipe_mlp_model_best.h5')\n",
    "    \n",
    "    # Create test dataset pipeline\n",
    "    eval_batch_size = 256 if USE_GPU else 128\n",
    "    test_ds = make_dataset(X_test, y_test, eval_batch_size, training=False)\n",
    "    \n",
    "    print(f'\\nðŸ” Evaluating on {len(X_test)} test samples...')\n",
    "    print(f'   Batch size: {eval_batch_size}')\n",
    "    print(f'   Device: {DEVICE}')\n",
    "    \n",
    "    # Evaluate\n",
    "    start_time = time.time()\n",
    "    with tf.device(DEVICE):\n",
    "        test_loss, test_accuracy = model_best.evaluate(test_ds, verbose=1)\n",
    "    eval_time = time.time() - start_time\n",
    "    \n",
    "    print('\\n' + '=' * 60)\n",
    "    print('âœ… EVALUATION COMPLETE!')\n",
    "    print('=' * 60)\n",
    "    print(f'â±ï¸  Evaluation time: {eval_time:.4f} seconds')\n",
    "    print(f'\\nðŸ“Š Test Performance Metrics:')\n",
    "    print(f'   Test Loss: {test_loss:.4f}')\n",
    "    print(f'   Test Accuracy: {test_accuracy*100:.2f}%')\n",
    "    \n",
    "    # Performance interpretation\n",
    "    if test_accuracy >= 0.95:\n",
    "        print(f'\\n   ðŸŒŸ Excellent! Model is highly accurate.')\n",
    "    elif test_accuracy >= 0.90:\n",
    "        print(f'\\n   â­ Very good performance!')\n",
    "    elif test_accuracy >= 0.80:\n",
    "        print(f'\\n   ðŸ‘ Good performance. Consider more training data or fine-tuning.')\n",
    "    else:\n",
    "        print(f'\\n   âš ï¸  May need improvement. Check data quality or increase epochs.')\n",
    "    print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e0f09c",
   "metadata": {},
   "source": [
    "## Section 9: Real-Time Inference (Webcam) with Arabic Letters\n",
    "\n",
    "Deploy the trained model for real-time Arabic letter recognition using your webcam. Press 'q' to quit.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. Make sure your webcam is connected and working\n",
    "2. Position your hand clearly in front of the camera\n",
    "3. The system will display the predicted letter in the top-left corner\n",
    "4. Special gestures:\n",
    "   - **SPACE**: Add a space between words\n",
    "   - **DELETE**: Remove the last letter\n",
    "   - **NOTHING**: Ignore the prediction\n",
    "\n",
    "The predicted sentence will be displayed at the bottom of the video feed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c310455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Section 9: REAL-TIME INFERENCE (WEBCAM)\n",
    "# ============================================\n",
    "# Commit-once-then-wait strategy (same as English notebooks)\n",
    "# Control labels match CSV: 'space', 'del', 'nothing' (lowercase)\n",
    "\n",
    "print('=' * 60)\n",
    "print('ðŸŽ¥ REAL-TIME ARABIC LETTER RECOGNITION')\n",
    "print('=' * 60)\n",
    "\n",
    "if df.empty:\n",
    "    print('âŒ ERROR: No data available. Run preprocessing cell first.')\n",
    "else:\n",
    "    # Load encoder\n",
    "    print('\\nðŸ“¦ Loading model and encoder...')\n",
    "    encoder = LabelEncoder()\n",
    "    df_labels = pd.read_csv(CSV_PATH)\n",
    "    # Handle both possible column names ('label' or 'letter')\n",
    "    label_col = 'label' if 'label' in df_labels.columns else 'letter'\n",
    "    encoder.fit(df_labels[label_col])\n",
    "    print(f'   Encoder classes ({len(encoder.classes_)}): {list(encoder.classes_[:5])}...')\n",
    "    \n",
    "    # Load trained model\n",
    "    tf.keras.mixed_precision.set_global_policy('float32')\n",
    "    mlp_model = tf.keras.models.load_model('arsl_mediapipe_mlp_model_best.h5')\n",
    "    print('âœ… Model and encoder loaded!')\n",
    "    \n",
    "    # Initialize MediaPipe\n",
    "    mp_hands = mp.solutions.hands\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    hands = mp_hands.Hands(\n",
    "        min_detection_confidence=0.7,\n",
    "        min_tracking_confidence=0.7\n",
    "    )\n",
    "    \n",
    "    # Stabilization settings\n",
    "    STABILIZATION_WINDOW_SIZE = 10\n",
    "    STABILIZATION_THRESHOLD = 7\n",
    "    MIN_CONFIDENCE = 0.70\n",
    "    HOLD_TIME_REQUIRED = 0.8\n",
    "    DISPLAY_WIDTH = 1280\n",
    "    DISPLAY_HEIGHT = 720\n",
    "    \n",
    "    # Open webcam\n",
    "    print('\\nðŸŽ¥ Initializing webcam...')\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print('âŒ ERROR: Could not open webcam!')\n",
    "    else:\n",
    "        print('âœ… Webcam opened successfully!')\n",
    "        print('\\nðŸ“ Instructions:')\n",
    "        print('   - Position your hand in front of the camera')\n",
    "        print('   - Hold a sign steady until it commits')\n",
    "        print('   - Change sign or remove hand for next letter')\n",
    "        print('   - Press \"q\" to quit, \"c\" to clear')\n",
    "        print('\\n' + '=' * 60)\n",
    "        print('ðŸ”´ Recording... Press \"q\" to stop')\n",
    "        print('=' * 60 + '\\n')\n",
    "        \n",
    "        # State variables\n",
    "        predicted_sentence = ''\n",
    "        stabilization_buffer = deque(maxlen=STABILIZATION_WINDOW_SIZE)\n",
    "        \n",
    "        # Commit-once-then-wait state\n",
    "        committed_label = None\n",
    "        current_sign_label = None\n",
    "        current_sign_start = None\n",
    "        waiting_for_change = False\n",
    "        \n",
    "        window_name = 'Arabic Sign Language Recognition (MediaPipe + MLP)'\n",
    "        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(window_name, DISPLAY_WIDTH, DISPLAY_HEIGHT)\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Process UNFLIPPED frame with MediaPipe (matches training data)\n",
    "                frame = cv2.resize(frame, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "                h, w, c = frame.shape\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                rgb_frame.flags.writeable = False\n",
    "                results = hands.process(rgb_frame)\n",
    "                rgb_frame.flags.writeable = True\n",
    "                \n",
    "                display_status = ''\n",
    "                status_color = (200, 200, 200)\n",
    "                \n",
    "                if results.multi_hand_landmarks:\n",
    "                    for hand_landmarks, handedness in zip(\n",
    "                        results.multi_hand_landmarks,\n",
    "                        results.multi_handedness\n",
    "                    ):\n",
    "                        mp_drawing.draw_landmarks(\n",
    "                            frame, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "                        )\n",
    "                        \n",
    "                        # Extract landmarks â€” NO mirroring (matches training)\n",
    "                        landmarks = np.array([\n",
    "                            [lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark\n",
    "                        ])\n",
    "                        input_data = landmarks.flatten().reshape(1, -1).astype('float32')\n",
    "                        \n",
    "                        try:\n",
    "                            prediction = mlp_model.predict(input_data, verbose=0)\n",
    "                        except Exception as e:\n",
    "                            display_status = f'Prediction error: {e}'\n",
    "                            status_color = (0, 0, 255)\n",
    "                            break\n",
    "                        \n",
    "                        pred_class = np.argmax(prediction)\n",
    "                        pred_confidence = float(np.max(prediction))\n",
    "                        pred_label = encoder.inverse_transform([pred_class])[0]\n",
    "                        \n",
    "                        # Skip low confidence\n",
    "                        if pred_confidence < MIN_CONFIDENCE:\n",
    "                            display_status = f'{pred_label} ({pred_confidence:.0%}) Low conf'\n",
    "                            status_color = (0, 100, 255)\n",
    "                            break\n",
    "                        \n",
    "                        # Stability buffer\n",
    "                        stabilization_buffer.append(pred_label)\n",
    "                        buffer_count = stabilization_buffer.count(pred_label)\n",
    "                        is_stable = (buffer_count >= STABILIZATION_THRESHOLD and \n",
    "                                     len(stabilization_buffer) == STABILIZATION_WINDOW_SIZE)\n",
    "                        \n",
    "                        if not is_stable:\n",
    "                            progress = buffer_count / STABILIZATION_THRESHOLD * 100\n",
    "                            display_status = f'{pred_label} ({pred_confidence:.0%}) Stabilizing {progress:.0f}%'\n",
    "                            status_color = (0, 255, 255)\n",
    "                            break\n",
    "                        \n",
    "                        now = time.time()\n",
    "                        \n",
    "                        # Check if waiting after a commit\n",
    "                        if waiting_for_change:\n",
    "                            if pred_label == committed_label:\n",
    "                                display_status = f'{pred_label} ({pred_confidence:.0%}) Committed - change sign'\n",
    "                                status_color = (255, 200, 0)\n",
    "                                break\n",
    "                            else:\n",
    "                                waiting_for_change = False\n",
    "                                committed_label = None\n",
    "                                current_sign_label = pred_label\n",
    "                                current_sign_start = now\n",
    "                        \n",
    "                        # Track hold time\n",
    "                        if pred_label != current_sign_label:\n",
    "                            current_sign_label = pred_label\n",
    "                            current_sign_start = now\n",
    "                        \n",
    "                        hold_duration = now - current_sign_start if current_sign_start else 0\n",
    "                        \n",
    "                        if hold_duration < HOLD_TIME_REQUIRED:\n",
    "                            hold_pct = hold_duration / HOLD_TIME_REQUIRED * 100\n",
    "                            display_status = f'{pred_label} ({pred_confidence:.0%}) Hold: {hold_pct:.0f}%'\n",
    "                            status_color = (0, 255, 255)\n",
    "                            break\n",
    "                        \n",
    "                        # COMMIT â€” control labels match CSV: 'space', 'del', 'nothing'\n",
    "                        if pred_label == 'space':\n",
    "                            if predicted_sentence and predicted_sentence[-1] != ' ':\n",
    "                                predicted_sentence += ' '\n",
    "                        elif pred_label == 'del':\n",
    "                            if predicted_sentence:\n",
    "                                predicted_sentence = predicted_sentence[:-1]\n",
    "                        elif pred_label != 'nothing':\n",
    "                            predicted_sentence += pred_label\n",
    "                        \n",
    "                        committed_label = pred_label\n",
    "                        waiting_for_change = True\n",
    "                        current_sign_label = None\n",
    "                        current_sign_start = None\n",
    "                        stabilization_buffer.clear()\n",
    "                        \n",
    "                        display_status = f'{pred_label} ({pred_confidence:.0%}) COMMITTED!'\n",
    "                        status_color = (0, 255, 0)\n",
    "                else:\n",
    "                    # No hand â†’ full reset\n",
    "                    committed_label = None\n",
    "                    waiting_for_change = False\n",
    "                    current_sign_label = None\n",
    "                    current_sign_start = None\n",
    "                    stabilization_buffer.clear()\n",
    "                    display_status = 'No hand detected'\n",
    "                    status_color = (150, 150, 150)\n",
    "                \n",
    "                # Flip for selfie-view display\n",
    "                frame = cv2.flip(frame, 1)\n",
    "                \n",
    "                # Draw info panel at top-left\n",
    "                cv2.rectangle(frame, (10, 10), (500, 100), (0, 0, 0), -1)\n",
    "                cv2.putText(frame, 'Arabic Letter Recognition', (20, 35),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 1)\n",
    "                cv2.putText(frame, display_status, (20, 75),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)\n",
    "                \n",
    "                # Draw sentence panel at bottom\n",
    "                cv2.rectangle(frame, (0, h - 80), (w, h), (0, 0, 0), -1)\n",
    "                cv2.putText(frame, 'Predicted:', (10, h - 55),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 200, 200), 1)\n",
    "                cv2.putText(frame, predicted_sentence[-50:], (10, h - 20),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 255), 2)\n",
    "                \n",
    "                cv2.imshow(window_name, frame)\n",
    "                \n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('c'):\n",
    "                    predicted_sentence = ''\n",
    "                    committed_label = None\n",
    "                    waiting_for_change = False\n",
    "                    stabilization_buffer.clear()\n",
    "                    print('Sentence cleared')\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            print('\\nâš ï¸ Interrupted by user')\n",
    "        except Exception as e:\n",
    "            print(f'âŒ Error: {e}')\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "            print('\\n' + '=' * 60)\n",
    "            print('Session ended')\n",
    "            if predicted_sentence:\n",
    "                print(f'Final sentence: {predicted_sentence}')\n",
    "            print('=' * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd06964",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Congratulations! You have successfully created a complete Arabic sign language recognition system using MediaPipe and neural networks.\n",
    "\n",
    "### What You've Accomplished:\n",
    "\n",
    "âœ… Extracted MediaPipe hand keypoints from your Arabic dataset  \n",
    "âœ… Preprocessed and split data into training, validation, and test sets  \n",
    "âœ… Built and trained a GPU-optimized MLP model  \n",
    "âœ… Evaluated model performance on test data  \n",
    "âœ… Deployed real-time inference using webcam\n",
    "\n",
    "### Generated Files:\n",
    "\n",
    "- `arsl_mediapipe_keypoints_final.csv` - Extracted keypoints dataset\n",
    "- `arsl_mediapipe_mlp_model_best.h5` - Best trained model (highest validation accuracy)\n",
    "- `arsl_mediapipe_mlp_model_final.h5` - Final model after training\n",
    "\n",
    "### Troubleshooting Tips:\n",
    "\n",
    "**If extraction is slow:**\n",
    "\n",
    "- Reduce dataset size or use a subset for testing\n",
    "- GPU will not significantly speed up image reading, only model training\n",
    "\n",
    "**If training is slow or runs out of memory:**\n",
    "\n",
    "- Reduce `BATCH_SIZE` to 128 or 64\n",
    "- Close other applications\n",
    "- Check GPU memory with `nvidia-smi -l 1`\n",
    "\n",
    "**If real-time inference is slow:**\n",
    "\n",
    "- Ensure GPU is being used (check `DEVICE` variable)\n",
    "- Reduce preprocessing in the inference loop if needed\n",
    "\n",
    "**If accuracy is low:**\n",
    "\n",
    "- Check data quality (images should be clear, well-lit)\n",
    "- Ensure hands are visible in most training images\n",
    "- Increase training data\n",
    "- Train for more epochs (remove early stopping or increase patience)\n",
    "\n",
    "### Future Improvements:\n",
    "\n",
    "- Use transfer learning with pre-trained models (ResNet, MobileNet)\n",
    "- Implement sequence modeling (LSTM, Transformers) for better temporal understanding\n",
    "- Add hand gesture smoothing for more stable predictions\n",
    "- Include two-hand detection for two-handed signs\n",
    "- Build a larger, more diverse dataset\n",
    "- Deploy as a web application or mobile app\n",
    "\n",
    "**Good luck with your Arabic sign language recognition project!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
