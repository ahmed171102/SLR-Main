{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104a9990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: module 'importlib.metadata' has no attribute 'packages_distributions'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\google\\api_core\\_python_version_support.py:252: FutureWarning: You are using a Python version (3.9.13) past its end of life. Google will update google.api_core with critical bug fixes on a best-effort basis, but not with any other fixes or features. Please upgrade to the latest Python version, or at least Python 3.10, and then update google.api_core.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "GPUs detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "Enabled memory growth for GPUs\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  NVIDIA GeForce MX150, compute capability 6.1\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "Mixed precision enabled: <Policy \"mixed_float16\">\n",
      "Matrix multiply device: /job:localhost/replica:0/task:0/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# GPU and TF setup\n",
    "import os\n",
    "import tensorflow as tf\n",
    "print('TensorFlow version:', tf.__version__)\n",
    "\n",
    "# Detect GPUs and configure memory growth safely\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('GPUs detected:', gpus)\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print('Enabled memory growth for GPUs')\n",
    "    except RuntimeError as e:\n",
    "        print('Error setting memory growth:', e)\n",
    "else:\n",
    "    print('No GPU detected â€” training will run on CPU (much slower)')\n",
    "\n",
    "# Enable mixed precision only when a GPU is present and TF supports it\n",
    "try:\n",
    "    if gpus and tf.test.is_built_with_cuda():\n",
    "        from tensorflow.keras import mixed_precision\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print('Mixed precision enabled:', mixed_precision.global_policy())\n",
    "    else:\n",
    "        print('Mixed precision not enabled: no compatible GPU or CUDA support')\n",
    "except Exception as e:\n",
    "    print('Mixed precision not enabled:', e)\n",
    "\n",
    "# Quick GPU computation test (sanity check)\n",
    "if gpus:\n",
    "    try:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.uniform((512, 512))\n",
    "            b = tf.random.uniform((512, 512))\n",
    "            c = tf.matmul(a, b)\n",
    "            print('Matrix multiply device:', c.device)\n",
    "    except Exception as e:\n",
    "        print('GPU computation test failed:', e)\n",
    "else:\n",
    "    print('Skipping GPU matmul test (no GPU)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0173e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths: change as needed\n",
    "BASE_DIR = r'M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database'\n",
    "DATASET_DIR = os.path.join(BASE_DIR, 'ArASL_Database')\n",
    "TEST_DIR = os.path.join(BASE_DIR, 'ArASL_35')\n",
    "print('Dataset:', DATASET_DIR)\n",
    "print('Test folder:', TEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9412473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Define the dataset directory\n",
    "dataset_dir = DATASET_DIR  # Use the same DATASET_DIR as above\n",
    "\n",
    "# Parameters\n",
    "batch_size = 32\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "validation_split = 0.2\n",
    "seed = 123\n",
    "\n",
    "print(\"Loading Training Dataset...\")\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir,\n",
    "  validation_split=validation_split,\n",
    "  subset=\"training\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "print(\"\\nLoading Validation Dataset...\")\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  dataset_dir,\n",
    "  validation_split=validation_split,\n",
    "  subset=\"validation\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "# Get class names\n",
    "class_names = train_ds.class_names\n",
    "print(f\"\\nClass names ({len(class_names)}): {class_names}\")\n",
    "\n",
    "# Visualize 9 images from the training set\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_ds.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Optimize for performance (Autotuning)\n",
    "# This keeps data in memory (cache) and prepares the next batch while the GPU is working (prefetch)\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(\"\\nDatasets are optimized for training (Cached & Prefetched).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b75353",
   "metadata": {},
   "source": [
    "## Parameters and data pipeline\n",
    "We use `tf.keras.utils.image_dataset_from_directory` with caching and prefetching for best throughput to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61165fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from tensorflow.data import AUTOTUNE\n",
    "\n",
    "IMG_SIZE = 128  # MobileNetV2 works well at 128 or 160\n",
    "BATCH_SIZE = 64\n",
    "SEED = 123\n",
    "VAL_SPLIT = 0.2\n",
    "\n",
    "print('Image size:', IMG_SIZE)\n",
    "print('Batch size:', BATCH_SIZE)\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_ds = image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset='training',\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "val_ds = image_dataset_from_directory(\n",
    "    DATASET_DIR,\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset='validation',\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "NUM_CLASSES = len(class_names)\n",
    "print('Classes ({}):'.format(NUM_CLASSES), class_names)\n",
    "\n",
    "# Performance: cache + prefetch. Important changes:\n",
    "# - Do not divide by 255 in the dataset mapping when using `preprocess_input` inside the model.\n",
    "# - Keep augmentation inside the model graph (data_augmentation layer) so augmentation uses vectorized ops\n",
    "#   and does not duplicate work in the tf.data pipeline.\n",
    "def preprocess(image, label):\n",
    "    # Keep image in [0,255] range as `tf.keras.applications.mobilenet_v2.preprocess_input`\n",
    "    # expects inputs in that range. We only cast to float32 here.\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    return image, label\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# Optional augmentation layer (applied in the model graph to avoid double work in pipeline)\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomRotation(0.05),\n",
    "    tf.keras.layers.RandomZoom(0.05),\n",
    "    tf.keras.layers.RandomTranslation(0.05, 0.05),\n",
    "], name='data_augmentation')\n",
    "\n",
    "# Keep dataset pipelines lightweight: cache then prefetch with AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
    "\n",
    "# Use dataset cardinality (number of batches) as steps per epoch\n",
    "STEPS_PER_EPOCH = int(tf.data.experimental.cardinality(train_ds).numpy())\n",
    "VALIDATION_STEPS = int(tf.data.experimental.cardinality(val_ds).numpy())\n",
    "print('Steps per epoch (approx):', STEPS_PER_EPOCH)\n",
    "print('Validation steps (approx):', VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8d0774",
   "metadata": {},
   "source": [
    "## Build MobileNetV2 model (transfer learning)\n",
    "We freeze the base model for initial training, use BatchNorm and Dropout in the head, and ensure final Dense uses `dtype='float32'` to avoid loss scaling issues when using mixed precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51592f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "print('Building MobileNetV2 base...')\n",
    "with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    x = data_augmentation(inputs)  # small augmentation in model graph as well (optional)\n",
    "    x = tf.keras.applications.mobilenet_v2.preprocess_input(x)\n",
    "    x = base_model(x, training=False)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    outputs = Dense(NUM_CLASSES, activation='softmax', dtype='float32', name='predictions')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile model with a conservative learning rate for transfer learning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "print('Model compiled.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ea48a5",
   "metadata": {},
   "source": [
    "## Callbacks and initial training\n",
    "We save the best model by `val_accuracy`, reduce LR on plateau, and log to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae91ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint('mobilenet_arabic_best_initial.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-7, verbose=1)\n",
    "csv_logger = CSVLogger('training_initial.csv')\n",
    "earlystop = EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True, verbose=1)\n",
    "\n",
    "INITIAL_EPOCHS = 8\n",
    "print('Starting initial training for', INITIAL_EPOCHS, 'epochs...')\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=INITIAL_EPOCHS,\n",
    "    callbacks=[checkpoint_cb, reduce_lr, csv_logger],\n",
    "    verbose=1\n",
    ")\n",
    "print('Initial training completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ffca7",
   "metadata": {},
   "source": [
    "## Fine-tuning: unfreeze last N layers and continue training with smaller LR\n",
    "Unfreeze the top of the base model and train with a smaller LR to refine pre-trained weights for your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8f604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze some layers for fine-tuning\n",
    "NUM_UNFREEZE = 30  # adjust depending on dataset & GPU memory\n",
    "\n",
    "# Make base_trainable from some layer onwards\n",
    "for layer in base_model.layers[-NUM_UNFREEZE:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with lower LR\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "FINE_TUNE_EPOCHS = 10\n",
    "total_epochs = INITIAL_EPOCHS + FINE_TUNE_EPOCHS\n",
    "print('Starting fine-tuning for', FINE_TUNE_EPOCHS, 'epochs (total epochs:', total_epochs, ')')\n",
    "\n",
    "checkpoint_cb2 = ModelCheckpoint('mobilenet_arabic_best_finetuned.h5', monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "csv_logger2 = CSVLogger('training_finetune.csv')\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=INITIAL_EPOCHS,\n",
    "    callbacks=[checkpoint_cb2, reduce_lr, csv_logger2],\n",
    "    verbose=1\n",
    ")\n",
    "print('Fine-tuning completed.')\n",
    "\n",
    "# Save final model (HDF5 or SavedModel)\n",
    "model.save('mobilenet_arabic_final.h5')\n",
    "print('Saved final model to mobilenet_arabic_final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e55a29",
   "metadata": {},
   "source": [
    "## Evaluate and visualize training history\n",
    "Plot accuracy and loss curves, and optionally compute confusion matrix on a held-out test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec98903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(h, title_suffix=''):\n",
    "    hist = h.history\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(hist.get('accuracy', []), label='train_acc')\n",
    "    plt.plot(hist.get('val_accuracy', []), label='val_acc')\n",
    "    plt.title('Accuracy '+title_suffix)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(hist.get('loss', []), label='train_loss')\n",
    "    plt.plot(hist.get('val_loss', []), label='val_loss')\n",
    "    plt.title('Loss '+title_suffix)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history, '(initial)')\n",
    "plot_history(history_finetune, '(finetune)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ccf5e1",
   "metadata": {},
   "source": [
    "## Quick inference example (single image)\n",
    "Load a single image, run prediction, and show the top result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeae72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def predict_image(img_path, model_path='mobilenet_arabic_best_finetuned.h5'):\n",
    "    model = load_model(model_path)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "    img = img.astype('float32') / 255.0\n",
    "    x = np.expand_dims(img, axis=0)\n",
    "    preds = model.predict(x)\n",
    "    idx = np.argmax(preds[0])\n",
    "    return class_names[idx], float(np.max(preds[0]))\n",
    "\n",
    "# Example (change path to a real file on your system)\n",
    "#label, score = predict_image(r'M:\\\\path\\\\to\\\\example.jpg')\n",
    "#print('Predicted:', label, 'score:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4442e040",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook created: `Mobilenet-arabic-optimized.ipynb`.\n",
    "Next steps: run the cells in order. If you want, I can update your original notebook in-place, or run a smaller smoke test here (I cannot execute code in your environment without you running it)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
