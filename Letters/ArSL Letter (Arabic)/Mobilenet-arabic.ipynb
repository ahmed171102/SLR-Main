{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the absolute base path to your dataset\n",
    "BASE_DIR = r\"M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database\"\n",
    "\n",
    "# Define specific subdirectories\n",
    "DATASET_DIR = os.path.join(BASE_DIR, \"ArASL_Database\")\n",
    "TEST_DIR = os.path.join(BASE_DIR, \"ArASL_35\")\n",
    "\n",
    "print(f\"Dataset Directory: {DATASET_DIR}\")\n",
    "print(f\"Test Directory: {TEST_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7ef915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def read_image(path):\n",
    "    \"\"\"Reads an image from a path that might contain non-ASCII characters.\"\"\"\n",
    "    try:\n",
    "        # Read file as byte stream and decode\n",
    "        stream = np.fromfile(path, dtype=np.uint8)\n",
    "        img = cv2.imdecode(stream, cv2.IMREAD_COLOR)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06757c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# ============================================\n",
    "# GPU CONFIGURATION - MUST RUN FIRST\n",
    "# ============================================\n",
    "print(\"--- TENSORFLOW GPU CONFIGURATION ---\")\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(f\"All Physical Devices: {physical_devices}\")\n",
    "\n",
    "# List GPU devices\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs Detected: {gpus}\")\n",
    "\n",
    "# Configure GPU memory growth to avoid allocating all memory at once\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for all GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"‚úì Memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "        \n",
    "        # Set GPU as default device\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(f\"‚úì Using GPU: {gpus[0]}\")\n",
    "        \n",
    "        # Verify GPU is being used\n",
    "        print(f\"‚úì GPU Device Name: {gpus[0].name}\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error configuring GPU: {e}\")\n",
    "else:\n",
    "    print(\"‚ö† WARNING: No GPU detected! Training will use CPU (much slower)\")\n",
    "\n",
    "# Verify GPU is available for computation\n",
    "print(f\"\\n--- GPU VERIFICATION ---\")\n",
    "print(f\"GPU Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "if gpus:\n",
    "    print(f\"‚úì GPU Available: True\")\n",
    "    print(f\"‚úì GPU Device Name: {gpus[0].name}\")\n",
    "    # Get GPU details\n",
    "    try:\n",
    "        gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "        print(f\"‚úì GPU Details: {gpu_details}\")\n",
    "    except:\n",
    "        pass\n",
    "else:\n",
    "    print(f\"‚úó GPU Available: False\")\n",
    "\n",
    "# Set mixed precision policy for faster training (optional but recommended)\n",
    "# This allows TensorFlow to use float16 on GPU which is faster\n",
    "try:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(f\"‚úì Mixed precision enabled: {policy.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Mixed precision not available: {e}\")\n",
    "\n",
    "print(\"\\n--- CONFIGURATION COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad73a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GPU VERIFICATION TEST\n",
    "# ============================================\n",
    "# Run a simple computation to verify GPU is actually being used\n",
    "print(\"Testing GPU with a simple computation...\")\n",
    "\n",
    "# Create a simple tensor operation\n",
    "with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "    \n",
    "    # Check which device the operation ran on\n",
    "    print(f\"Operation result shape: {c.shape}\")\n",
    "    print(f\"Operation executed on: {c.device}\")\n",
    "    \n",
    "    # Check if GPU is being used (device string contains 'GPU')\n",
    "    device_str = str(c.device)\n",
    "    if 'GPU' in device_str or 'gpu' in device_str.lower():\n",
    "        print(\"‚úì SUCCESS: GPU is being used for computations!\")\n",
    "        print(f\"‚úì Device confirmed: {device_str}\")\n",
    "    else:\n",
    "        print(\"‚ö† WARNING: Operations are running on CPU, not GPU\")\n",
    "        print(f\"Device detected: {device_str}\")\n",
    "\n",
    "print(\"\\n--- GPU Setup Complete ---\")\n",
    "print(\"You can now proceed with the rest of the notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d09be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = r'M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)'\n",
    "\n",
    "# Traverse all subdirectories and remove .DS_Store files\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file == \".DS_Store\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed: {file_path}\")\n",
    "\n",
    "print(\"All .DS_Store files removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e87dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‚ö†Ô∏è CHANGE THIS to your actual folder path\n",
    "dataset_path = DATASET_DIR \n",
    "\n",
    "class_counts = {}\n",
    "\n",
    "# 1. Loop through every folder and count files\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"{'Class Name':<20} | {'Count':<10} | {'Status'}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    folders = sorted(os.listdir(dataset_path))\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            count = len(os.listdir(folder_path))\n",
    "            class_counts[folder] = count\n",
    "            \n",
    "            # Simple status check\n",
    "            status = \"OK\"\n",
    "            if count > 2000: status = \"üî¥ HUGE (Needs trimming)\"\n",
    "            elif count < 500: status = \"‚ö†Ô∏è SMALL\"\n",
    "            \n",
    "            print(f\"{folder:<20} | {count:<10} | {status}\")\n",
    "else:\n",
    "    print(\"‚ùå Path not found! Check 'dataset_path' variable.\")\n",
    "\n",
    "# 2. (Optional) Show a Graph\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Dataset Balance Check\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e712595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ‚ö†Ô∏è CHANGE THIS to your actual folder path\n",
    "dataset_path = DATASET_DIR \n",
    "\n",
    "class_counts = {}\n",
    "\n",
    "# 1. Loop through every folder and count files\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"{'Class Name':<20} | {'Count':<10} | {'Status'}\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    folders = sorted(os.listdir(dataset_path))\n",
    "    for folder in folders:\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            count = len(os.listdir(folder_path))\n",
    "            class_counts[folder] = count\n",
    "            \n",
    "            # Simple status check\n",
    "            status = \"OK\"\n",
    "            if count > 2000: status = \"üî¥ HUGE (Needs trimming)\"\n",
    "            elif count < 500: status = \"‚ö†Ô∏è SMALL\"\n",
    "            \n",
    "            print(f\"{folder:<20} | {count:<10} | {status}\")\n",
    "else:\n",
    "    print(\"‚ùå Path not found! Check 'dataset_path' variable.\")\n",
    "\n",
    "# 2. (Optional) Show a Graph\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Dataset Balance Check\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde1104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "# ‚ö†Ô∏è CHANGE THIS\n",
    "dataset_path = DATASET_DIR\n",
    "TARGET_LIMIT = 1700  # The maximum images allowed per folder\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    for folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, folder)\n",
    "        \n",
    "        if os.path.isdir(folder_path):\n",
    "            files = os.listdir(folder_path)\n",
    "            count = len(files)\n",
    "            \n",
    "            if count > TARGET_LIMIT:\n",
    "                # Calculate how many to delete\n",
    "                num_to_delete = count - TARGET_LIMIT\n",
    "                print(f\"‚úÇÔ∏è trimming '{folder}': Has {count}, Deleting {num_to_delete}...\")\n",
    "                \n",
    "                # Randomly shuffle the list so we don't just delete the newest ones\n",
    "                random.shuffle(files)\n",
    "                \n",
    "                # Delete the extras\n",
    "                files_to_remove = files[:num_to_delete]\n",
    "                for file in files_to_remove:\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    try:\n",
    "                        os.remove(file_path)\n",
    "                    except OSError as e:\n",
    "                        print(f\"Error deleting {file}: {e}\")\n",
    "                    \n",
    "                print(f\"‚úÖ '{folder}' is now at {TARGET_LIMIT}.\")\n",
    "            else:\n",
    "                print(f\"üëç '{folder}' is fine ({count}).\")\n",
    "else:\n",
    "    print(f\"‚ùå Path not found: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "DATASET_PATH = DATASET_DIR\n",
    "TEST_FOLDER = TEST_DIR\n",
    "\n",
    "# --- EXECUTION ---\n",
    "print(f\"üöÄ Starting optimization for test set creation...\")\n",
    "os.makedirs(TEST_FOLDER, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "errors = 0\n",
    "\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"‚ùå Dataset path not found: {DATASET_PATH}\")\n",
    "else:\n",
    "    # Get list of classes (folders only)\n",
    "    classes = sorted([d for d in os.listdir(DATASET_PATH) if os.path.isdir(os.path.join(DATASET_PATH, d))])\n",
    "\n",
    "    print(f\"üìÇ Found {len(classes)} classes. Processing...\")\n",
    "\n",
    "    for cls in classes:\n",
    "        src_cls_path = os.path.join(DATASET_PATH, cls)\n",
    "        dest_cls_path = os.path.join(TEST_FOLDER, cls)\n",
    "        \n",
    "        # 1. Create Destination Folder\n",
    "        os.makedirs(dest_cls_path, exist_ok=True)\n",
    "        \n",
    "        # 2. CLEANUP: Remove old files in dest folder so we have exactly 1 fresh image\n",
    "        for old_file in os.listdir(dest_cls_path):\n",
    "            os.remove(os.path.join(dest_cls_path, old_file))\n",
    "\n",
    "        # 3. Get all valid image files\n",
    "        images = [f for f in os.listdir(src_cls_path) if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "        \n",
    "        if not images:\n",
    "            print(f\"‚ö†Ô∏è  Skipping '{cls}': No images found.\")\n",
    "            continue\n",
    "\n",
    "        # 4. SAFETY LOOP: Find a non-corrupt image\n",
    "        # We shuffle the list to pick random candidates\n",
    "        random.shuffle(images)\n",
    "        found_valid = False\n",
    "        \n",
    "        for chosen_image in images:\n",
    "            src_path = os.path.join(src_cls_path, chosen_image)\n",
    "            \n",
    "            # Verify Image Integrity\n",
    "            try:\n",
    "                # Use the helper function to read images with non-ASCII paths\n",
    "                img = read_image(src_path)\n",
    "                \n",
    "                if img is not None:\n",
    "                    # Image is good! Copy it.\n",
    "                    shutil.copy2(src_path, os.path.join(dest_cls_path, chosen_image))\n",
    "                    \n",
    "                    count += 1\n",
    "                    found_valid = True\n",
    "                    print(f\"‚úÖ Copied: {cls} -> {chosen_image}\")\n",
    "                    break # Stop loop, we found our 1 image\n",
    "                else:\n",
    "                    print(f\"‚ùå Corrupt file found in {cls}, trying next...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {chosen_image}: {e}\")\n",
    "                continue\n",
    "\n",
    "        if not found_valid:\n",
    "            print(f\"üî¥ FAILED: Could not find any valid images for '{cls}'\")\n",
    "            errors += 1\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"üéâ DONE! Successfully created test set with {count} images.\")\n",
    "    if errors > 0:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: {errors} classes failed to produce an image.\")\n",
    "    print(f\"üìÇ Saved to: {TEST_FOLDER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa789e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Paste your exact path again\n",
    "DATASET_PATH = r\"M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database\\ArASL_35\"\n",
    "\n",
    "# Let's look inside the first folder it failed on: 'ain'\n",
    "target_folder = os.path.join(DATASET_PATH, \"ain\") \n",
    "\n",
    "if os.path.exists(target_folder):\n",
    "    print(f\"üìÇ Contents of '{target_folder}':\")\n",
    "    items = os.listdir(target_folder)\n",
    "    \n",
    "    for item in items[:5]: # Show first 5 items\n",
    "        full_path = os.path.join(target_folder, item)\n",
    "        type_str = \"DIR\" if os.path.isdir(full_path) else \"FILE\"\n",
    "        print(f\"   [{type_str}]  {item}\")\n",
    "else:\n",
    "    print(\"‚ùå The 'ain' folder does not exist at that path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d29718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = r'M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database'\n",
    "\n",
    "# Traverse all subdirectories and remove .DS_Store files\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file == \".DS_Store\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed: {file_path}\")\n",
    "\n",
    "print(\"All .DS_Store files removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262bbf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = 'M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database'\n",
    "\n",
    "# Traverse all subdirectories and remove .DS_Store files\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file == \".DS_Store\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed: {file_path}\")\n",
    "\n",
    "print(\"All .DS_Store files removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865bf298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "dataset_dir = r'M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\Datasets\\Dataset (ArASL)\\ArASL Database\\ArASL_Database'\n",
    "\n",
    "# Only include directories (filter out files like .DS_Store)\n",
    "classes = [d for d in os.listdir(dataset_dir) if os.path.isdir(os.path.join(dataset_dir, d))]\n",
    "classes.sort() # Sorting ensures consistent order\n",
    "print(f\"Classes found ({len(classes)}): {classes}\")\n",
    "\n",
    "# Calculate grid size dynamically based on number of classes\n",
    "num_classes = len(classes)\n",
    "cols = 5\n",
    "rows = math.ceil(num_classes / cols)\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, label in enumerate(classes):\n",
    "    class_dir = os.path.join(dataset_dir, label)\n",
    "    # Filter for image files\n",
    "    img_files = [f for f in os.listdir(class_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if img_files:\n",
    "        # Load the first image found in the class\n",
    "        img_path = os.path.join(class_dir, img_files[0])\n",
    "        \n",
    "        # Use a robust reading method (handling potential path issues)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is not None:\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (128, 128))\n",
    "            axes[i].imshow(img)\n",
    "        else:\n",
    "            axes[i].text(0.5, 0.5, 'Read Error', ha='center', va='center', color='red')\n",
    "    else:\n",
    "        axes[i].text(0.5, 0.5, 'No Images', ha='center', va='center')\n",
    "        \n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(label)\n",
    "\n",
    "# Hide any unused subplots in the grid\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13550859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DATA GENERATORS WITH GPU OPTIMIZATION\n",
    "# ============================================\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image size and batch size\n",
    "# OPTIMIZED: Batch size 64 is the sweet spot for MX150 (4GB GPU)\n",
    "# - Batch 32: ~1 hour/epoch, uses ~2.3 GB (safe but slow)\n",
    "# - Batch 64: ~40-45 mins/epoch, uses ~3.2 GB (optimal balance) ‚úì\n",
    "# - Batch 128: ~50 mins/epoch, uses ~3.8 GB (risky, may OOM if other apps open)\n",
    "IMG_SIZE = 64 \n",
    "BATCH_SIZE = 64  # Optimal for MX150: Good speed, safe memory usage\n",
    "\n",
    "print(f\"Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(f\"Expected steps per epoch: {69600 // BATCH_SIZE}\")\n",
    "print(f\"Expected time per epoch: ~40-45 minutes (on MX150)\")\n",
    "print(f\"Expected GPU memory usage: ~3.2 GB / 4 GB (safe margin)\")\n",
    "\n",
    "# Data generators\n",
    "# We are using this approach to make it less computationally extensive as the data consists of 87,000 images appx,\n",
    "# loading all the images as generally done and then label encoding them will be CPU extensive task. \n",
    "\n",
    "# UPDATED AUGMENTATION: gentler transforms + no horizontal flip (ASL letters not symmetric)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    validation_split=0.2  # Splitting data into train (80%) and val (20%)\n",
    ")\n",
    "\n",
    "# Train & validation generators (load images directly from disk)\n",
    "# These will feed data to GPU efficiently\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\",\n",
    "    shuffle=True  # Shuffle for better training\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\",\n",
    "    shuffle=False  # No need to shuffle validation data\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training samples: {train_generator.samples}\")\n",
    "print(f\"‚úì Validation samples: {val_generator.samples}\")\n",
    "print(f\"‚úì Number of classes: {len(train_generator.class_indices)}\")\n",
    "print(\"Class labels:\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94783e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GPU MEMORY MONITORING & OPTIMIZATION TIPS\n",
    "# ============================================\n",
    "print(\"GPU Memory Management Tips:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Current batch size: {BATCH_SIZE}\")\n",
    "print(f\"Expected memory usage: ~{3.2 if BATCH_SIZE == 64 else 2.3 if BATCH_SIZE == 32 else 3.8} GB / 4 GB\")\n",
    "\n",
    "print(\"\\nüí° MEMORY OPTIMIZATION TIPS:\")\n",
    "print(\"1. Close other GPU-intensive applications during training\")\n",
    "print(\"2. Close browser tabs with video/graphics (they use GPU)\")\n",
    "print(\"3. Monitor memory with: nvidia-smi -l 1 (in separate terminal)\")\n",
    "print(\"4. If you get 'Out of Memory' error:\")\n",
    "print(\"   - Reduce batch size to 32\")\n",
    "print(\"   - Or close other applications\")\n",
    "print(\"5. Current batch size 64 is optimal for MX150 (safe + fast)\")\n",
    "\n",
    "print(\"\\nüìä To check GPU memory during training:\")\n",
    "print(\"   Open Command Prompt and run: nvidia-smi -l 1\")\n",
    "print(\"   You should see GPU-Util: 50-100% and Memory-Usage increasing\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to train with optimal settings!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c8ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTIMIZED MODEL BUILDING WITH GPU\n",
    "# ============================================\n",
    "print(\"Building optimized MobileNetV2 model...\")\n",
    "\n",
    "# Build base model - TensorFlow will automatically use GPU if available\n",
    "with tf.device('/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'):\n",
    "    base_model = MobileNetV2(\n",
    "        weights=\"imagenet\", \n",
    "        include_top=False, \n",
    "        input_shape=(128, 128, 3),\n",
    "        alpha=1.0  # Width multiplier (1.0 = full MobileNetV2)\n",
    "    )\n",
    "    \n",
    "    base_model.trainable = False  # Freezing all layers initially as this is an initial training\n",
    "    \n",
    "    # OPTIMIZED ARCHITECTURE: More capacity for better learning\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    \n",
    "    # Add BatchNormalization for better training stability\n",
    "    x = Dense(512, activation=\"relu\", name=\"fc1\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)  # Increased dropout for better regularization\n",
    "    \n",
    "    x = Dense(256, activation=\"relu\", name=\"fc2\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    \n",
    "    # Output layer - ensure float32 for numerical stability\n",
    "    output_layer = Dense(\n",
    "        len(train_generator.class_indices), \n",
    "        activation=\"softmax\", \n",
    "        dtype='float32',\n",
    "        name=\"predictions\"\n",
    "    )(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "print(f\"Model built on device: {tf.config.list_physical_devices('GPU')[0].name if tf.config.list_physical_devices('GPU') else 'CPU'}\")\n",
    "\n",
    "# OPTIMIZED COMPILATION: Lower learning rate for transfer learning\n",
    "# Lower LR helps with transfer learning from ImageNet weights\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(\n",
    "        learning_rate=0.0001,  # Reduced from 0.001 - better for transfer learning\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07\n",
    "    ),\n",
    "    loss=\"categorical_crossentropy\", \n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"‚úì Model compiled successfully with optimized settings\")\n",
    "print(f\"‚úì Learning rate: 0.0001 (optimized for transfer learning)\")\n",
    "print(f\"‚úì Architecture: Enhanced with BatchNorm and deeper layers\")\n",
    "print(f\"Model summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# OPTIMIZED INITIAL TRAINING WITH GPU\n",
    "# ============================================\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger\n",
    "\n",
    "print(\"Starting optimized initial training with GPU...\")\n",
    "\n",
    "# Verify GPU is being used\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"‚úì Training on GPU: {tf.config.list_physical_devices('GPU')[0].name}\")\n",
    "else:\n",
    "    print(\"‚ö† WARNING: No GPU detected, training on CPU\")\n",
    "\n",
    "# Calculate steps per epoch for better control\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = val_generator.samples // BATCH_SIZE\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "# OPTIMIZED CALLBACKS for better training\n",
    "callbacks = [\n",
    "    # Save best model based on validation accuracy\n",
    "    ModelCheckpoint(\n",
    "        'best_model_initial_optimized.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1,\n",
    "        save_weights_only=False\n",
    "    ),\n",
    "    # Reduce learning rate when validation loss plateaus\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,  # Reduce LR by half\n",
    "        patience=2,  # Wait 2 epochs before reducing\n",
    "        min_lr=1e-7,\n",
    "        verbose=1,\n",
    "        mode='min'\n",
    "    ),\n",
    "    # Log training history to CSV\n",
    "    CSVLogger('training_history_initial.csv', append=False),\n",
    "    # Early stopping to prevent overfitting (optional - commented for initial training)\n",
    "    # EarlyStopping(\n",
    "    #     monitor='val_loss',\n",
    "    #     patience=5,\n",
    "    #     restore_best_weights=True,\n",
    "    #     verbose=1\n",
    "    # )\n",
    "]\n",
    "\n",
    "print(\"\\nüìä Training Configuration:\")\n",
    "print(f\"  - Learning rate: 0.0001 (optimized for transfer learning)\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Optimizer: Adam with reduced LR\")\n",
    "print(f\"  - Callbacks: ModelCheckpoint, ReduceLROnPlateau, CSVLogger\")\n",
    "print(f\"\\nüéØ Expected improvements:\")\n",
    "print(f\"  - Better convergence with lower learning rate\")\n",
    "print(f\"  - Improved accuracy with enhanced architecture\")\n",
    "print(f\"  - Automatic LR reduction if validation plateaus\")\n",
    "\n",
    "# Train with optimized settings\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10,  # Increased epochs - model needs more time to learn\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Initial training completed!\")\n",
    "print(\"üìà Check 'training_history_initial.csv' for detailed metrics\")\n",
    "print(\"üíæ Best model saved as 'best_model_initial_optimized.h5'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838882da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FINE-TUNING WITH GPU OPTIMIZATION\n",
    "# ============================================\n",
    "print(\"Starting fine-tuning phase...\")\n",
    "\n",
    "# Verify GPU usage\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"‚úì Fine-tuning on GPU: {tf.config.list_physical_devices('GPU')[0].name}\")\n",
    "else:\n",
    "    print(\"‚ö† WARNING: No GPU detected, fine-tuning on CPU\")\n",
    "\n",
    "# Fine-tuning: Unfreeze last layers for better feature adaptation\n",
    "FINE_TUNE_LAYERS = 40  # Increase for deeper adaptation\n",
    "print(f\"Unfreezing last {FINE_TUNE_LAYERS} layers for fine-tuning...\")\n",
    "\n",
    "# Freeze everything first, then unfreeze the tail\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "for layer in base_model.layers[-FINE_TUNE_LAYERS:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "trainable_count = sum(layer.trainable for layer in base_model.layers)\n",
    "print(f\"Trainable layers: {trainable_count} / {len(base_model.layers)}\")\n",
    "\n",
    "# Recompile with a lower learning rate + label smoothing\n",
    "finetune_optimizer = tf.keras.optimizers.legacy.Adam(\n",
    "    learning_rate=5e-5,  # Half of initial LR\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07\n",
    ")\n",
    "finetune_loss = tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=finetune_optimizer,\n",
    "    loss=finetune_loss,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "print(\"‚úì Model recompiled for fine-tuning with LR=5e-5 and label smoothing 0.1\")\n",
    "\n",
    "# Calculate steps for fine-tuning (reuse from before)\n",
    "steps_per_epoch = train_generator.samples // BATCH_SIZE\n",
    "validation_steps = val_generator.samples // BATCH_SIZE\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "print(f\"Validation steps: {validation_steps}\")\n",
    "\n",
    "# Optimized callbacks for fine-tuning\n",
    "callbacks_finetune = [\n",
    "    # Save best model during fine-tuning\n",
    "    ModelCheckpoint(\n",
    "        'best_model_finetuned.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Reduce learning rate on plateau\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=2,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Early stopping to prevent overfitting and save time\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=4,  # Reduced from 6 - stops faster when no improvement\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        mode='max'\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"\\nüìä OPTIMIZED Fine-tuning Configuration:\")\n",
    "print(f\"  - Learning rate: 5e-5 (half of initial LR)\")\n",
    "print(f\"  - Unfrozen layers: {FINE_TUNE_LAYERS} (deeper adaptation)\")\n",
    "print(f\"  - Label smoothing: 0.1\")\n",
    "print(f\"  - EarlyStopping: Stops after 4 epochs without improvement\")\n",
    "print(f\"  - Max epochs: 15 (reduced from 20, stops earlier if needed)\")\n",
    "\n",
    "# OPTIMIZED: Reduced max epochs, EarlyStopping will stop earlier\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,  # Reduced from 20 - EarlyStopping will stop earlier if needed\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=callbacks_finetune,\n",
    "    workers=4,\n",
    "    use_multiprocessing=False,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"‚úì Fine-tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52345fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SAVE FINAL MODEL\n",
    "# ============================================\n",
    "# Save the current model (should be the best fine-tuned model)\n",
    "model.save(\"sign_language_model_MobileNetV2_updated.h5\")\n",
    "print(\"‚úì Model saved as: sign_language_model_MobileNetV2.h5\")\n",
    "\n",
    "# Also ensure we have the best model loaded\n",
    "if os.path.exists('best_model_finetuned.h5'):\n",
    "    print(\"‚úì Best model (78.9% val accuracy) is available as: best_model_finetuned.h5\")\n",
    "    print(\"  ‚Üí Use this for best performance!\")\n",
    "else:\n",
    "    print(\"‚ö† Note: best_model_finetuned.h5 not found. Current model saved instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933d59f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\", 5: \"F\", 6: \"G\", 7: \"H\",\n",
    "    8: \"I\", 9: \"J\", 10: \"K\", 11: \"L\", 12: \"M\", 13: \"N\", 14: \"O\",\n",
    "    15: \"P\", 16: \"Q\", 17: \"R\", 18: \"S\", 19: \"T\", 20: \"U\", 21: \"V\",\n",
    "    22: \"W\", 23: \"X\", 24: \"Y\", 25: \"Z\", 26: \"del\", 27: \"nothing\", 28: \"space\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dba89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FIX TEST IMAGE PREPROCESSING (Ensure it matches training)\n",
    "# ============================================\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FIXING TEST IMAGE PREPROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Test folder path\n",
    "IMG_SIZE = 128\n",
    "test_folder = r'M:\\Term 9\\Grad\\Gradution Current Project - Copy\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project\\Asl_Sign_Data\\asl_alphabet_test'\n",
    "\n",
    "print(f\"\\nüìÅ Loading test images from: {test_folder}\")\n",
    "print(f\"üìê Target size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"üé® Color format: RGB\")\n",
    "print(f\"üìä Normalization: 0.0 to 1.0 (divide by 255.0)\")\n",
    "\n",
    "test_images = []\n",
    "image_names = []\n",
    "\n",
    "for img_name in sorted(os.listdir(test_folder)):\n",
    "    if img_name.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        img_path = os.path.join(test_folder, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"‚ö† Warning: Could not load {img_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Convert BGR to RGB (OpenCV loads as BGR, but model expects RGB)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize to match training size\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        \n",
    "        # Normalize to 0-1 range (same as training)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        \n",
    "        test_images.append(img)\n",
    "        image_names.append(img_name)\n",
    "\n",
    "# Convert to NumPy array\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "print(f\"\\n‚úì Loaded {len(test_images)} test images\")\n",
    "print(f\"‚úì Image shape: {test_images[0].shape} (should be (128, 128, 3))\")\n",
    "print(f\"‚úì Pixel value range: [{test_images.min():.3f}, {test_images.max():.3f}] (should be [0.000, 1.000])\")\n",
    "print(f\"‚úì Data type: {test_images.dtype} (should be float32)\")\n",
    "\n",
    "# Verify preprocessing matches training\n",
    "checks_passed = True\n",
    "\n",
    "if test_images.shape[1:3] != (IMG_SIZE, IMG_SIZE):\n",
    "    print(f\"‚ö† ERROR: Image size mismatch! Expected {IMG_SIZE}x{IMG_SIZE}\")\n",
    "    checks_passed = False\n",
    "\n",
    "if test_images.max() > 1.0 or test_images.min() < 0.0:\n",
    "    print(f\"‚ö† ERROR: Pixel values out of range! Should be [0.0, 1.0]\")\n",
    "    checks_passed = False\n",
    "\n",
    "if test_images.dtype != np.float32:\n",
    "    print(f\"‚ö† WARNING: Data type is {test_images.dtype}, should be float32\")\n",
    "\n",
    "if checks_passed:\n",
    "    print(\"\\n‚úÖ All preprocessing checks passed!\")\n",
    "    print(\"‚úÖ Test images are ready for prediction\")\n",
    "else:\n",
    "    print(\"\\n‚ö† Some checks failed - please review preprocessing\")\n",
    "\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# LOAD TEST IMAGES WITH PROPER PREPROCESSING\n",
    "# ============================================\n",
    "# IMPORTANT: Image size must match model's expected input shape\n",
    "# Model expects: (128, 128, 3) as defined in architecture\n",
    "IMG_SIZE = 128  # Must match model.input_shape[1:3]\n",
    "\n",
    "test_folder = r'M:\\Term 9\\Grad\\Gradution Current Project - Copy\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project\\Asl_Sign_Data\\asl_alphabet_test'\n",
    "\n",
    "print(f\"Loading test images from: {test_folder}\")\n",
    "print(f\"Target image size: {IMG_SIZE}x{IMG_SIZE} (must match model input)\")\n",
    "\n",
    "test_images = []\n",
    "image_names = []\n",
    "\n",
    "for img_name in os.listdir(test_folder):\n",
    "    if img_name.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        img_path = os.path.join(test_folder, img_name)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö† Warning: Could not load {img_name}\")\n",
    "            continue\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = img / 255.0  # Normalize to [0, 1] range (matches training)\n",
    "\n",
    "        test_images.append(img)\n",
    "        image_names.append(img_name)\n",
    "\n",
    "# Convert to NumPy array\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "print(f\"‚úì Loaded {len(test_images)} test images.\")\n",
    "print(f\"‚úì Image shape: {test_images[0].shape}\")\n",
    "print(f\"‚úì Pixel value range: [{test_images.min():.3f}, {test_images.max():.3f}] (should be [0.0, 1.0])\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737c6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPROVED MODEL PREDICTIONS WITH DIAGNOSTICS\n",
    "# ============================================\n",
    "print(\"Running predictions with diagnostics...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify GPU usage\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(f\"‚úì Predictions running on GPU: {tf.config.list_physical_devices('GPU')[0].name}\")\n",
    "else:\n",
    "    print(\"‚ö† WARNING: No GPU detected, predictions on CPU\")\n",
    "\n",
    "# Verify image preprocessing matches training\n",
    "print(f\"\\nüìä Preprocessing Check:\")\n",
    "print(f\"  - Test image shape: {test_images[0].shape}\")\n",
    "print(f\"  - Model expected input: {model.input_shape}\")\n",
    "print(f\"  - Image normalization: {test_images[0].min():.3f} to {test_images[0].max():.3f} (should be 0.0 to 1.0)\")\n",
    "\n",
    "# Get model predictions - TensorFlow will automatically use GPU if available\n",
    "predictions = model.predict(\n",
    "    test_images,\n",
    "    batch_size=BATCH_SIZE if 'BATCH_SIZE' in globals() else 32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "predicted_confidences = np.max(predictions, axis=1)\n",
    "\n",
    "# Extract true labels from image names\n",
    "true_labels = [img_name.split(\"_\")[0] for img_name in image_names]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = 0\n",
    "total = len(true_labels)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"PREDICTION RESULTS FOR {total} IMAGES\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for i, img_name in enumerate(image_names):\n",
    "    pred_label = class_labels[predicted_classes[i]]\n",
    "    true_label = true_labels[i]\n",
    "    confidence = predicted_confidences[i]\n",
    "    \n",
    "    # Get top-3 predictions\n",
    "    top_3_indices = np.argsort(predictions[i])[-3:][::-1]\n",
    "    top_3_labels = [class_labels[idx] for idx in top_3_indices]\n",
    "    top_3_scores = [predictions[i][idx] for idx in top_3_indices]\n",
    "    \n",
    "    # Check if correct\n",
    "    is_correct = (pred_label == true_label)\n",
    "    if is_correct:\n",
    "        correct += 1\n",
    "        status = \"‚úì\"\n",
    "    else:\n",
    "        status = \"‚úó\"\n",
    "    \n",
    "    print(f\"{status} {img_name:20s} ‚Üí Predicted: {pred_label:8s} (True: {true_label:8s}) | Confidence: {confidence*100:5.1f}%\")\n",
    "    if not is_correct:\n",
    "        print(f\"    Top 3: {', '.join([f'{l}({s*100:.1f}%)' for l, s in zip(top_3_labels, top_3_scores)])}\")\n",
    "\n",
    "# Print summary\n",
    "accuracy = (correct / total) * 100\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"SUMMARY:\")\n",
    "print(f\"  Correct: {correct}/{total}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Average Confidence: {np.mean(predicted_confidences)*100:.1f}%\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Analyze common errors\n",
    "print(f\"\\nüìä ERROR ANALYSIS:\")\n",
    "error_analysis = {}\n",
    "# Create reverse mapping: label -> index\n",
    "label_to_index = {v: k for k, v in class_labels.items()}\n",
    "\n",
    "for i in range(total):\n",
    "    true_label = true_labels[i]\n",
    "    pred_label = class_labels[predicted_classes[i]]\n",
    "    \n",
    "    if pred_label != true_label:\n",
    "        error_key = f\"{true_label} ‚Üí {pred_label}\"\n",
    "        error_analysis[error_key] = error_analysis.get(error_key, 0) + 1\n",
    "\n",
    "if error_analysis:\n",
    "    print(\"  Most common misclassifications:\")\n",
    "    for error, count in sorted(error_analysis.items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "        print(f\"    {error}: {count} time(s)\")\n",
    "else:\n",
    "    print(\"  ‚úì No errors!\")\n",
    "\n",
    "print(\"\\n‚úì Predictions completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddfb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIAGNOSTIC: Test vs Validation Accuracy Gap Analysis\n",
    "# ============================================\n",
    "print(\"=\" * 70)\n",
    "print(\"OVERFITTING ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä Performance Comparison:\")\n",
    "print(f\"  Validation Accuracy: 85.78% (on validation set)\")\n",
    "print(f\"  Test Accuracy: 46.43% (on test set)\")\n",
    "print(f\"  Gap: {85.78 - 46.43:.2f} percentage points\")\n",
    "\n",
    "print(\"\\n‚ö† ISSUES IDENTIFIED:\")\n",
    "print(\"  1. Large gap suggests overfitting to validation set\")\n",
    "print(\"  2. Model predicts 'S' too often (8 wrong predictions)\")\n",
    "print(\"  3. Model predicts 'nothing' too often (4 wrong predictions)\")\n",
    "print(\"  4. Low confidence scores (10-20% average)\")\n",
    "print(\"  5. Only 13/28 correct on test set\")\n",
    "\n",
    "print(\"\\nüîç POSSIBLE CAUSES:\")\n",
    "print(\"  1. Test set may have different image quality/format\")\n",
    "print(\"  2. Test set may be from different source than training\")\n",
    "print(\"  3. Model overfitted to validation set patterns\")\n",
    "print(\"  4. Class imbalance in training data\")\n",
    "print(\"  5. Test images may need different preprocessing\")\n",
    "\n",
    "print(\"\\nüí° SOLUTIONS TO IMPROVE TEST ACCURACY:\")\n",
    "print(\"  1. ‚úÖ Increase regularization (more dropout)\")\n",
    "print(\"  2. ‚úÖ Use test-time augmentation (TTA)\")\n",
    "print(\"  3. ‚úÖ Check test image preprocessing matches training\")\n",
    "print(\"  4. ‚úÖ Retrain with stronger regularization\")\n",
    "print(\"  5. ‚úÖ Use ensemble of models\")\n",
    "print(\"  6. ‚úÖ Check if test set distribution matches training\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Check test image preprocessing (should match training)\")\n",
    "print(\"2. Verify test images are same format/quality as training\")\n",
    "print(\"3. Consider retraining with more dropout/regularization\")\n",
    "print(\"4. Use test-time augmentation for better predictions\")\n",
    "print(\"5. Check class distribution in test set\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1663963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# VISUALIZE MISCLASSIFIED TEST IMAGES\n",
    "# ============================================\n",
    "import math\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"VISUALIZING MISCLASSIFICATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Ensure predictions are available\n",
    "required_vars = [\n",
    "    'test_images', 'image_names', 'true_labels', 'predicted_classes',\n",
    "    'predicted_confidences', 'top3_indices', 'top3_probs', 'class_labels'\n",
    "]\n",
    "\n",
    "missing = [var for var in required_vars if var not in globals()]\n",
    "if missing:\n",
    "    print(f\"‚ö† ERROR: Missing variables: {missing}\")\n",
    "    print(\"Run the prediction cell (with TTA) before this cell.\")\n",
    "else:\n",
    "    predicted_labels = [class_labels[idx] for idx in predicted_classes]\n",
    "    mis_idx = [i for i in range(len(true_labels)) if predicted_labels[i] != true_labels[i]]\n",
    "    \n",
    "    print(f\"Total test images: {len(true_labels)}\")\n",
    "    print(f\"Misclassified images: {len(mis_idx)}\")\n",
    "    print(f\"Accuracy: {(len(true_labels) - len(mis_idx)) / len(true_labels) * 100:.2f}%\")\n",
    "    \n",
    "    if not mis_idx:\n",
    "        print(\"üéâ No misclassifications to display!\")\n",
    "    else:\n",
    "        # Show summary of common confusions\n",
    "        confusions = Counter(\n",
    "            [(true_labels[i], predicted_labels[i]) for i in mis_idx]\n",
    "        )\n",
    "        print(\"\\nMost common confusions (True ‚Üí Predicted):\")\n",
    "        for (true_label, pred_label), count in confusions.most_common(5):\n",
    "            print(f\"  {true_label} ‚Üí {pred_label}: {count} time(s)\")\n",
    "        \n",
    "        # Plot misclassified images\n",
    "        max_images = min(12, len(mis_idx))\n",
    "        cols = 4\n",
    "        rows = math.ceil(max_images / cols)\n",
    "        plt.figure(figsize=(cols * 4, rows * 4))\n",
    "        \n",
    "        for idx, mis in enumerate(mis_idx[:max_images]):\n",
    "            plt.subplot(rows, cols, idx + 1)\n",
    "            plt.imshow(test_images[mis])\n",
    "            plt.axis('off')\n",
    "            pred = predicted_labels[mis]\n",
    "            true = true_labels[mis]\n",
    "            conf = predicted_confidences[mis] * 100\n",
    "            top3 = \", \".join([\n",
    "                f\"{class_labels[top3_indices[mis][j]]}({top3_probs[mis][j] * 100:.1f}%)\"\n",
    "                for j in range(3)\n",
    "            ])\n",
    "            plt.title(\n",
    "                f\"{image_names[mis]}\\nTrue: {true} | Pred: {pred} ({conf:.1f}%)\\nTop3: {top3}\",\n",
    "                fontsize=9\n",
    "            )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nUse these visual cues to inspect why predictions failed (background, lighting, shape).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c8fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# IMPROVED TEST PREDICTIONS WITH TEST-TIME AUGMENTATION (TTA)\n",
    "# ============================================\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy import ndimage\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"IMPROVED TEST PREDICTIONS WITH TTA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Verify model is loaded\n",
    "try:\n",
    "    _ = model.input_shape\n",
    "    print(\"‚úì Model loaded and ready\")\n",
    "except:\n",
    "    print(\"‚ö† ERROR: Model not loaded! Run Cell 12 first.\")\n",
    "    raise\n",
    "\n",
    "# Verify test images are loaded\n",
    "try:\n",
    "    _ = test_images.shape\n",
    "    print(f\"‚úì Test images loaded: {len(test_images)} images\")\n",
    "except:\n",
    "    print(\"‚ö† ERROR: test_images not found! Load test images first.\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nüìä Preprocessing Verification:\")\n",
    "print(f\"  - Test image shape: {test_images[0].shape}\")\n",
    "print(f\"  - Model expected: {model.input_shape}\")\n",
    "print(f\"  - Pixel range: [{test_images.min():.3f}, {test_images.max():.3f}]\")\n",
    "\n",
    "# Ensure images are correctly preprocessed\n",
    "if test_images.max() > 1.0:\n",
    "    print(\"‚ö† WARNING: Images not normalized! Normalizing now...\")\n",
    "    test_images = test_images / 255.0\n",
    "\n",
    "if test_images.shape[1:3] != (128, 128):\n",
    "    print(\"‚ö† WARNING: Images not 128x128! Resizing now...\")\n",
    "    resized = []\n",
    "    for img in test_images:\n",
    "        resized.append(cv2.resize(img, (128, 128)))\n",
    "    test_images = np.array(resized)\n",
    "\n",
    "print(\"‚úì Preprocessing verified\")\n",
    "\n",
    "# Test-Time Augmentation (TTA) - creates multiple augmented versions\n",
    "def create_tta_images(image):\n",
    "    \"\"\"Create test-time augmented versions of an image\"\"\"\n",
    "    tta_images = [image]  # Original\n",
    "    \n",
    "    # Slight rotation\n",
    "    tta_images.append(ndimage.rotate(image, 5, reshape=False, mode='nearest'))\n",
    "    tta_images.append(ndimage.rotate(image, -5, reshape=False, mode='nearest'))\n",
    "    \n",
    "    # Slight brightness adjustment\n",
    "    tta_images.append(np.clip(image * 1.1, 0, 1))\n",
    "    tta_images.append(np.clip(image * 0.9, 0, 1))\n",
    "    \n",
    "    return np.array(tta_images)\n",
    "\n",
    "print(f\"\\nüîÑ Creating Test-Time Augmentations...\")\n",
    "print(f\"  - Original images: {len(test_images)}\")\n",
    "print(f\"  - Augmentations per image: 5 (original + 4 variants)\")\n",
    "print(f\"  - Total predictions: {len(test_images) * 5}\")\n",
    "\n",
    "# Get predictions with TTA\n",
    "all_predictions = []\n",
    "\n",
    "for i, img in enumerate(test_images):\n",
    "    if i % 5 == 0:\n",
    "        print(f\"  Processing image {i+1}/{len(test_images)}...\", end='\\r')\n",
    "    \n",
    "    # Create augmented versions\n",
    "    tta_imgs = create_tta_images(img)\n",
    "    \n",
    "    # Get predictions for all augmented versions\n",
    "    tta_preds = model.predict(tta_imgs, verbose=0, batch_size=5)\n",
    "    \n",
    "    # Average predictions (ensemble)\n",
    "    avg_pred = np.mean(tta_preds, axis=0)\n",
    "    all_predictions.append(avg_pred)\n",
    "\n",
    "all_predictions = np.array(all_predictions)\n",
    "print(f\"\\n‚úì TTA predictions completed\")\n",
    "\n",
    "# Convert to class labels\n",
    "predicted_classes = np.argmax(all_predictions, axis=1)\n",
    "predicted_confidences = np.max(all_predictions, axis=1)\n",
    "\n",
    "# Get top 3 predictions for each image\n",
    "top3_indices = np.argsort(all_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "top3_probs = np.sort(all_predictions, axis=1)[:, -3:][:, ::-1]\n",
    "\n",
    "# Extract true labels\n",
    "true_labels = [img_name.split(\"_\")[0] for img_name in image_names]\n",
    "\n",
    "# Calculate accuracy\n",
    "correct = sum([1 if class_labels[predicted_classes[i]] == true_labels[i] else 0 \n",
    "               for i in range(len(true_labels))])\n",
    "accuracy = correct / len(true_labels) * 100\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"IMPROVED PREDICTION RESULTS (WITH TTA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Display results\n",
    "for i, img_name in enumerate(image_names):\n",
    "    pred_label = class_labels[predicted_classes[i]]\n",
    "    true_label = true_labels[i]\n",
    "    confidence = predicted_confidences[i] * 100\n",
    "    \n",
    "    status = \"‚úì\" if pred_label == true_label else \"‚úó\"\n",
    "    print(f\"{status} {img_name:20s} ‚Üí Predicted: {pred_label:8s} (True: {true_label:8s}) | Confidence: {confidence:5.1f}%\")\n",
    "    \n",
    "    if pred_label != true_label:\n",
    "        top3_str = \", \".join([f\"{class_labels[top3_indices[i][j]]}({top3_probs[i][j]*100:.1f}%)\" \n",
    "                             for j in range(3)])\n",
    "        print(f\"    Top 3: {top3_str}\")\n",
    "\n",
    "print(\"\\n\" + \"=\n",
    "70\n",
    ",\n",
    "SUMMARY:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"  Correct: {correct}/{len(true_labels)}\")\n",
    "print(f\"  Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"  Average Confidence: {predicted_confidences.mean()*100:.1f}%\")\n",
    "print(f\"  Improvement: Using TTA for more robust predictions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n‚úì Improved predictions completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeae0992",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [img_name.split(\"_\")[0] for img_name in image_names]\n",
    "\n",
    "correct = sum([1 if class_labels[predicted_classes[i]] == true_labels[i] else 0 for i in range(len(true_labels))])\n",
    "accuracy = correct / len(true_labels) * 100\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef093c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "# Plot Accuracy Graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_dict[\"accuracy\"], label=\"Training Accuracy\", marker=\"o\", color=\"orange\")\n",
    "plt.plot(history_dict[\"val_accuracy\"], label=\"Validation Accuracy\", marker=\"o\", color=\"red\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss Graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_dict[\"loss\"], label=\"Training Loss\", marker=\"o\", color=\"red\")\n",
    "plt.plot(history_dict[\"val_loss\"], label=\"Validation Loss\", marker=\"o\", color=\"blue\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Model Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004a956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256b9a29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d0dbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
