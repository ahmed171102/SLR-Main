{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "346a5124",
   "metadata": {},
   "source": [
    "# Mediapipe Optimized Training Notebook\n",
    "\n",
    "This notebook demonstrates an optimized pipeline for sign language recognition using Mediapipe keypoints and a neural network classifier. It features:\n",
    "- GPU detection/configuration\n",
    "- Mixed precision logic (enabled if supported)\n",
    "- Batch size and memory tips\n",
    "- Progress bars for long steps\n",
    "- Clear training/evaluation summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5891d8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1: Install and Import Required Libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import mixed_precision\n",
    "import mediapipe as mp\n",
    "print('‚úì All libraries imported successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4678abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: GPU Detection and Configuration\n",
    "print('='*60)\n",
    "print('üîç GPU DETECTION & CONFIGURATION')\n",
    "print('='*60)\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f'Found GPUs: {gpus}')\n",
    "USE_GPU = False\n",
    "DEVICE = '/CPU:0'\n",
    "if gpus:\n",
    "    try:\n",
    "        for g in gpus:\n",
    "            tf.config.experimental.set_memory_growth(g, True)\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        USE_GPU = True\n",
    "        DEVICE = '/GPU:0'\n",
    "        print(f'‚úÖ GPU configured: {gpus[0]}')\n",
    "    except RuntimeError as e:\n",
    "        print('‚ö†Ô∏è  GPU config error:', e)\n",
    "# Mixed precision (optional and beneficial on modern GPUs)\n",
    "try:\n",
    "    if USE_GPU:\n",
    "        policy = mixed_precision.Policy('mixed_float16')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print('‚ö° Mixed precision enabled:', policy.name)\n",
    "except Exception as e:\n",
    "    print('‚ö†Ô∏è  Mixed precision not enabled:', e)\n",
    "print('Configuration complete. Using device:', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bace8f0f",
   "metadata": {},
   "source": [
    "## Batch Size and Memory Tips\n",
    "- If you get 'Out of Memory' errors, reduce `BATCH_SIZE` or disable mixed precision.\n",
    "- Monitor GPU usage with `nvidia-smi -l 1` in a separate terminal.\n",
    "- Close other GPU-intensive applications during training for best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3: Extract Mediapipe Keypoints from Dataset\n",
    "# Update DATASET_DIR to your dataset location\n",
    "DATASET_DIR = r'M:\\Term 9\\Grad\\Main\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project Main\\ASL Letter (English)\\Datasets\\Asl_Sign_Data\\asl_alphabet_train'\n",
    "CSV_PATH = 'asl_mediapipe_keypoints_dataset_optimized.csv'\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print('Dataset CSV already exists:', CSV_PATH)\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f'Samples loaded: {len(df)}')\n",
    "else:\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7)\n",
    "    landmark_data = []\n",
    "    labels = []\n",
    "    class_labels = sorted([d for d in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, d))])\n",
    "    all_images = []\n",
    "    for label in class_labels:\n",
    "        folder = os.path.join(DATASET_DIR, label)\n",
    "        files = [f for f in os.listdir(folder) if f.lower().endswith(('.png','.jpg','.jpeg'))]\n",
    "        for f in files:\n",
    "            all_images.append((label, os.path.join(folder, f)))\n",
    "    print(f'Found {len(all_images)} images across {len(class_labels)} classes')\n",
    "    start = pd.Timestamp.now()\n",
    "    processed = 0\n",
    "    skipped = 0\n",
    "    for label, img_path in tqdm(all_images, desc='Extracting keypoints'):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(img_rgb)\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            lm = np.array([[p.x, p.y, p.z] for p in hand_landmarks.landmark]).flatten()\n",
    "            landmark_data.append(lm)\n",
    "            labels.append(label)\n",
    "            processed += 1\n",
    "        else:\n",
    "            skipped += 1\n",
    "    duration = (pd.Timestamp.now() - start).total_seconds()\n",
    "    if len(landmark_data) == 0:\n",
    "        print('No landmarks extracted. Check dataset or MediaPipe settings.')\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        df = pd.DataFrame(landmark_data)\n",
    "        df['label'] = labels\n",
    "        df.to_csv(CSV_PATH, index=False)\n",
    "        print('Saved keypoints to', CSV_PATH)\n",
    "        print(f'Processed: {processed}, Skipped: {skipped}, Time: {duration:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b4c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4: Preprocess and Split Data\n",
    "if not df.empty:\n",
    "    X = df.iloc[:, :-1].astype('float32').values\n",
    "    y = df['label'].values\n",
    "    encoder = LabelEncoder()\n",
    "    y_enc = encoder.fit_transform(y)\n",
    "    num_classes = len(encoder.classes_)\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(X, y_enc, test_size=0.2, random_state=42, stratify=y_enc)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full)\n",
    "    y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "    y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "    print('Train/Val/Test sizes:', X_train.shape[0], X_val.shape[0], X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2bc439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Build and Train MLP Model\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "def make_dataset(features, labels, batch_size, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if training:\n",
    "        buffer = min(len(features), 10000)\n",
    "        ds = ds.shuffle(buffer_size=buffer, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "BATCH_SIZE = 256 if USE_GPU else 64\n",
    "train_ds = make_dataset(X_train, y_train, BATCH_SIZE, training=True)\n",
    "val_ds = make_dataset(X_val, y_val, BATCH_SIZE, training=False)\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-4), input_shape=(X_train.shape[1],)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(128, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(1e-4)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Dense(64, activation='relu', kernel_initializer='he_normal'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax', dtype='float32')\n",
    "    ])\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    print('Model summary:')\n",
    "    model.summary()\n",
    "    print(f'Model will train on device: {DEVICE}')\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint('mediapipe_mlp_model_best.h5', monitor='val_accuracy', save_best_only=True, verbose=1),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1),\n",
    "]\n",
    "\n",
    "print('Training with batch size:', BATCH_SIZE)\n",
    "start = pd.Timestamp.now()\n",
    "with tf.device(DEVICE):\n",
    "    history = model.fit(train_ds, validation_data=val_ds, epochs=20, callbacks=callbacks, verbose=1)\n",
    "duration = (pd.Timestamp.now() - start).total_seconds()\n",
    "print(f'Training finished in {duration:.2f}s')\n",
    "model.save('mediapipe_mlp_model.h5')\n",
    "print('Saved model: mediapipe_mlp_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae730506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 6: Evaluate Model\n",
    "if not df.empty:\n",
    "    eval_batch = 256 if USE_GPU else 128\n",
    "    test_ds = make_dataset(X_test, y_test, eval_batch, training=False)\n",
    "    with tf.device(DEVICE):\n",
    "        loss, acc = model.evaluate(test_ds, verbose=1)\n",
    "    print(f'Test Loss: {loss:.4f}  Test Accuracy: {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434db7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Real-Time Inference (Webcam)\n",
    "encoder = LabelEncoder()\n",
    "df_labels = pd.read_csv(CSV_PATH)\n",
    "encoder.fit(df_labels['label'])\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "cap = cv2.VideoCapture(0)\n",
    "predicted_sentence = ''\n",
    "mlp_model = tf.keras.models.load_model('mediapipe_mlp_model.h5')\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "            lm = np.array([[p.x, p.y, p.z] for p in hand_landmarks.landmark]).flatten().reshape(1, -1)\n",
    "            input_tensor = tf.cast(lm, tf.float32)\n",
    "            with tf.device(DEVICE):\n",
    "                pred = mlp_model.predict(input_tensor, verbose=0)\n",
    "            pred_class = np.argmax(pred)\n",
    "            label = encoder.inverse_transform([pred_class])[0]\n",
    "            if label == 'SPACE':\n",
    "                predicted_sentence += ' '\n",
    "            elif label == 'DELETE':\n",
    "                predicted_sentence = predicted_sentence[:-1]\n",
    "            elif label == 'NOTHING':\n",
    "                pass\n",
    "            else:\n",
    "                predicted_sentence += label\n",
    "    h, w, _ = frame.shape\n",
    "    cv2.rectangle(frame, (0, h-60), (w, h), (0, 0, 0), -1)\n",
    "    cv2.putText(frame, predicted_sentence, (10, h-20), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
    "    cv2.imshow('Optimized Sign Prediction (MediaPipe + MLP)', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
