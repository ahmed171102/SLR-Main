{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Libraries imported successfully\n",
      "\n",
      "ðŸ“· Camera source: webcam\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from collections import deque\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from threading import Thread\n",
    "import queue\n",
    "\n",
    "# ============================================\n",
    "# ðŸ“± CAMERA SOURCE CONFIGURATION\n",
    "# ============================================\n",
    "# Choose your camera source:\n",
    "# - \"webcam\"    : Use built-in/USB webcam (default)\n",
    "# - \"phone_ip\"  : Use phone camera via IP Webcam app (Android)\n",
    "# - \"droidcam\"  : Use DroidCam app (connects via USB or WiFi)\n",
    "\n",
    "CAMERA_SOURCE = \"webcam\"  # â† CHANGE THIS to \"phone_ip\" or \"droidcam\"\n",
    "\n",
    "# ðŸ“± Phone Camera Settings (for IP Webcam app - Android)\n",
    "PHONE_IP = \"172.24.148.110\"  # â† Replace with YOUR phone's IP address\n",
    "PHONE_PORT = \"4747\"         # Default port for IP Webcam app\n",
    "\n",
    "# Alternative: DroidCam settings\n",
    "DROIDCAM_IP = \"172.24.148.110\"  # â† Replace with your phone's IP\n",
    "DROIDCAM_PORT = \"4747\"          # Default DroidCam port\n",
    "\n",
    "# USB camera index (for webcam)\n",
    "USB_CAMERA_INDEX = 0  # Try 0, 1, 2 if webcam doesn't work\n",
    "\n",
    "def get_camera_url():\n",
    "    \"\"\"Returns the appropriate camera URL based on CAMERA_SOURCE setting.\"\"\"\n",
    "    if CAMERA_SOURCE == \"phone_ip\":\n",
    "        return f\"http://{PHONE_IP}:{PHONE_PORT}/video\"\n",
    "    elif CAMERA_SOURCE == \"droidcam\":\n",
    "        return f\"http://{DROIDCAM_IP}:{DROIDCAM_PORT}/video\"\n",
    "    else:\n",
    "        return USB_CAMERA_INDEX\n",
    "\n",
    "print(\"âœ“ Libraries imported successfully\")\n",
    "print(f\"\\nðŸ“· Camera source: {CAMERA_SOURCE}\")\n",
    "if CAMERA_SOURCE == \"phone_ip\":\n",
    "    print(f\"   URL: http://{PHONE_IP}:{PHONE_PORT}/video\")\n",
    "    print(\"   ðŸ“± Make sure IP Webcam app is running on your phone!\")\n",
    "elif CAMERA_SOURCE == \"droidcam\":\n",
    "    print(f\"   URL: http://{DROIDCAM_IP}:{DROIDCAM_PORT}/video\")\n",
    "    print(\"   ðŸ“± Make sure DroidCam is running!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Testing camera connection...\n",
      "   Connecting to: 0\n",
      "âœ“ Camera connected!\n",
      "   âœ“ Frame 1: (480, 640, 3)\n",
      "   âœ“ Frame 2: (480, 640, 3)\n",
      "   âœ“ Frame 3: (480, 640, 3)\n",
      "âœ“ Camera test complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class OptimizedCamera:\n",
    "    \"\"\"\n",
    "    Optimized camera capture with threading for better performance.\n",
    "    Supports webcam, phone IP camera, and DroidCam.\n",
    "    \"\"\"\n",
    "    def __init__(self, source, buffer_size=2):\n",
    "        self.source = source\n",
    "        self.cap = None\n",
    "        self.frame_queue = queue.Queue(maxsize=buffer_size)\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.last_frame = None\n",
    "        self.fps = 0\n",
    "        self.frame_count = 0\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start the camera capture thread.\"\"\"\n",
    "        self.cap = cv2.VideoCapture(self.source)\n",
    "        \n",
    "        if not self.cap.isOpened():\n",
    "            raise ConnectionError(f\"âŒ Cannot connect to camera: {self.source}\")\n",
    "        \n",
    "        # Optimize capture settings for USB webcam\n",
    "        if isinstance(self.source, int):\n",
    "            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "            self.cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)\n",
    "        \n",
    "        self.running = True\n",
    "        self.thread = Thread(target=self._capture_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "        print(f\"âœ“ Camera started: {self.source}\")\n",
    "        return self\n",
    "    \n",
    "    def _capture_loop(self):\n",
    "        \"\"\"Background thread for capturing frames.\"\"\"\n",
    "        while self.running:\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                self.last_frame = frame\n",
    "                self.frame_count += 1\n",
    "                \n",
    "                elapsed = time.time() - self.start_time\n",
    "                if elapsed > 1:\n",
    "                    self.fps = self.frame_count / elapsed\n",
    "                    self.frame_count = 0\n",
    "                    self.start_time = time.time()\n",
    "                \n",
    "                if not self.frame_queue.full():\n",
    "                    self.frame_queue.put(frame)\n",
    "                else:\n",
    "                    try:\n",
    "                        self.frame_queue.get_nowait()\n",
    "                        self.frame_queue.put(frame)\n",
    "                    except queue.Empty:\n",
    "                        pass\n",
    "            else:\n",
    "                time.sleep(0.001)\n",
    "    \n",
    "    def read(self):\n",
    "        \"\"\"Get the latest frame (non-blocking).\"\"\"\n",
    "        if self.last_frame is not None:\n",
    "            return True, self.last_frame.copy()\n",
    "        return False, None\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop the camera capture.\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=1.0)\n",
    "        if self.cap:\n",
    "            self.cap.release()\n",
    "        print(\"âœ“ Camera stopped\")\n",
    "    \n",
    "    def get_fps(self):\n",
    "        return self.fps\n",
    "    \n",
    "    def isOpened(self):\n",
    "        return self.cap is not None and self.cap.isOpened()\n",
    "\n",
    "\n",
    "def test_camera_connection():\n",
    "    \"\"\"Test camera connection.\"\"\"\n",
    "    print(\"ðŸ” Testing camera connection...\")\n",
    "    camera_url = get_camera_url()\n",
    "    print(f\"   Connecting to: {camera_url}\")\n",
    "    \n",
    "    try:\n",
    "        cap = cv2.VideoCapture(camera_url)\n",
    "        \n",
    "        if not cap.isOpened():\n",
    "            print(\"âŒ Failed to connect!\")\n",
    "            if CAMERA_SOURCE == \"phone_ip\":\n",
    "                print(\"\\nðŸ“± Troubleshooting for IP Webcam:\")\n",
    "                print(\"   1. Open IP Webcam app on your phone\")\n",
    "                print(\"   2. Scroll down and tap 'Start server'\")\n",
    "                print(\"   3. Note the IP address shown\")\n",
    "                print(\"   4. Update PHONE_IP in Cell 1\")\n",
    "                print(\"   5. Make sure phone and PC are on same WiFi\")\n",
    "            elif CAMERA_SOURCE == \"droidcam\":\n",
    "                print(\"\\nðŸ“± Troubleshooting for DroidCam:\")\n",
    "                print(\"   1. Install DroidCam on phone AND PC\")\n",
    "                print(\"   2. Open app on phone, note the IP\")\n",
    "                print(\"   3. Update DROIDCAM_IP in Cell 1\")\n",
    "            return False\n",
    "        \n",
    "        print(\"âœ“ Camera connected!\")\n",
    "        for i in range(3):\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                print(f\"   âœ“ Frame {i+1}: {frame.shape}\")\n",
    "        \n",
    "        cap.release()\n",
    "        print(\"âœ“ Camera test complete!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run the test\n",
    "test_camera_connection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ GPU configured: 1 GPU(s) available\n",
      "âœ“ MobileNetV2 model loaded\n",
      "âœ“ MediaPipe MLP model loaded\n",
      "âœ“ Encoder loaded\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ§  MODEL LOADING (Optimized)\n",
    "# ============================================\n",
    "\n",
    "# Enable GPU memory growth to prevent OOM errors\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"âœ“ GPU configured: {len(gpus)} GPU(s) available\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"âš ï¸ GPU config error: {e}\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ No GPU detected, using CPU\")\n",
    "\n",
    "# Load models with error handling\n",
    "mobilenet_model = None\n",
    "mlp_model = None\n",
    "\n",
    "try:\n",
    "    if os.path.exists(\"sign_language_model_MobileNetV2.h5\"):\n",
    "        mobilenet_model = tf.keras.models.load_model(\"sign_language_model_MobileNetV2.h5\")\n",
    "        print(\"âœ“ MobileNetV2 model loaded\")\n",
    "    else:\n",
    "        print(\"âš ï¸ MobileNetV2 model not found (optional)\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ MobileNetV2 load error: {e}\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(\"asl_mediapipe_mlp_model.h5\"):\n",
    "        mlp_model = tf.keras.models.load_model(\"asl_mediapipe_mlp_model.h5\")\n",
    "        print(\"âœ“ MediaPipe MLP model loaded\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"asl_mediapipe_mlp_model.h5 not found\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ MLP model error: {e}\")\n",
    "\n",
    "# Load encoder\n",
    "encoder = None\n",
    "try:\n",
    "    if os.path.exists(\"asl_mediapipe_keypoints_dataset.csv\"):\n",
    "        df = pd.read_csv(\"asl_mediapipe_keypoints_dataset.csv\")\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(df[\"label\"])\n",
    "        del df  # Free memory\n",
    "        print(\"âœ“ Encoder loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Encoder error: {e}\")\n",
    "\n",
    "# Class labels\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
    "                'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "if mlp_model is None or encoder is None:\n",
    "    print(\"\\nâš ï¸  Warning: Essential models failed to load. Camera detection will not work.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the mediapipe keypoints and defining the class labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Settings configured:\n",
      "   Display: 1280x720 (Resizable: True)\n",
      "   Target FPS: 30\n",
      "   Confidence threshold: 0.92\n",
      "   Cooldown time: 3.0s\n",
      "   Buffer size: 7 frames\n",
      "   Buffer threshold: 5 matches required\n",
      "   Hold time: 0.5s\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# âš™ï¸ OPTIMIZED SETTINGS\n",
    "# ============================================\n",
    "\n",
    "# Display settings\n",
    "DISPLAY_WIDTH = 1280\n",
    "DISPLAY_HEIGHT = 720\n",
    "WINDOW_RESIZABLE = True  # Allow window resizing\n",
    "\n",
    "# Performance settings\n",
    "PROCESS_EVERY_N_FRAMES = 1     # 1 = process all, 2 = skip every other frame\n",
    "TARGET_FPS = 30                 # Target display FPS\n",
    "SKIP_PREDICTION_NO_HAND = True  # Skip prediction when no hand detected\n",
    "\n",
    "# MediaPipe settings (optimized for speed)\n",
    "MP_MIN_DETECTION_CONFIDENCE = 0.6\n",
    "MP_MIN_TRACKING_CONFIDENCE = 0.6\n",
    "MP_MAX_HANDS = 1\n",
    "\n",
    "# ============================================\n",
    "# LETTER COMMIT STRATEGY (commit-once-then-wait)\n",
    "# ============================================\n",
    "# After a letter is accepted, the system WAITS until:\n",
    "#   1. The hand disappears from the frame, OR\n",
    "#   2. A DIFFERENT stable sign is held\n",
    "# This completely prevents repeated letters like \"mmmmmmooooooccccc\"\n",
    "\n",
    "STABILIZATION_WINDOW_SIZE = 10  # Buffer size for majority vote\n",
    "STABILIZATION_THRESHOLD = 7    # Minimum matches in buffer (7/10 = 70% majority)\n",
    "MIN_CONFIDENCE = 0.70          # Minimum confidence to consider a prediction\n",
    "HOLD_TIME_REQUIRED = 0.8       # Seconds to hold sign before accepting\n",
    "\n",
    "# Bounding box expansion (for MobileNet)\n",
    "HEIGHT_EXPAND = 200\n",
    "WIDTH_EXPAND = 130\n",
    "\n",
    "print(\"âš™ï¸ Settings configured:\")\n",
    "print(f\"   Display: {DISPLAY_WIDTH}x{DISPLAY_HEIGHT} (Resizable: {WINDOW_RESIZABLE})\")\n",
    "print(f\"   Target FPS: {TARGET_FPS}\")\n",
    "print(f\"   Confidence threshold: {MIN_CONFIDENCE}\")\n",
    "print(f\"   Stability: {STABILIZATION_THRESHOLD}/{STABILIZATION_WINDOW_SIZE} frames majority required\")\n",
    "print(f\"   Hold time: {HOLD_TIME_REQUIRED}s before accepting\")\n",
    "print(f\"   Strategy: commit-once, then wait for hand-drop or sign-change\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mediapipe for hand keypoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MediaPipe Hands initialized\n",
      "   - Detection confidence: 0.6\n",
      "   - Tracking confidence: 0.6\n",
      "   - Max hands: 1\n"
     ]
    }
   ],
   "source": [
    "# Initialize MediaPipe with optimized settings\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=MP_MAX_HANDS,\n",
    "    min_detection_confidence=MP_MIN_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MP_MIN_TRACKING_CONFIDENCE\n",
    ")\n",
    "\n",
    "print(\"âœ“ MediaPipe Hands initialized\")\n",
    "print(f\"   - Detection confidence: {MP_MIN_DETECTION_CONFIDENCE}\")\n",
    "print(f\"   - Tracking confidence: {MP_MIN_TRACKING_CONFIDENCE}\")\n",
    "print(f\"   - Max hands: {MP_MAX_HANDS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the height and width of the box, where mobilenet is to be predicting classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Feature extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "# Pre-allocate arrays for better performance\n",
    "_landmark_buffer = np.zeros((21, 3), dtype=np.float32)\n",
    "\n",
    "def extract_landmark_features_fast(hand_landmarks, is_right_hand=False):\n",
    "    \"\"\"Optimized landmark extraction with pre-allocated buffer.\n",
    "    \n",
    "    No mirroring is applied â€” training data was recorded without flipping,\n",
    "    so inference landmarks must be used as-is regardless of handedness.\n",
    "    \"\"\"\n",
    "    global _landmark_buffer\n",
    "    \n",
    "    for i, lm in enumerate(hand_landmarks.landmark):\n",
    "        _landmark_buffer[i, 0] = lm.x\n",
    "        _landmark_buffer[i, 1] = lm.y\n",
    "        _landmark_buffer[i, 2] = lm.z\n",
    "    \n",
    "    return _landmark_buffer.flatten().reshape(1, -1)\n",
    "\n",
    "def get_hand_bounding_box(hand_landmarks, frame_shape):\n",
    "    \"\"\"Get expanded bounding box for hand region.\"\"\"\n",
    "    h, w = frame_shape[:2]\n",
    "    \n",
    "    x_coords = [lm.x * w for lm in hand_landmarks.landmark]\n",
    "    y_coords = [lm.y * h for lm in hand_landmarks.landmark]\n",
    "    \n",
    "    x_min = max(0, int(min(x_coords) - WIDTH_EXPAND))\n",
    "    y_min = max(0, int(min(y_coords) - HEIGHT_EXPAND))\n",
    "    x_max = min(w, int(max(x_coords) + WIDTH_EXPAND))\n",
    "    y_max = min(h, int(max(y_coords) + HEIGHT_EXPAND))\n",
    "    \n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "# Keep original function for compatibility\n",
    "def extract_landmark_features(hand_landmarks, handedness):\n",
    "    \"\"\"Original landmark extraction (for backward compatibility).\"\"\"\n",
    "    is_right = handedness.classification[0].label == \"Right\"\n",
    "    return extract_landmark_features_fast(hand_landmarks, is_right)\n",
    "\n",
    "print(\"âœ“ Feature extraction functions defined (no mirroring â€” matches training data)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Logic for comparision of confidence score of both models - MobileNet V2 and Mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (4036012915.py, line 209)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[39], line 209\u001b[1;36m\u001b[0m\n\u001b[1;33m    filename = f\"screenshot_{int(time.time())}.png\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def run_sign_recognition():\n",
    "    \"\"\"Main sign language recognition loop â€” commit-once-then-wait strategy.\n",
    "    \n",
    "    How it works:\n",
    "    1. Predictions go into a stability buffer (majority vote).\n",
    "    2. Once a letter is stable + held long enough â†’ it is committed ONCE.\n",
    "    3. After committing, the system LOCKS that label and waits for either:\n",
    "       a) The hand leaves the frame (resets everything), OR\n",
    "       b) A DIFFERENT stable sign emerges (unlocks and starts tracking the new sign).\n",
    "    This completely prevents \"mmmmmooooooccccc\" repetition.\n",
    "    \"\"\"\n",
    "    \n",
    "    if mlp_model is None or encoder is None:\n",
    "        print(\"âŒ Models not loaded. Cannot run recognition.\")\n",
    "        return\n",
    "    \n",
    "    # Initialize camera\n",
    "    camera_url = get_camera_url()\n",
    "    print(f\"ðŸ“· Connecting to camera: {camera_url}\")\n",
    "    \n",
    "    try:\n",
    "        camera = OptimizedCamera(camera_url).start()\n",
    "    except ConnectionError as e:\n",
    "        print(e)\n",
    "        return\n",
    "    \n",
    "    time.sleep(1.0)  # Wait for camera to warm up\n",
    "    \n",
    "    # State variables\n",
    "    predicted_sentence = \"\"\n",
    "    stabilization_buffer = deque(maxlen=STABILIZATION_WINDOW_SIZE)\n",
    "    frame_count = 0\n",
    "    fps_display = 0\n",
    "    fps_start_time = time.time()\n",
    "    \n",
    "    # --- Commit-once-then-wait state ---\n",
    "    committed_label = None       # The label we already committed (locked)\n",
    "    current_sign_label = None    # The current stable sign being tracked\n",
    "    current_sign_start = None    # When we first saw this stable sign\n",
    "    waiting_for_change = False   # True after committing â†’ waiting for hand-drop or new sign\n",
    "    \n",
    "    # Create resizable window\n",
    "    window_name = \"Sign Language Recognition\"\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    if WINDOW_RESIZABLE:\n",
    "        cv2.resizeWindow(window_name, DISPLAY_WIDTH, DISPLAY_HEIGHT)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ¤Ÿ SIGN LANGUAGE RECOGNITION STARTED\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"Controls:\")\n",
    "    print(\"  'q' - Quit\")\n",
    "    print(\"  'c' - Clear sentence\")\n",
    "    print(\"  's' - Save screenshot\")\n",
    "    print(\"  Window is RESIZABLE - drag corners to resize!\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = camera.read()\n",
    "            if not ret:\n",
    "                continue\n",
    "            \n",
    "            frame_count += 1\n",
    "            \n",
    "            if PROCESS_EVERY_N_FRAMES > 1 and frame_count % PROCESS_EVERY_N_FRAMES != 0:\n",
    "                continue\n",
    "            \n",
    "            # Process unflipped frame with MediaPipe (matches training data)\n",
    "            frame = cv2.resize(frame, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            rgb_frame.flags.writeable = False\n",
    "            results = hands.process(rgb_frame)\n",
    "            rgb_frame.flags.writeable = True\n",
    "            \n",
    "            current_label = None\n",
    "            current_confidence = 0\n",
    "            display_status = \"\"\n",
    "            status_color = (200, 200, 200)\n",
    "            \n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    mp_drawing.draw_landmarks(\n",
    "                        frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                        mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                        mp_drawing_styles.get_default_hand_connections_style()\n",
    "                    )\n",
    "                    \n",
    "                    is_right = handedness.classification[0].label == \"Right\"\n",
    "                    \n",
    "                    # MLP Prediction\n",
    "                    features = extract_landmark_features_fast(hand_landmarks, is_right)\n",
    "                    mlp_pred = mlp_model.predict(features, verbose=0)\n",
    "                    mlp_idx = np.argmax(mlp_pred)\n",
    "                    mlp_conf = mlp_pred[0][mlp_idx]\n",
    "                    mlp_label = encoder.inverse_transform([mlp_idx])[0]\n",
    "                    \n",
    "                    current_label = mlp_label\n",
    "                    current_confidence = mlp_conf\n",
    "                    \n",
    "                    # MobileNet prediction (if model available)\n",
    "                    if mobilenet_model is not None:\n",
    "                        x1, y1, x2, y2 = get_hand_bounding_box(hand_landmarks, frame.shape)\n",
    "                        hand_crop = frame[y1:y2, x1:x2]\n",
    "                        \n",
    "                        if hand_crop.size > 0:\n",
    "                            hand_resized = cv2.resize(hand_crop, (128, 128))\n",
    "                            hand_input = np.expand_dims(hand_resized, axis=0) / 255.0\n",
    "                            \n",
    "                            mob_pred = mobilenet_model.predict(hand_input, verbose=0)\n",
    "                            mob_idx = np.argmax(mob_pred)\n",
    "                            mob_conf = mob_pred[0][mob_idx]\n",
    "                            mob_label = class_labels[mob_idx]\n",
    "                            \n",
    "                            if mob_conf > current_confidence:\n",
    "                                current_label = mob_label\n",
    "                                current_confidence = mob_conf\n",
    "                        \n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # --- COMMIT-ONCE-THEN-WAIT LOGIC ---\n",
    "                    \n",
    "                    # Skip low-confidence predictions entirely\n",
    "                    if current_confidence < MIN_CONFIDENCE:\n",
    "                        display_status = f\"{current_label} ({current_confidence:.0%}) Low conf\"\n",
    "                        status_color = (0, 100, 255)  # Orange\n",
    "                        break\n",
    "                    \n",
    "                    # Add to stability buffer\n",
    "                    stabilization_buffer.append(current_label)\n",
    "                    \n",
    "                    # Check majority vote in buffer\n",
    "                    buffer_count = stabilization_buffer.count(current_label)\n",
    "                    is_stable = (buffer_count >= STABILIZATION_THRESHOLD and \n",
    "                                 len(stabilization_buffer) == STABILIZATION_WINDOW_SIZE)\n",
    "                    \n",
    "                    if not is_stable:\n",
    "                        # Not stable yet â€” show progress\n",
    "                        progress = buffer_count / STABILIZATION_THRESHOLD * 100\n",
    "                        display_status = f\"{current_label} ({current_confidence:.0%}) Stabilizing {progress:.0f}%\"\n",
    "                        status_color = (0, 255, 255)  # Yellow\n",
    "                        break\n",
    "                    \n",
    "                    # Stable prediction achieved\n",
    "                    now = time.time()\n",
    "                    \n",
    "                    # If we're waiting after a commit, check if the sign changed\n",
    "                    if waiting_for_change:\n",
    "                        if current_label == committed_label:\n",
    "                            # Same sign still held â€” keep waiting\n",
    "                            display_status = f\"{current_label} ({current_confidence:.0%}) âœ“ Committed - change sign for next\"\n",
    "                            status_color = (255, 200, 0)  # Cyan\n",
    "                            break\n",
    "                        else:\n",
    "                            # Different sign! Unlock and start tracking the new one\n",
    "                            waiting_for_change = False\n",
    "                            committed_label = None\n",
    "                            current_sign_label = current_label\n",
    "                            current_sign_start = now\n",
    "                    \n",
    "                    # Track hold time for current sign\n",
    "                    if current_label != current_sign_label:\n",
    "                        # New sign â€” start hold timer\n",
    "                        current_sign_label = current_label\n",
    "                        current_sign_start = now\n",
    "                    \n",
    "                    hold_duration = now - current_sign_start if current_sign_start else 0\n",
    "                    \n",
    "                    if hold_duration < HOLD_TIME_REQUIRED:\n",
    "                        # Still need to hold longer\n",
    "                        hold_pct = hold_duration / HOLD_TIME_REQUIRED * 100\n",
    "                        display_status = f\"{current_label} ({current_confidence:.0%}) Hold: {hold_pct:.0f}%\"\n",
    "                        status_color = (0, 255, 255)  # Yellow\n",
    "                        break\n",
    "                    \n",
    "                    # --- COMMIT THE LETTER ---\n",
    "                    if current_label not in [\"nothing\", \"del\", \"space\"]:\n",
    "                        predicted_sentence += current_label\n",
    "                    elif current_label == \"space\":\n",
    "                        if not predicted_sentence.endswith(\" \"):\n",
    "                            predicted_sentence += \" \"\n",
    "                    elif current_label == \"del\" and predicted_sentence:\n",
    "                        predicted_sentence = predicted_sentence[:-1]\n",
    "                    \n",
    "                    committed_label = current_label\n",
    "                    waiting_for_change = True\n",
    "                    current_sign_label = None\n",
    "                    current_sign_start = None\n",
    "                    stabilization_buffer.clear()\n",
    "                    \n",
    "                    display_status = f\"{current_label} ({current_confidence:.0%}) âœ“ COMMITTED!\"\n",
    "                    status_color = (0, 255, 0)  # Green\n",
    "            else:\n",
    "                # No hand detected â€” FULL RESET (allows re-doing same letter)\n",
    "                committed_label = None\n",
    "                waiting_for_change = False\n",
    "                current_sign_label = None\n",
    "                current_sign_start = None\n",
    "                stabilization_buffer.clear()\n",
    "                display_status = \"No hand detected\"\n",
    "                status_color = (150, 150, 150)\n",
    "            \n",
    "            # Flip for selfie-view display\n",
    "            frame = cv2.flip(frame, 1)\n",
    "            \n",
    "            # Calculate FPS\n",
    "            fps_elapsed = time.time() - fps_start_time\n",
    "            if fps_elapsed >= 1.0:\n",
    "                fps_display = frame_count / fps_elapsed\n",
    "                frame_count = 0\n",
    "                fps_start_time = time.time()\n",
    "            \n",
    "            # Draw UI elements\n",
    "            # Top info bar\n",
    "            cv2.rectangle(frame, (0, 0), (DISPLAY_WIDTH, 70), (30, 30, 30), -1)\n",
    "            info_text = f\"FPS: {fps_display:.1f} | Camera: {CAMERA_SOURCE} | 'q'=quit 'c'=clear 's'=screenshot\"\n",
    "            cv2.putText(frame, info_text, (10, 25), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "            cv2.putText(frame, display_status, (10, 55), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, status_color, 2)\n",
    "            \n",
    "            # Bottom bar for sentence\n",
    "            bar_height = 80\n",
    "            cv2.rectangle(frame, (0, DISPLAY_HEIGHT - bar_height), \n",
    "                         (DISPLAY_WIDTH, DISPLAY_HEIGHT), (30, 30, 30), -1)\n",
    "            cv2.putText(frame, predicted_sentence[-50:], (20, DISPLAY_HEIGHT - 30),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "            \n",
    "            cv2.imshow(window_name, frame)\n",
    "            \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('c'):\n",
    "                predicted_sentence = \"\"\n",
    "                committed_label = None\n",
    "                waiting_for_change = False\n",
    "                stabilization_buffer.clear()\n",
    "                print(\"ðŸ—‘ï¸ Sentence cleared\")\n",
    "            elif key == ord('s'):\n",
    "                filename = f\"screenshot_{int(time.time())}.png\"\n",
    "                cv2.imwrite(filename, frame)\n",
    "                print(f\"ðŸ“¸ Screenshot saved: {filename}\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸ Interrupted by user\")\n",
    "    \n",
    "    finally:\n",
    "        camera.stop()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"\\nðŸ“ Final sentence: {predicted_sentence}\")\n",
    "\n",
    "# Run the recognition\n",
    "run_sign_recognition()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the experiment above it can be concluded that models based on convolutional neural networks aren't able to predict the signs correctly and even when combined with predictions of mediapipe multi-level perceptron, it still gives the wrong output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore fine tuning the mediapipe will be the best option available in the market.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning the Mediapipe Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MediaPipe MLP model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Error handling for model loading (Fine-tuned version)\n",
    "try:\n",
    "    if os.path.exists(\"asl_mediapipe_mlp_model.h5\"):\n",
    "        mlp_model = tf.keras.models.load_model(\"asl_mediapipe_mlp_model.h5\")\n",
    "        print(\"âœ“ MediaPipe MLP model loaded successfully\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"asl_mediapipe_mlp_model.h5 not found\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading MediaPipe MLP model: {e}\")\n",
    "    mlp_model = None\n",
    "    print(\"âš ï¸  Warning: Model failed to load. Camera detection will not work.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ CSV file loaded and encoder fitted successfully\n"
     ]
    }
   ],
   "source": [
    "# Error handling for CSV file (Fine-tuned version)\n",
    "try:\n",
    "    if os.path.exists(\"asl_mediapipe_keypoints_dataset.csv\"):\n",
    "        df = pd.read_csv(\"asl_mediapipe_keypoints_dataset.csv\")\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(df[\"label\"])\n",
    "        print(\"âœ“ CSV file loaded and encoder fitted successfully\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"asl_mediapipe_keypoints_dataset.csv not found\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading CSV file: {e}\")\n",
    "    encoder = None\n",
    "    print(\"âš ï¸  Warning: Encoder failed to load. Camera detection will not work.\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7, max_num_hands=2)  # Allow max 2 hands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence formation logic\n",
    "predicted_sentence = \"\"\n",
    "last_predicted_label = None\n",
    "last_prediction_time = 0\n",
    "\n",
    "# 5 seconds cooldown for repeated letters\n",
    "cooldown_time = 5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Buffer settings (slowed down):\n",
      "   - Buffer size: 7 frames\n",
      "   - Required matches: 5\n",
      "   - Min confidence: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Optimized Stabilization buffer - SLOWED DOWN\n",
    "# Stores last 7 predictions (increased for better stability)\n",
    "stabilization_window = deque(maxlen=7)\n",
    "\n",
    "# Must match for 5 out of 7 frames (stricter requirement)\n",
    "stabilization_threshold = 5\n",
    "\n",
    "# Confidence threshold - only very high confidence can bypass buffer\n",
    "min_confidence = 0.92  # Increased from 0.85\n",
    "\n",
    "# Cooldown for repeated letters\n",
    "cooldown_time = 3  # Increased from 5 to 3 (but combined with hold time)\n",
    "\n",
    "two_hands_detected = False\n",
    "\n",
    "print(\"â±ï¸ Buffer settings (slowed down):\")\n",
    "print(f\"   - Buffer size: {stabilization_window.maxlen} frames\")\n",
    "print(f\"   - Required matches: {stabilization_threshold}\")\n",
    "print(f\"   - Min confidence: {min_confidence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_landmark_features(hand_landmarks, handedness):\n",
    "    \"\"\"Extract and normalize 21 hand landmarks from MediaPipe.\n",
    "    \n",
    "    No mirroring is applied â€” training data was recorded without flipping,\n",
    "    so inference landmarks must be used as-is.\n",
    "    \"\"\"\n",
    "    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
    "    return landmarks.flatten().reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is fine tuned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Making of a buffer ensuring that model doesnt predicts images in between the hand sign change\n",
    "- only predic when there is one hand in the frame, when 2 hand appear, give an warning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Camera opened successfully\n",
      "Press 'q' to quit (or Interrupt in Jupyter)\n",
      "ðŸªŸ Window is RESIZABLE - drag corners to resize!\n",
      "âœ“ Camera released\n"
     ]
    }
   ],
   "source": [
    "# Fine-Tuned MLP-Only Camera â€” commit-once-then-wait strategy\n",
    "if mlp_model is None or encoder is None:\n",
    "    print(\"âŒ Error: Models not loaded. Please check Cell 14 and Cell 15 for errors.\")\n",
    "else:\n",
    "    camera_url = get_camera_url()\n",
    "    cap = cv2.VideoCapture(camera_url)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"âŒ Error: Cannot access camera.\")\n",
    "    else:\n",
    "        print(\"âœ“ Camera opened successfully\")\n",
    "        print(\"Press 'q' to quit, 'c' to clear | Window is RESIZABLE\")\n",
    "        \n",
    "        window_name = \"Sign Language Recognition (Fine Tuned MediaPipe)\"\n",
    "        cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "        cv2.resizeWindow(window_name, DISPLAY_WIDTH, DISPLAY_HEIGHT)\n",
    "        \n",
    "        # State variables\n",
    "        predicted_sentence = \"\"\n",
    "        stabilization_buffer = deque(maxlen=STABILIZATION_WINDOW_SIZE)\n",
    "        two_hands_detected = False\n",
    "        \n",
    "        # Commit-once-then-wait state\n",
    "        committed_label = None\n",
    "        current_sign_label = None\n",
    "        current_sign_start = None\n",
    "        waiting_for_change = False\n",
    "        \n",
    "        try:\n",
    "            while cap.isOpened():\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                # Process unflipped frame for MediaPipe (matches training data)\n",
    "                frame = cv2.resize(frame, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                rgb_frame.flags.writeable = False\n",
    "                results = hands.process(rgb_frame)\n",
    "                rgb_frame.flags.writeable = True\n",
    "\n",
    "                two_hands_detected = (results.multi_hand_landmarks and \n",
    "                                      len(results.multi_hand_landmarks) > 1)\n",
    "\n",
    "                display_status = \"\"\n",
    "                status_color = (200, 200, 200)\n",
    "\n",
    "                if results.multi_hand_landmarks and not two_hands_detected:\n",
    "                    for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "                        # MLP prediction\n",
    "                        landmark_features = extract_landmark_features(hand_landmarks, handedness)\n",
    "                        mlp_pred = mlp_model.predict(landmark_features, verbose=0)\n",
    "                        mlp_class_index = np.argmax(mlp_pred)\n",
    "                        mlp_confidence = mlp_pred[0][mlp_class_index]\n",
    "                        mlp_label = encoder.inverse_transform([mlp_class_index])[0]\n",
    "\n",
    "                        # Skip low confidence\n",
    "                        if mlp_confidence < MIN_CONFIDENCE:\n",
    "                            display_status = f\"{mlp_label} ({mlp_confidence:.0%}) Low conf\"\n",
    "                            status_color = (0, 100, 255)\n",
    "                            break\n",
    "\n",
    "                        # Add to stability buffer\n",
    "                        stabilization_buffer.append(mlp_label)\n",
    "\n",
    "                        buffer_count = stabilization_buffer.count(mlp_label)\n",
    "                        is_stable = (buffer_count >= STABILIZATION_THRESHOLD and \n",
    "                                     len(stabilization_buffer) == STABILIZATION_WINDOW_SIZE)\n",
    "\n",
    "                        if not is_stable:\n",
    "                            progress = buffer_count / STABILIZATION_THRESHOLD * 100\n",
    "                            display_status = f\"{mlp_label} ({mlp_confidence:.0%}) Stabilizing {progress:.0f}%\"\n",
    "                            status_color = (0, 255, 255)\n",
    "                            break\n",
    "\n",
    "                        now = time.time()\n",
    "\n",
    "                        # Check if waiting after a commit\n",
    "                        if waiting_for_change:\n",
    "                            if mlp_label == committed_label:\n",
    "                                display_status = f\"{mlp_label} ({mlp_confidence:.0%}) âœ“ Committed - change sign\"\n",
    "                                status_color = (255, 200, 0)\n",
    "                                break\n",
    "                            else:\n",
    "                                waiting_for_change = False\n",
    "                                committed_label = None\n",
    "                                current_sign_label = mlp_label\n",
    "                                current_sign_start = now\n",
    "\n",
    "                        # Track hold time\n",
    "                        if mlp_label != current_sign_label:\n",
    "                            current_sign_label = mlp_label\n",
    "                            current_sign_start = now\n",
    "\n",
    "                        hold_duration = now - current_sign_start if current_sign_start else 0\n",
    "\n",
    "                        if hold_duration < HOLD_TIME_REQUIRED:\n",
    "                            hold_pct = hold_duration / HOLD_TIME_REQUIRED * 100\n",
    "                            display_status = f\"{mlp_label} ({mlp_confidence:.0%}) Hold: {hold_pct:.0f}%\"\n",
    "                            status_color = (0, 255, 255)\n",
    "                            break\n",
    "\n",
    "                        # COMMIT\n",
    "                        if mlp_label not in [\"nothing\", \"del\", \"space\"]:\n",
    "                            predicted_sentence += mlp_label\n",
    "                        elif mlp_label == \"space\":\n",
    "                            if not predicted_sentence.endswith(\" \"):\n",
    "                                predicted_sentence += \" \"\n",
    "                        elif mlp_label == \"del\" and predicted_sentence:\n",
    "                            predicted_sentence = predicted_sentence[:-1]\n",
    "\n",
    "                        committed_label = mlp_label\n",
    "                        waiting_for_change = True\n",
    "                        current_sign_label = None\n",
    "                        current_sign_start = None\n",
    "                        stabilization_buffer.clear()\n",
    "\n",
    "                        display_status = f\"{mlp_label} ({mlp_confidence:.0%}) âœ“ COMMITTED!\"\n",
    "                        status_color = (0, 255, 0)\n",
    "                else:\n",
    "                    # No hand â†’ full reset\n",
    "                    committed_label = None\n",
    "                    waiting_for_change = False\n",
    "                    current_sign_label = None\n",
    "                    current_sign_start = None\n",
    "                    stabilization_buffer.clear()\n",
    "\n",
    "                # Flip for selfie-view display\n",
    "                frame = cv2.flip(frame, 1)\n",
    "\n",
    "                # Status text\n",
    "                cv2.putText(frame, display_status, (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, status_color, 2)\n",
    "\n",
    "                # Bottom bar\n",
    "                bar_height = 60\n",
    "                frame_height, frame_width, _ = frame.shape\n",
    "                cv2.rectangle(frame, (0, frame_height - bar_height), \n",
    "                             (frame_width, frame_height), (0, 0, 0), -1)\n",
    "\n",
    "                if two_hands_detected:\n",
    "                    cv2.putText(frame, \"Only One Hand Allowed!\", (50, frame_height - 20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                else:\n",
    "                    cv2.putText(frame, predicted_sentence[-50:], (50, frame_height - 20),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "                cv2.imshow(window_name, frame)\n",
    "                key = cv2.waitKey(1) & 0xFF\n",
    "                if key == ord('q'):\n",
    "                    break\n",
    "                elif key == ord('c'):\n",
    "                    predicted_sentence = \"\"\n",
    "                    committed_label = None\n",
    "                    waiting_for_change = False\n",
    "                    stabilization_buffer.clear()\n",
    "                    print(\"ðŸ—‘ï¸ Sentence cleared\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nâš ï¸ Interrupted by user\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error during camera loop: {e}\")\n",
    "        finally:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            print(f\"ðŸ“ Final sentence: {predicted_sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
