{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- PYTORCH CHECK ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVersion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\__init__.py:129\u001b[0m\n\u001b[0;32m    127\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    128\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] The specified procedure could not be found. Error loading \"c:\\Users\\adelg\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\lib\\shm.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"--- PYTORCH CHECK ---\")\n",
    "print(f\"Version: {torch.__version__}\")\n",
    "is_available = torch.cuda.is_available()\n",
    "print(f\"GPU Available: {is_available}\")\n",
    "\n",
    "if is_available:\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "# Expected: GPU Available: True and the name of your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- OPENCV CHECK (cv2) ---\n",
      "Version: 4.11.0\n",
      "Module Status: Loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(\"--- OPENCV CHECK (cv2) ---\")\n",
    "print(f\"Version: {cv2.__version__}\")\n",
    "\n",
    "# Check if a core component loads successfully\n",
    "try:\n",
    "    # A simple check to see if the library's features are accessible\n",
    "    print(f\"Module Status: Loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Status: FAIL - Crash: {e}\")\n",
    "\n",
    "# EXPECTED SUCCESS: Version 4.8.0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TENSORFLOW CHECK ---\n",
      "Version: 2.10.0\n",
      "GPUs Detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"--- TENSORFLOW CHECK ---\")\n",
    "print(f\"Version: {tf.__version__}\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs Detected: {gpus}\")\n",
    "\n",
    "# Expected: Version 2.10.0 and a list showing your physical device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ------------\n",
      "absl-py                      2.3.1\n",
      "anyio                        4.11.0\n",
      "argon2-cffi                  25.1.0\n",
      "argon2-cffi-bindings         25.1.0\n",
      "arrow                        1.4.0\n",
      "asttokens                    3.0.0\n",
      "astunparse                   1.6.3\n",
      "async-lru                    2.0.5\n",
      "attrs                        25.4.0\n",
      "babel                        2.17.0\n",
      "beautifulsoup4               4.14.2\n",
      "bleach                       6.2.0\n",
      "cachetools                   6.2.2\n",
      "certifi                      2025.11.12\n",
      "cffi                         2.0.0\n",
      "charset-normalizer           3.4.4\n",
      "colorama                     0.4.6\n",
      "comm                         0.2.3\n",
      "contourpy                    1.3.0\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.8.17\n",
      "decorator                    5.2.1\n",
      "defusedxml                   0.7.1\n",
      "exceptiongroup               1.2.2\n",
      "executing                    2.2.1\n",
      "fastjsonschema               2.21.2\n",
      "filelock                     3.19.1\n",
      "flatbuffers                  25.9.23\n",
      "fonttools                    4.60.1\n",
      "fqdn                         1.5.1\n",
      "fsspec                       2025.10.0\n",
      "gast                         0.4.0\n",
      "glob2                        0.7\n",
      "google-auth                  2.43.0\n",
      "google-auth-oauthlib         0.4.6\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.74.0\n",
      "h11                          0.16.0\n",
      "h5py                         3.14.0\n",
      "httpcore                     1.0.9\n",
      "httpx                        0.28.1\n",
      "idna                         3.11\n",
      "importlib_metadata           8.7.0\n",
      "importlib_resources          6.5.2\n",
      "ipykernel                    6.31.0\n",
      "ipython                      8.18.1\n",
      "ipywidgets                   8.1.8\n",
      "isoduration                  20.11.0\n",
      "jedi                         0.19.2\n",
      "Jinja2                       3.1.6\n",
      "joblib                       1.5.2\n",
      "json5                        0.12.1\n",
      "jsonpointer                  3.0.0\n",
      "jsonschema                   4.25.1\n",
      "jsonschema-specifications    2025.9.1\n",
      "jupyter                      1.1.1\n",
      "jupyter_client               8.6.3\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.8.1\n",
      "jupyter-events               0.12.0\n",
      "jupyter-lsp                  2.3.0\n",
      "jupyter_server               2.17.0\n",
      "jupyter_server_terminals     0.5.3\n",
      "jupyterlab                   4.4.10\n",
      "jupyterlab_pygments          0.3.0\n",
      "jupyterlab_server            2.28.0\n",
      "jupyterlab_widgets           3.0.16\n",
      "keras                        2.10.0\n",
      "Keras-Preprocessing          1.1.2\n",
      "kiwisolver                   1.4.7\n",
      "lark                         1.3.1\n",
      "libclang                     18.1.1\n",
      "Markdown                     3.9\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   3.0.3\n",
      "matplotlib                   3.9.4\n",
      "matplotlib-inline            0.2.1\n",
      "mdurl                        0.1.2\n",
      "mediapipe                    0.10.9\n",
      "mistune                      3.1.4\n",
      "ml_dtypes                    0.5.3\n",
      "mpmath                       1.3.0\n",
      "namex                        0.1.0\n",
      "nbclient                     0.10.2\n",
      "nbconvert                    7.16.6\n",
      "nbformat                     5.10.4\n",
      "nest-asyncio                 1.6.0\n",
      "networkx                     3.2.1\n",
      "notebook                     7.4.7\n",
      "notebook_shim                0.2.4\n",
      "npm                          0.1.1\n",
      "numpy                        1.23.5\n",
      "oauthlib                     3.3.1\n",
      "opencv-contrib-python        4.11.0.86\n",
      "opencv-python                4.8.0.74\n",
      "opt_einsum                   3.4.0\n",
      "optree                       0.17.0\n",
      "overrides                    7.7.0\n",
      "packaging                    25.0\n",
      "pandas                       2.3.3\n",
      "pandocfilters                1.5.1\n",
      "parso                        0.8.5\n",
      "pillow                       11.3.0\n",
      "pip                          25.3\n",
      "platformdirs                 4.4.0\n",
      "prometheus_client            0.23.1\n",
      "prompt_toolkit               3.0.52\n",
      "protobuf                     3.20.3\n",
      "psutil                       7.1.3\n",
      "pure_eval                    0.2.3\n",
      "pyasn1                       0.6.1\n",
      "pyasn1_modules               0.4.2\n",
      "pycparser                    2.23\n",
      "Pygments                     2.19.2\n",
      "pyparsing                    3.2.5\n",
      "python-dateutil              2.9.0.post0\n",
      "python-json-logger           4.0.0\n",
      "pytz                         2025.2\n",
      "pywin32                      311\n",
      "pywinpty                     3.0.2\n",
      "PyYAML                       6.0.3\n",
      "pyzmq                        27.1.0\n",
      "referencing                  0.36.2\n",
      "requests                     2.32.5\n",
      "requests-oauthlib            2.0.0\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rfc3987-syntax               1.1.0\n",
      "rich                         14.2.0\n",
      "rpds-py                      0.27.1\n",
      "rsa                          4.9.1\n",
      "scikit-learn                 1.6.1\n",
      "scipy                        1.13.1\n",
      "Send2Trash                   1.8.3\n",
      "sentencepiece                0.2.1\n",
      "setuptools                   58.1.0\n",
      "six                          1.17.0\n",
      "sniffio                      1.3.1\n",
      "sounddevice                  0.5.3\n",
      "soupsieve                    2.8\n",
      "stack-data                   0.6.3\n",
      "sympy                        1.13.1\n",
      "tensorboard                  2.10.1\n",
      "tensorboard-data-server      0.6.1\n",
      "tensorboard-plugin-wit       1.8.1\n",
      "tensorflow                   2.10.0\n",
      "tensorflow-estimator         2.10.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    3.1.0\n",
      "terminado                    0.18.1\n",
      "threadpoolctl                3.6.0\n",
      "tinycss2                     1.4.0\n",
      "tomli                        2.3.0\n",
      "torch                        1.12.1+cu113\n",
      "torchaudio                   0.12.1+cu113\n",
      "torchvision                  0.13.1+cu113\n",
      "tornado                      6.5.2\n",
      "traitlets                    5.14.3\n",
      "typing_extensions            4.15.0\n",
      "tzdata                       2025.2\n",
      "uri-template                 1.3.0\n",
      "urllib3                      2.5.0\n",
      "wcwidth                      0.2.14\n",
      "webcolors                    24.11.1\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.9.0\n",
      "Werkzeug                     3.1.3\n",
      "wheel                        0.45.1\n",
      "widgetsnbextension           4.0.15\n",
      "wrapt                        2.0.1\n",
      "zipp                         3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DETAILED TENSORFLOW GPU CHECK ---\n",
      "✅ Device Found: /physical_device:GPU:0\n",
      "✅ **GPU NAME**: NVIDIA GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check if the device is found and accessible\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Access the properties of the first detected device (GPU:0)\n",
    "        details = tf.config.experimental.get_device_details(gpus[0])\n",
    "        # The friendly name is usually stored here\n",
    "        name = details.get('device_name', 'Name retrieval failed.') \n",
    "        print(f\"--- DETAILED TENSORFLOW GPU CHECK ---\")\n",
    "        print(f\"✅ Device Found: {gpus[0].name}\")\n",
    "        print(f\"✅ **GPU NAME**: {name}\")\n",
    "    except Exception:\n",
    "        # Fallback if the experimental API is not stable\n",
    "        print(\"✅ GPU Detected. Name retrieval failed (API instability).\")\n",
    "        print(f\"Path: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"❌ No GPU found by TensorFlow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the .DS_Store(hidden files) files in the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- MEDIAPIPE CHECK ---\n",
      "Status: SUCCESS - Module loaded and initialized.\n"
     ]
    }
   ],
   "source": [
    "import mediapipe as mp\n",
    "print(\"--- MEDIAPIPE CHECK ---\")\n",
    "\n",
    "try:\n",
    "    # Attempt to initialize a core module (Hands) that relies on Protobuf\n",
    "    mp_hands = mp.solutions.hands\n",
    "    print(\"Status: SUCCESS - Module loaded and initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"Status: FAIL - Crash: {e}\")\n",
    "# EXPECTED SUCCESS: Status: SUCCESS - Module loaded and initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- TENSORFLOW CHECK ---\n",
      "Version: 2.10.0\n",
      "GPUs Detected: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"--- TENSORFLOW CHECK ---\")\n",
    "print(f\"Version: {tf.__version__}\")\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPUs Detected: {gpus}\")\n",
    "\n",
    "# EXPECTED SUCCESS: Version 2.10.0 and a list of physical devices (e.g., '[PhysicalDevice...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running DLL Check...\n",
      "CUDA_PATH: C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.2\n",
      "✅ SUCCESS! Detected: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "print(\"Running DLL Check...\")\n",
    "\n",
    "# 1. Check if CUDA is in Path\n",
    "cuda_path = os.environ.get('CUDA_PATH')\n",
    "print(f\"CUDA_PATH: {cuda_path}\")\n",
    "\n",
    "# 2. Try to load the GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus) > 0:\n",
    "    print(f\"✅ SUCCESS! Detected: {gpus[0].name}\")\n",
    "else:\n",
    "    print(\"❌ FAILURE: No GPU detected.\")\n",
    "    print(\"Possible missing files in 'C:\\\\Program Files\\\\NVIDIA GPU Computing Toolkit\\\\CUDA\\\\v11.2\\\\bin':\")\n",
    "    print(\"- cudart64_110.dll (From CUDA Toolkit)\")\n",
    "    print(\"- cudnn64_8.dll    (From cuDNN)\")\n",
    "    print(\"- zlibwapi.dll     (The secret file)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dataset_dir = '/Users/js/Desktop/Sign Recognition Application/Sign_to_Sentence Project/Asl_Sign_Data/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "# Traverse all subdirectories and remove .DS_Store files\n",
    "for root, dirs, files in os.walk(dataset_dir):\n",
    "    for file in files:\n",
    "        if file == \".DS_Store\":\n",
    "            file_path = os.path.join(root, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Removed: {file_path}\")\n",
    "\n",
    "print(\"All .DS_Store files removed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = '/Users/js/Desktop/Sign Recognition Application/Sign_to_Sentence Project/Asl_Sign_Data/asl_alphabet_train/asl_alphabet_train'\n",
    "\n",
    "classes = os.listdir(dataset_dir)\n",
    "print(f\"Classes: {classes}\")\n",
    "\n",
    "# Visualize some images\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 8))\n",
    "\n",
    "for i, label in enumerate(classes[:5]):  # Show first 5 classes\n",
    "    class_dir = os.path.join(dataset_dir, label)\n",
    "    img_files = os.listdir(class_dir)[:3]  # Show 3 images per class\n",
    "\n",
    "    for j, img_name in enumerate(img_files):\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        img = cv2.resize(img, (128, 128))\n",
    "        \n",
    "        axes[j, i].imshow(img)\n",
    "        axes[j, i].axis(\"off\")\n",
    "        axes[j, i].set_title(label)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define image size and batch size\n",
    "IMG_SIZE = 128 \n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Data generators\n",
    "# We are using this approach to make it less computaionally extensive as the data consists of 87,000 images appx,\n",
    "# loading all the images as generally done and then label encoding them will be CPU extensive task. \n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Splitting data into train (80%) and val (20%)\n",
    ")\n",
    "\n",
    "# Train & validation generators (load images directly from disk)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_dir,\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode=\"categorical\",\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "print(\"Class labels:\", train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the MobileNet V2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNetV2(\n",
    "    weights=\"imagenet\", \n",
    "    include_top=False, \n",
    "    input_shape=(128, 128, 3)\n",
    ")\n",
    "\n",
    "base_model.trainable = False  # Freezing all layers initially as this is an intial training\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)\n",
    "output_layer = Dense(len(train_generator.class_indices), activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output_layer)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial training on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=5 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning the model further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[-20:]:  # Unfreeze last 20 layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# Recompile with a lower learning rate to avoid overfitting\n",
    "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=1e-4),\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"sign_language_model_MobileNetV2.h5\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"sign_language_model_MobileNetV2.h5\")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\", 4: \"E\", 5: \"F\", 6: \"G\", 7: \"H\",\n",
    "    8: \"I\", 9: \"J\", 10: \"K\", 11: \"L\", 12: \"M\", 13: \"N\", 14: \"O\",\n",
    "    15: \"P\", 16: \"Q\", 17: \"R\", 18: \"S\", 19: \"T\", 20: \"U\", 21: \"V\",\n",
    "    22: \"W\", 23: \"X\", 24: \"Y\", 25: \"Z\", 26: \"del\", 27: \"nothing\", 28: \"space\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128 \n",
    "test_folder = '/Users/js/Desktop/Sign Recognition Application/Sign_to_Sentence Project/Asl_Sign_Data/asl_alphabet_test/asl_alphabet_test'\n",
    "test_images = []\n",
    "image_names = []\n",
    "\n",
    "for img_name in os.listdir(test_folder):\n",
    "    if img_name.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        img_path = os.path.join(test_folder, img_name)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img = img / 255.0\n",
    "\n",
    "        test_images.append(img)\n",
    "        image_names.append(img_name)\n",
    "\n",
    "# Convert to NumPy array\n",
    "test_images = np.array(test_images)\n",
    "\n",
    "print(f\"Loaded {len(test_images)} test images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "for i, img_name in enumerate(image_names):\n",
    "    pred_label = class_labels[predicted_classes[i]]\n",
    "    print(f\"Image: {img_name} → Predicted as: {pred_label}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [img_name.split(\"_\")[0] for img_name in image_names]\n",
    "\n",
    "correct = sum([1 if class_labels[predicted_classes[i]] == true_labels[i] else 0 for i in range(len(true_labels))])\n",
    "accuracy = correct / len(true_labels) * 100\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "\n",
    "# Plot Accuracy Graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_dict[\"accuracy\"], label=\"Training Accuracy\", marker=\"o\", color=\"orange\")\n",
    "plt.plot(history_dict[\"val_accuracy\"], label=\"Validation Accuracy\", marker=\"o\", color=\"red\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Model Accuracy Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Loss Graph\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_dict[\"loss\"], label=\"Training Loss\", marker=\"o\", color=\"red\")\n",
    "plt.plot(history_dict[\"val_loss\"], label=\"Validation Loss\", marker=\"o\", color=\"blue\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Model Loss Over Epochs\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(true_labels, [class_labels[i] for i in predicted_classes], labels=list(class_labels.values()))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels.values(), yticklabels=class_labels.values())\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix for ASL Sign Recognition\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the Signs from camera feed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The approach is to use mediapipe to find the center of the hand, make a box/ frame around it, pass the image inside of frame to the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since MobileNet V2 is convolutional neural network, it would not give accurate results on real life data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load trained MobileNetV2 model\n",
    "mobilenet_model = tf.keras.models.load_model(\"sign_language_model_MobileNetV2.h5\")\n",
    "\n",
    "# Correct class labels (matching `train_generator.class_indices`)\n",
    "class_labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J',\n",
    "                'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                'U', 'V', 'W', 'X', 'Y', 'Z', 'del', 'nothing', 'space']\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "#Bounding Box Adjustments\n",
    "HEIGHT_EXPAND = 220\n",
    "WIDTH_EXPAND = 150\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Get bounding box coordinates\n",
    "            x_min = min([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1]\n",
    "            y_min = min([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0]\n",
    "            x_max = max([lm.x for lm in hand_landmarks.landmark]) * frame.shape[1]\n",
    "            y_max = max([lm.y for lm in hand_landmarks.landmark]) * frame.shape[0]\n",
    "\n",
    "            x_min = max(0, int(x_min - WIDTH_EXPAND))   \n",
    "            y_min = max(0, int(y_min - HEIGHT_EXPAND)) \n",
    "            x_max = min(frame.shape[1], int(x_max + WIDTH_EXPAND))  \n",
    "            y_max = min(frame.shape[0], int(y_max + HEIGHT_EXPAND)) \n",
    "\n",
    "            # Crop the hand region\n",
    "            hand_crop = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Resize to (128x128) for MobileNetV2\n",
    "            if hand_crop.shape[0] > 0 and hand_crop.shape[1] > 0:\n",
    "                hand_resized = cv2.resize(hand_crop, (128, 128))\n",
    "                hand_resized = np.expand_dims(hand_resized, axis=0) / 255.0  # Normalize\n",
    "\n",
    "                # Predict using MobileNetV2\n",
    "                prediction = mobilenet_model.predict(hand_resized)\n",
    "                \n",
    "                # Get top 3 predictions\n",
    "                top_3_indices = np.argsort(prediction[0])[-3:][::-1]\n",
    "                top_3_labels = [class_labels[i] for i in top_3_indices]\n",
    "                top_3_scores = [prediction[0][i] for i in top_3_indices]\n",
    "\n",
    "                # Debugging: Print top 3 predictions\n",
    "                print(f\"Top 3 Predictions: {list(zip(top_3_labels, top_3_scores))}\")\n",
    "\n",
    "                # Get the best prediction\n",
    "                predicted_label = top_3_labels[0]\n",
    "\n",
    "                # Display Prediction\n",
    "                cv2.putText(frame, f\"Predicted: {predicted_label}\", (50, 50),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "                \n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"MobileNetV2 Sign Prediction (Fixed Bounding Box Height)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After testing it is seen that this model is useful and clasifies the signs correctly which has distinct unique shapes like - letter A, L, etc but fails in complex signs like - letter K, X etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
