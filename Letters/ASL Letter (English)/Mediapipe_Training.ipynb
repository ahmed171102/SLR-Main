{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting all the keypoints from the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential # type: ignore\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization # type: ignore\n",
    "from tensorflow.keras.utils import to_categorical # type: ignore\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau # type: ignore\n",
    "import time\n",
    "from tqdm import tqdm  # Progress bar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Detection and Configuration\n",
    "\n",
    ",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üîç GPU DETECTION AND CONFIGURATION (OPTIMIZED)\n",
      "============================================================\n",
      "\n",
      "üì¶ TensorFlow Version: 2.10.0\n",
      "All Physical Devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "\n",
      "üîç Detecting GPU devices...\n",
      "üéÆ GPU Devices Found: 1\n",
      "\n",
      "‚úÖ GPU IS AVAILABLE!\n",
      "\n",
      "‚öôÔ∏è  Configuring GPU Memory Growth...\n",
      "   ‚úÖ Memory growth enabled for 1 GPU(s)\n",
      "   ‚úÖ Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "   ‚úÖ GPU Device Name: /physical_device:GPU:0\n",
      "\n",
      "üìä GPU Details:\n",
      "   GPU Details: {'device_name': 'NVIDIA GeForce MX150', 'compute_capability': (6, 1)}\n",
      "   Device Name: NVIDIA GeForce MX150\n",
      "   Compute Capability: (6, 1)\n",
      "\n",
      "‚ö° Enabling Mixed Precision Training...\n",
      "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
      "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
      "  NVIDIA GeForce MX150, compute capability 6.1\n",
      "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
      "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n",
      "   ‚úÖ Mixed precision enabled: mixed_float16\n",
      "   ‚ÑπÔ∏è  Note: Output layer will use float32 for numerical stability\n",
      "\n",
      "üß™ GPU Verification Test...\n",
      "   GPU Built with CUDA: True\n",
      "   ‚úÖ GPU Available: True\n",
      "   ‚úÖ GPU Device Name: /physical_device:GPU:0\n",
      "   Operation executed on: /job:localhost/replica:0/task:0/device:GPU:0\n",
      "   ‚úÖ SUCCESS: GPU is being used for computations!\n",
      "\n",
      "üöÄ Training will use: /GPU:0\n",
      "\n",
      "============================================================\n",
      "‚úÖ GPU Configuration Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GPU DETECTION AND CONFIGURATION (OPTIMIZED)\n",
    "# ============================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üîç GPU DETECTION AND CONFIGURATION (OPTIMIZED)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Quick TensorFlow version check\n",
    "print(f\"\\nüì¶ TensorFlow Version: {tf.__version__}\")\n",
    "\n",
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(f\"All Physical Devices: {physical_devices}\")\n",
    "\n",
    "# GPU detection\n",
    "print(\"\\nüîç Detecting GPU devices...\")\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"üéÆ GPU Devices Found: {len(gpus)}\")\n",
    "\n",
    "if len(gpus) > 0:\n",
    "    print(\"\\n‚úÖ GPU IS AVAILABLE!\")\n",
    "    \n",
    "    # Configure GPU memory growth to avoid allocating all memory at once\n",
    "    print(\"\\n‚öôÔ∏è  Configuring GPU Memory Growth...\")\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"   ‚úÖ Memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "        \n",
    "        # Set GPU as default device\n",
    "        tf.config.set_visible_devices(gpus[0], 'GPU')\n",
    "        print(f\"   ‚úÖ Using GPU: {gpus[0]}\")\n",
    "        \n",
    "        # Verify GPU is being used\n",
    "        print(f\"   ‚úÖ GPU Device Name: {gpus[0].name}\")\n",
    "        \n",
    "    except RuntimeError as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Error configuring GPU: {e}\")\n",
    "    \n",
    "    # Get GPU details\n",
    "    print(\"\\nüìä GPU Details:\")\n",
    "    try:\n",
    "        gpu_details = tf.config.experimental.get_device_details(gpus[0])\n",
    "        print(f\"   GPU Details: {gpu_details}\")\n",
    "        if 'device_name' in gpu_details:\n",
    "            print(f\"   Device Name: {gpu_details['device_name']}\")\n",
    "        if 'compute_capability' in gpu_details:\n",
    "            print(f\"   Compute Capability: {gpu_details['compute_capability']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ÑπÔ∏è  GPU details not available: {e}\")\n",
    "    \n",
    "    # Enable mixed precision training (optional but recommended)\n",
    "    print(\"\\n‚ö° Enabling Mixed Precision Training...\")\n",
    "    try:\n",
    "        policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "        tf.keras.mixed_precision.set_global_policy(policy)\n",
    "        print(f\"   ‚úÖ Mixed precision enabled: {policy.name}\")\n",
    "        print(\"   ‚ÑπÔ∏è  Note: Output layer will use float32 for numerical stability\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Mixed precision not available: {e}\")\n",
    "        print(\"   ‚ÑπÔ∏è  Continuing with float32 precision\")\n",
    "    \n",
    "    # Verify GPU is available for computation\n",
    "    print(\"\\nüß™ GPU Verification Test...\")\n",
    "    print(f\"   GPU Built with CUDA: {tf.test.is_built_with_cuda()}\")\n",
    "    if gpus:\n",
    "        print(f\"   ‚úÖ GPU Available: True\")\n",
    "        print(f\"   ‚úÖ GPU Device Name: {gpus[0].name}\")\n",
    "        \n",
    "        # Run a simple computation to verify GPU is actually being used\n",
    "        try:\n",
    "            with tf.device('/GPU:0'):\n",
    "                a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "                b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "                c = tf.matmul(a, b)\n",
    "                \n",
    "                # Check which device the operation ran on\n",
    "                device_str = str(c.device)\n",
    "                print(f\"   Operation executed on: {c.device}\")\n",
    "                if 'GPU' in device_str or 'gpu' in device_str.lower():\n",
    "                    print(\"   ‚úÖ SUCCESS: GPU is being used for computations!\")\n",
    "                else:\n",
    "                    print(\"   ‚ö†Ô∏è  WARNING: Operations are running on CPU, not GPU\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  GPU test warning: {e}\")\n",
    "            print(\"   ‚ÑπÔ∏è  GPU may still work for training\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå GPU Available: False\")\n",
    "    \n",
    "    USE_GPU = True\n",
    "    DEVICE = '/GPU:0'\n",
    "    print(f\"\\nüöÄ Training will use: {DEVICE}\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ùå NO GPU FOUND - Will use CPU\")\n",
    "    print(\"   ‚ö†Ô∏è  Training will be slower on CPU\")\n",
    "    USE_GPU = False\n",
    "    DEVICE = '/CPU:0'\n",
    "    \n",
    "    # Quick CUDA check\n",
    "    print(\"\\nüîç Checking CUDA support...\")\n",
    "    try:\n",
    "        if tf.test.is_built_with_cuda():\n",
    "            print(\"   ‚úÖ TensorFlow was built with CUDA support\")\n",
    "            print(\"   ‚ö†Ô∏è  But no GPU device was detected\")\n",
    "            print(\"   üí° Make sure you have:\")\n",
    "            print(\"      - NVIDIA GPU with CUDA support\")\n",
    "            print(\"      - CUDA toolkit installed\")\n",
    "            print(\"      - cuDNN library installed\")\n",
    "            print(\"      - TensorFlow-GPU version installed\")\n",
    "        else:\n",
    "            print(\"   ‚ùå TensorFlow was NOT built with CUDA support\")\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è  Could not check CUDA support\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ GPU Configuration Complete!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üí° GPU MEMORY MANAGEMENT TIPS\n",
      "============================================================\n",
      "Current batch size: 256 (default for MLP models)\n",
      "Expected memory usage: ~1.5-2.5 GB (MLP is memory-efficient)\n",
      "\n",
      "üí° MEMORY OPTIMIZATION TIPS:\n",
      "1. Close other GPU-intensive applications during training\n",
      "2. Close browser tabs with video/graphics (they use GPU)\n",
      "3. Monitor memory with: nvidia-smi -l 1 (in separate terminal)\n",
      "4. If you get 'Out of Memory' error:\n",
      "   - Reduce batch size to 128 or 64\n",
      "   - Or close other applications\n",
      "5. MLP models are memory-efficient - batch 256 is typically safe\n",
      "\n",
      "üìä To check GPU memory during training:\n",
      "   Open Command Prompt/PowerShell and run: nvidia-smi -l 1\n",
      "   You should see GPU-Util: 50-100% and Memory-Usage increasing\n",
      "\n",
      "‚úÖ Ready to train with optimized settings!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GPU MEMORY MONITORING & OPTIMIZATION TIPS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"üí° GPU MEMORY MANAGEMENT TIPS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if USE_GPU:\n",
    "    print(f\"Current batch size: 256 (default for MLP models)\")\n",
    "    print(f\"Expected memory usage: ~1.5-2.5 GB (MLP is memory-efficient)\")\n",
    "    \n",
    "    print(\"\\nüí° MEMORY OPTIMIZATION TIPS:\")\n",
    "    print(\"1. Close other GPU-intensive applications during training\")\n",
    "    print(\"2. Close browser tabs with video/graphics (they use GPU)\")\n",
    "    print(\"3. Monitor memory with: nvidia-smi -l 1 (in separate terminal)\")\n",
    "    print(\"4. If you get 'Out of Memory' error:\")\n",
    "    print(\"   - Reduce batch size to 128 or 64\")\n",
    "    print(\"   - Or close other applications\")\n",
    "    print(\"5. MLP models are memory-efficient - batch 256 is typically safe\")\n",
    "    \n",
    "    print(\"\\nüìä To check GPU memory during training:\")\n",
    "    print(\"   Open Command Prompt/PowerShell and run: nvidia-smi -l 1\")\n",
    "    print(\"   You should see GPU-Util: 50-100% and Memory-Usage increasing\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected - memory tips not applicable\")\n",
    "    print(\"   Training will use CPU memory instead\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to train with optimized settings!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìÅ Dataset CSV already exists!\n",
      "   File: asl_mediapipe_keypoints_dataset.csv\n",
      "   Samples: 59801\n",
      "   ‚úÖ Skipping extraction. Use existing dataset.\n",
      "============================================================\n",
      "\n",
      "üí° To re-extract, delete the CSV file first.\n",
      "\n",
      "üì¶ Dataset loaded: 59801 samples\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# OPTIMIZED MEDIAPIPE KEYPOINT EXTRACTION\n",
    "# ============================================\n",
    "\n",
    "# Check if CSV already exists (skip processing if it does)\n",
    "CSV_PATH = \"asl_mediapipe_keypoints_dataset.csv\"\n",
    "if os.path.exists(CSV_PATH):\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìÅ Dataset CSV already exists!\")\n",
    "    print(f\"   File: {CSV_PATH}\")\n",
    "    df_existing = pd.read_csv(CSV_PATH)\n",
    "    print(f\"   Samples: {len(df_existing)}\")\n",
    "    print(\"   ‚úÖ Skipping extraction. Use existing dataset.\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nüí° To re-extract, delete the CSV file first.\")\n",
    "else:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üîç EXTRACTING MEDIAPIPE KEYPOINTS FROM DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"‚è±Ô∏è  This will take time depending on dataset size...\")\n",
    "    print(\"   (Typical ASL dataset: ~29,000 images = 30-60 minutes)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialize MediaPipe Hands\n",
    "    mp_hands = mp.solutions.hands\n",
    "    hands = mp_hands.Hands(static_image_mode=True, min_detection_confidence=0.7)\n",
    "    \n",
    "    # Dataset directory\n",
    "    DATASET_DIR = r'M:\\Term 9\\Grad\\Gradution Current Project - Copy\\Sign-Language-Recognition-System-main\\Sign-Language-Recognition-System-main\\Sign_to_Sentence Project\\Asl_Sign_Data\\asl_alphabet_train'\n",
    "    \n",
    "    # Initialize lists to store extracted data\n",
    "    landmark_data = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get all image files first (for progress tracking)\n",
    "    print(\"\\nüìÇ Scanning dataset...\")\n",
    "    all_images = []\n",
    "    class_labels = sorted([d for d in os.listdir(DATASET_DIR) if os.path.isdir(os.path.join(DATASET_DIR, d))])\n",
    "    \n",
    "    for label in class_labels:\n",
    "        folder_path = os.path.join(DATASET_DIR, label)\n",
    "        files = [f for f in os.listdir(folder_path) if f.endswith((\".png\", \".jpg\", \".jpeg\"))]\n",
    "        for file in files:\n",
    "            all_images.append((label, os.path.join(folder_path, file)))\n",
    "    \n",
    "    total_images = len(all_images)\n",
    "    print(f\"   Found {total_images} images across {len(class_labels)} classes\")\n",
    "    print(f\"   Classes: {', '.join(class_labels[:10])}{'...' if len(class_labels) > 10 else ''}\")\n",
    "    \n",
    "    # Process images with progress bar\n",
    "    print(\"\\nüîÑ Processing images...\")\n",
    "    start_time = time.time()\n",
    "    processed_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Process with progress bar\n",
    "    for label, img_path in tqdm(all_images, desc=\"Extracting keypoints\", unit=\"img\"):\n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            \n",
    "            # Check if image is valid\n",
    "            if image is None:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Convert image to RGB (MediaPipe requires RGB)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Process image with MediaPipe\n",
    "            results = hands.process(image_rgb)\n",
    "            \n",
    "            # If a hand is detected, extract landmarks\n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks in results.multi_hand_landmarks:\n",
    "                    # Extract landmark points (x, y, z) for 21 keypoints\n",
    "                    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "                    \n",
    "                    # Save data\n",
    "                    landmark_data.append(landmarks)\n",
    "                    labels.append(label)\n",
    "                    processed_count += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "                \n",
    "        except Exception as e:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "    \n",
    "    processing_time = time.time() - start_time\n",
    "    \n",
    "    # Convert to DataFrame and Save\n",
    "    print(\"\\nüíæ Saving dataset...\")\n",
    "    if len(landmark_data) == 0:\n",
    "        print(\"‚ùå ERROR: No hand landmarks were saved. Check dataset format.\")\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        df = pd.DataFrame(landmark_data)\n",
    "        df[\"label\"] = labels\n",
    "        df.to_csv(CSV_PATH, index=False)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"‚úÖ EXTRACTION COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üìä Statistics:\")\n",
    "        print(f\"   Total images processed: {total_images}\")\n",
    "        print(f\"   Successfully extracted: {processed_count}\")\n",
    "        print(f\"   Skipped (no hand detected): {skipped_count}\")\n",
    "        print(f\"   Processing time: {processing_time/60:.2f} minutes ({processing_time:.2f} seconds)\")\n",
    "        print(f\"   Average time per image: {processing_time/total_images:.3f} seconds\")\n",
    "        print(f\"   Dataset saved: {CSV_PATH}\")\n",
    "        print(f\"   Dataset size: {len(df)} samples\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "# Load the dataset (either existing or newly created)\n",
    "if os.path.exists(CSV_PATH):\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    print(f\"\\nüì¶ Dataset loaded: {len(df)} samples\")\n",
    "else:\n",
    "    print(\"\\n‚ùå No dataset found. Please run the extraction cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the Mediapipe Keypoints file data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 38272\n",
      "Validation samples: 9568\n",
      "Test samples: 11961\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"asl_mediapipe_keypoints_dataset.csv\")\n",
    "\n",
    "# Separate features and labels (convert to float32 early to save memory)\n",
    "X = df.iloc[:, :-1].astype(\"float32\").values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "# Encode labels as numbers\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "num_classes = len(encoder.classes_)\n",
    "\n",
    "# Split dataset into train/test/validation using encoded labels for stratification\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X,\n",
    "    y_encoded,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train_full\n",
    ")\n",
    "\n",
    "# Convert labels to one-hot after splitting\n",
    "X_train = X_train.astype(\"float32\")\n",
    "X_val = X_val.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val = to_categorical(y_val, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val.shape[0]}\")\n",
    "print(f\"Test samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility to build performant tf.data pipelines\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def make_dataset(features, labels, batch_size, training=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if training:\n",
    "        buffer_size = min(len(features), 10000)\n",
    "        ds = ds.shuffle(buffer_size=buffer_size, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creation of a Multi-Level-Perceptron Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Building MLP Model for GPU Training...\n",
      "   Input shape: 63\n",
      "   Number of classes: 28\n",
      "\n",
      "üìä Model Summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               16384     \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 28)                1820      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,892\n",
      "Trainable params: 60,124\n",
      "Non-trainable params: 768\n",
      "_________________________________________________________________\n",
      "\n",
      "üéØ Model will train on: /GPU:0\n",
      "   ‚úÖ GPU acceleration enabled\n",
      "   ‚ö° Mixed precision training: Enabled (if supported)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GPU-OPTIMIZED MODEL CREATION\n",
    "# ============================================\n",
    "\n",
    "print(\"üî® Building MLP Model for GPU Training...\")\n",
    "print(f\"   Input shape: {X_train.shape[1]}\")\n",
    "print(f\"   Number of classes: {len(np.unique(y_encoded))}\")\n",
    "\n",
    "num_classes = len(np.unique(y_encoded))\n",
    "\n",
    "# Clear any previous graph to free GPU memory\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build model with GPU optimization\n",
    "with tf.device(DEVICE):\n",
    "    model = Sequential([\n",
    "        Dense(\n",
    "            256,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "            input_shape=(X_train.shape[1],)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(\n",
    "            128,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(1e-4)\n",
    "        ),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.25),\n",
    "        Dense(\n",
    "            64,\n",
    "            activation='relu',\n",
    "            kernel_initializer='he_normal'\n",
    "        ),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax', dtype='float32')  # Output layer in float32 for stability\n",
    "    ])\n",
    "    \n",
    "    # Use mixed precision friendly optimizer (legacy Adam plays nicer with float16)\n",
    "    optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Compile with GPU-optimized settings\n",
    "    model.compile(\n",
    "        optimizer=optimizer, \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "# Display model summary\n",
    "print(\"\\nüìä Model Summary:\")\n",
    "model.summary()\n",
    "\n",
    "# Check if model will use GPU\n",
    "print(f\"\\nüéØ Model will train on: {DEVICE}\")\n",
    "if USE_GPU:\n",
    "    print(\"   ‚úÖ GPU acceleration enabled\")\n",
    "    print(\"   ‚ö° Mixed precision training: Enabled (if supported)\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Training on CPU (slower)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the MLP Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (4055112095.py, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 27\u001b[1;36m\u001b[0m\n\u001b[1;33m    callbacks = [\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# GPU-OPTIMIZED TRAINING\n",
    "# ============================================\n",
    "\n",
    "print(\"üöÄ Starting GPU-Optimized Training...\")\n",
    "print(f\"   Training samples: {len(X_train)}\")\n",
    "print(f\"   Validation samples: {len(X_val)}\")\n",
    "print(f\"   Device: {DEVICE}\")\n",
    "\n",
    "# Report which device will actually be used\n",
    "if USE_GPU and tf.config.list_physical_devices('GPU'):\n",
    "    active_gpu = tf.config.list_physical_devices('GPU')[0]\n",
    "    print(f\"   ‚úì Training on GPU: {active_gpu.name}\")\n",
    "else:\n",
    "    print(\"   ‚ö† WARNING: No GPU detected, training will fall back to CPU\")\n",
    "\n",
    "# Optimize batch size based on GPU availability and model complexity\n",
    "if USE_GPU:\n",
    "    BATCH_SIZE = 256  # Keeps GPU busy without exhausting 4GB memory\n",
    "    print(f\"   Batch size: {BATCH_SIZE} (optimized for GPU)\")\n",
    "    print(\"   Expected memory usage: ~1.5-2.5 GB\")\n",
    "else:\n",
    "    BATCH_SIZE = 64  # Safer batch size for CPU training\n",
    "    print(f\"   Batch size: {BATCH_SIZE} (CPU mode)\")\n",
    "    print(\"   Tip: Increase to 128 if you have ample CPU RAM\")\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        'asl_mediapipe_mlp_model_best.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Build efficient tf.data pipelines (keeps GPU fed without CPU bottlenecks)\n",
    "train_ds = make_dataset(X_train, y_train, BATCH_SIZE, training=True)\n",
    "val_ds = make_dataset(X_val, y_val, BATCH_SIZE, training=False)\n",
    "\n",
    "optimizer_name = model.optimizer.__class__.__name__\n",
    "if hasattr(model.optimizer.learning_rate, 'numpy'):\n",
    "    lr_value = float(model.optimizer.learning_rate.numpy())\n",
    "else:\n",
    "    lr_value = float(model.optimizer.learning_rate)\n",
    "mixed_precision_status = \"Enabled\" if USE_GPU else \"N/A\"\n",
    "\n",
    "print(\"\\nüìä Training Configuration:\")\n",
    "print(f\"  - Optimizer: {optimizer_name} (lr={lr_value:.4e})\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(\"  - Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\")\n",
    "print(f\"  - Mixed precision: {mixed_precision_status}\")\n",
    "print(\"  - Validation data: dedicated holdout set (tf.data)\")\n",
    "\n",
    "# Train model with GPU\n",
    "print(\"\\n‚è±Ô∏è  Training started...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with tf.device(DEVICE):\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=20,  # Increased epochs, early stopping will prevent overfitting\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è  Training completed in {training_time:.2f} seconds ({training_time/60:.2f} minutes)\")\n",
    "\n",
    "# Save final model\n",
    "model.save(\"asl_mediapipe_mlp_model.h5\")\n",
    "print(\"‚úÖ Model saved as 'asl_mediapipe_mlp_model.h5'\")\n",
    "print(\"‚úÖ Best model saved as 'asl_mediapipe_mlp_model_best.h5'\")\n",
    "\n",
    "# Display training summary\n",
    "if hasattr(history, 'history'):\n",
    "    final_acc = history.history['accuracy'][-1]\n",
    "    final_val_acc = history.history['val_accuracy'][-1]\n",
    "    print(f\"\\nüìä Final Training Accuracy: {final_acc*100:.2f}%\")\n",
    "    print(f\"üìä Final Validation Accuracy: {final_val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Accuracy of the trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# GPU-ACCELERATED MODEL EVALUATION\n",
    "# ============================================\n",
    "\n",
    "print(\"üìä Loading model for evaluation...\")\n",
    "model = tf.keras.models.load_model(\"asl_mediapipe_mlp_model.h5\")\n",
    "\n",
    "print(f\"üß™ Evaluating on test data (Device: {DEVICE})...\")\n",
    "print(f\"   Test samples: {len(X_test)}\")\n",
    "\n",
    "eval_batch_size = 256 if USE_GPU else 128\n",
    "test_ds = make_dataset(X_test, y_test, eval_batch_size, training=False)\n",
    "\n",
    "# Evaluate on test data with GPU\n",
    "start_time = time.time()\n",
    "with tf.device(DEVICE):\n",
    "    loss, accuracy = model.evaluate(test_ds, verbose=1)\n",
    "\n",
    "eval_time = time.time() - start_time\n",
    "print(f\"\\n‚è±Ô∏è  Evaluation completed in {eval_time:.4f} seconds\")\n",
    "print(f\"üìä Test Loss: {loss:.4f}\")\n",
    "print(f\"üìä Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the Mediapipe Approach for Sign Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# REAL-TIME INFERENCE (WEBCAM)\n",
    "# ============================================\n",
    "# Commit-once-then-wait strategy (prevents letter repetition)\n",
    "# Control labels match CSV: 'space', 'del' (lowercase, no 'nothing' in ASL dataset)\n",
    "\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "print(f\"üì¶ Loading model for inference (Device: {DEVICE})...\")\n",
    "mlp_model = tf.keras.models.load_model(\"asl_mediapipe_mlp_model.h5\")\n",
    "if USE_GPU:\n",
    "    print(\"   ‚úÖ GPU acceleration enabled for inference\")\n",
    "\n",
    "# Load dataset to rebuild LabelEncoder\n",
    "df = pd.read_csv(\"asl_mediapipe_keypoints_dataset.csv\")\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df[\"label\"])\n",
    "print(f\"   Encoder classes ({len(encoder.classes_)}): {list(encoder.classes_[:5])}...\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "\n",
    "# Stabilization settings\n",
    "STABILIZATION_WINDOW_SIZE = 10\n",
    "STABILIZATION_THRESHOLD = 7\n",
    "MIN_CONFIDENCE = 0.70\n",
    "HOLD_TIME_REQUIRED = 0.8\n",
    "DISPLAY_WIDTH = 1280\n",
    "DISPLAY_HEIGHT = 720\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Cannot access camera\")\n",
    "else:\n",
    "    print(\"‚úÖ Camera opened. Press 'q' to quit, 'c' to clear\")\n",
    "    \n",
    "    window_name = \"Sign Language Recognition (MediaPipe MLP)\"\n",
    "    cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow(window_name, DISPLAY_WIDTH, DISPLAY_HEIGHT)\n",
    "    \n",
    "    # State variables\n",
    "    predicted_sentence = \"\"\n",
    "    stabilization_buffer = deque(maxlen=STABILIZATION_WINDOW_SIZE)\n",
    "    \n",
    "    # Commit-once-then-wait state\n",
    "    committed_label = None\n",
    "    current_sign_label = None\n",
    "    current_sign_start = None\n",
    "    waiting_for_change = False\n",
    "    \n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "    \n",
    "            # Process UNFLIPPED frame with MediaPipe (matches training data)\n",
    "            frame = cv2.resize(frame, (DISPLAY_WIDTH, DISPLAY_HEIGHT))\n",
    "            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            rgb_frame.flags.writeable = False\n",
    "            results = hands.process(rgb_frame)\n",
    "            rgb_frame.flags.writeable = True\n",
    "    \n",
    "            display_status = \"\"\n",
    "            status_color = (200, 200, 200)\n",
    "    \n",
    "            if results.multi_hand_landmarks:\n",
    "                for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "                    # Extract landmarks ‚Äî NO mirroring (matches training data)\n",
    "                    landmarks = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])\n",
    "                    input_data = landmarks.flatten().reshape(1, -1)\n",
    "                    input_tensor = tf.cast(input_data, tf.float32)\n",
    "    \n",
    "                    with tf.device(DEVICE):\n",
    "                        prediction = mlp_model.predict(input_tensor, verbose=0)\n",
    "                    predicted_class = np.argmax(prediction)\n",
    "                    confidence = float(np.max(prediction))\n",
    "                    predicted_label = encoder.inverse_transform([predicted_class])[0]\n",
    "    \n",
    "                    # Skip low confidence\n",
    "                    if confidence < MIN_CONFIDENCE:\n",
    "                        display_status = f\"{predicted_label} ({confidence:.0%}) Low conf\"\n",
    "                        status_color = (0, 100, 255)\n",
    "                        break\n",
    "    \n",
    "                    # Stability buffer\n",
    "                    stabilization_buffer.append(predicted_label)\n",
    "                    buffer_count = stabilization_buffer.count(predicted_label)\n",
    "                    is_stable = (buffer_count >= STABILIZATION_THRESHOLD and\n",
    "                                 len(stabilization_buffer) == STABILIZATION_WINDOW_SIZE)\n",
    "    \n",
    "                    if not is_stable:\n",
    "                        progress = buffer_count / STABILIZATION_THRESHOLD * 100\n",
    "                        display_status = f\"{predicted_label} ({confidence:.0%}) Stabilizing {progress:.0f}%\"\n",
    "                        status_color = (0, 255, 255)\n",
    "                        break\n",
    "    \n",
    "                    now = time.time()\n",
    "    \n",
    "                    # Check if waiting after a commit\n",
    "                    if waiting_for_change:\n",
    "                        if predicted_label == committed_label:\n",
    "                            display_status = f\"{predicted_label} ({confidence:.0%}) ‚úì Committed - change sign\"\n",
    "                            status_color = (255, 200, 0)\n",
    "                            break\n",
    "                        else:\n",
    "                            waiting_for_change = False\n",
    "                            committed_label = None\n",
    "                            current_sign_label = predicted_label\n",
    "                            current_sign_start = now\n",
    "    \n",
    "                    # Track hold time\n",
    "                    if predicted_label != current_sign_label:\n",
    "                        current_sign_label = predicted_label\n",
    "                        current_sign_start = now\n",
    "    \n",
    "                    hold_duration = now - current_sign_start if current_sign_start else 0\n",
    "    \n",
    "                    if hold_duration < HOLD_TIME_REQUIRED:\n",
    "                        hold_pct = hold_duration / HOLD_TIME_REQUIRED * 100\n",
    "                        display_status = f\"{predicted_label} ({confidence:.0%}) Hold: {hold_pct:.0f}%\"\n",
    "                        status_color = (0, 255, 255)\n",
    "                        break\n",
    "    \n",
    "                    # COMMIT ‚Äî control labels match CSV: 'space', 'del' (lowercase)\n",
    "                    if predicted_label == \"space\":\n",
    "                        if not predicted_sentence.endswith(\" \"):\n",
    "                            predicted_sentence += \" \"\n",
    "                    elif predicted_label == \"del\":\n",
    "                        if predicted_sentence:\n",
    "                            predicted_sentence = predicted_sentence[:-1]\n",
    "                    elif predicted_label not in (\"nothing\",):\n",
    "                        predicted_sentence += predicted_label\n",
    "    \n",
    "                    committed_label = predicted_label\n",
    "                    waiting_for_change = True\n",
    "                    current_sign_label = None\n",
    "                    current_sign_start = None\n",
    "                    stabilization_buffer.clear()\n",
    "    \n",
    "                    display_status = f\"{predicted_label} ({confidence:.0%}) ‚úì COMMITTED!\"\n",
    "                    status_color = (0, 255, 0)\n",
    "            else:\n",
    "                # No hand ‚Üí full reset\n",
    "                committed_label = None\n",
    "                waiting_for_change = False\n",
    "                current_sign_label = None\n",
    "                current_sign_start = None\n",
    "                stabilization_buffer.clear()\n",
    "                display_status = \"No hand detected\"\n",
    "                status_color = (150, 150, 150)\n",
    "    \n",
    "            # Flip for selfie-view display\n",
    "            frame = cv2.flip(frame, 1)\n",
    "    \n",
    "            # Status text\n",
    "            cv2.rectangle(frame, (0, 0), (DISPLAY_WIDTH, 50), (30, 30, 30), -1)\n",
    "            cv2.putText(frame, display_status, (10, 35),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.9, status_color, 2)\n",
    "    \n",
    "            # Bottom bar for sentence\n",
    "            bar_height = 60\n",
    "            frame_height, frame_width, _ = frame.shape\n",
    "            cv2.rectangle(frame, (0, frame_height - bar_height),\n",
    "                         (frame_width, frame_height), (0, 0, 0), -1)\n",
    "            cv2.putText(frame, predicted_sentence[-50:], (50, frame_height - 20),\n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "            cv2.imshow(window_name, frame)\n",
    "    \n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q'):\n",
    "                break\n",
    "            elif key == ord('c'):\n",
    "                predicted_sentence = \"\"\n",
    "                committed_label = None\n",
    "                waiting_for_change = False\n",
    "                stabilization_buffer.clear()\n",
    "                print(\"üóëÔ∏è Sentence cleared\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n‚ö†Ô∏è Interrupted by user\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(f\"\\nüìù Final sentence: {predicted_sentence}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
