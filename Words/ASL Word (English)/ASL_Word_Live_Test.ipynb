{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3f0f5d2",
   "metadata": {},
   "source": [
    "# ASL Word ‚Äî Live Webcam Testing\n",
    "\n",
    "This notebook lets you **test your trained ASL word model in real-time** using your webcam.\n",
    "\n",
    "### How it works:\n",
    "\n",
    "1. **Continuous capture** ‚Äî MediaPipe extracts hand landmarks every frame (supports 1 or 2 hands)\n",
    "2. **Sliding window** ‚Äî buffers the last 30 frames into a sequence\n",
    "3. **Prediction** ‚Äî feeds the sequence to the BiLSTM model every 0.5s\n",
    "4. **Sentence building** ‚Äî confirmed words are appended to a sentence\n",
    "\n",
    "### Two-Hand Support:\n",
    "\n",
    "- **Auto-detects** the model's expected input shape (63 or 126 features)\n",
    "- If the model expects **63 features** (1 hand) ‚Äî uses the dominant hand only\n",
    "- If the model expects **126 features** (2 hands) ‚Äî captures both hands and concatenates landmarks\n",
    "- Many ASL word signs require two hands for proper recognition\n",
    "\n",
    "### Controls:\n",
    "\n",
    "| Key         | Action                  |\n",
    "| ----------- | ----------------------- |\n",
    "| `q`         | Quit                    |\n",
    "| `r`         | Reset sentence          |\n",
    "| `SPACE`     | Add space between words |\n",
    "| `BACKSPACE` | Delete last word        |\n",
    "\n",
    "### Requirements:\n",
    "\n",
    "- Trained model: `asl_word_lstm_model_best.h5`\n",
    "- Class mapping: `asl_word_classes.csv`\n",
    "- Webcam connected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34079a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.10.0\n",
      "OpenCV: 4.11.0\n",
      "MediaPipe: 0.10.9\n",
      "‚úÖ GPU detected: /physical_device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 1: IMPORTS & SETUP\n",
    "# ===============================\n",
    "\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mediapipe as mp\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "print(f'TensorFlow: {tf.__version__}')\n",
    "print(f'OpenCV: {cv2.__version__}')\n",
    "print(f'MediaPipe: {mp.__version__}')\n",
    "\n",
    "# Check GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    print(f'‚úÖ GPU detected: {gpus[0].name}')\n",
    "else:\n",
    "    print('‚ö†Ô∏è No GPU ‚Äî running on CPU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1aeff2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Model  : M:\\Term 10\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\ASL Word (English)\\asl_word_lstm_model_final.h5\n",
      "üìÇ Classes: M:\\Term 10\\Grad\\Main\\Sign-Language-Recognition-System-main\\SLR Main\\Words\\ASL Word (English)\\asl_word_classes.csv\n",
      "üé¨ Sequence: 30 frames\n",
      "üéØ Confidence threshold: 0.35\n",
      "üîÅ Stability window: 3 predictions\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 2: CONFIGURATION\n",
    "# ===============================\n",
    "\n",
    "PROJECT_ROOT = Path(r'M:/Term 10/Grad')\n",
    "SLR_MAIN = PROJECT_ROOT / 'Main/Sign-Language-Recognition-System-main/SLR Main'\n",
    "WORDS_ROOT = SLR_MAIN / 'Words'\n",
    "OUTPUT_DIR = WORDS_ROOT / 'ASL Word (English)'\n",
    "SHARED_CSV = WORDS_ROOT / 'Shared/shared_word_vocabulary.csv'\n",
    "\n",
    "# Model files\n",
    "MODEL_PATH = OUTPUT_DIR / 'asl_word_lstm_model_final.h5'\n",
    "CLASSES_CSV = OUTPUT_DIR / 'asl_word_classes.csv'\n",
    "\n",
    "# Sequence parameters (must match training)\n",
    "SEQUENCE_LENGTH = 30    # frames per sequence\n",
    "\n",
    "# Hand detection mode: auto-detected from model input shape\n",
    "# - 63 features = 1 hand (21 landmarks x 3)\n",
    "# - 126 features = 2 hands (2 x 21 landmarks x 3)\n",
    "# Set to None for auto-detection, or override manually:\n",
    "NUM_FEATURES = None  # will be set after model loads\n",
    "\n",
    "# Live inference settings\n",
    "CONFIDENCE_THRESHOLD = 0.35     # minimum confidence to accept a prediction\n",
    "PREDICTION_INTERVAL = 0.5       # seconds between predictions\n",
    "STABILITY_WINDOW = 3            # consecutive same predictions needed to confirm\n",
    "COOLDOWN_TIME = 2.0             # seconds after confirming a word before next\n",
    "\n",
    "# Camera\n",
    "CAMERA_INDEX = 0\n",
    "CAMERA_WIDTH = 1280\n",
    "CAMERA_HEIGHT = 720\n",
    "\n",
    "print(f'üìÇ Model  : {MODEL_PATH}')\n",
    "print(f'üìÇ Classes: {CLASSES_CSV}')\n",
    "print(f'üé¨ Sequence: {SEQUENCE_LENGTH} frames')\n",
    "print(f'üéØ Confidence threshold: {CONFIDENCE_THRESHOLD}')\n",
    "print(f'üîÅ Stability window: {STABILITY_WINDOW} predictions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3eaf578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "‚úÖ Model loaded: ASL_Word_BiLSTM ‚Äî 294,429 parameters\n",
      "üñêÔ∏è Model expects 63 features ‚Üí 1 hand(s) mode\n",
      "üè∑Ô∏è 157 word classes loaded\n",
      "\n",
      "üìã Sample words: ['drink', 'chair', 'help', 'thin', 'walk', 'mother', 'table', 'bed', 'family', 'man', 'tall', 'doctor', 'eat', 'short', 'medicine']\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 3: LOAD MODEL & VOCABULARY\n",
    "# ===============================\n",
    "\n",
    "# --- Custom layer needed for model loading ---\n",
    "class TemporalAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"Temporal attention layer (must match training definition).\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='att_weight', shape=(input_shape[-1], 1),\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        self.b = self.add_weight(name='att_bias', shape=(input_shape[1], 1),\n",
    "                                 initializer='zeros', trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = tf.nn.tanh(tf.matmul(x, self.W) + self.b)\n",
    "        a = tf.nn.softmax(e, axis=1)\n",
    "        output = tf.reduce_sum(x * a, axis=1)\n",
    "        return output\n",
    "\n",
    "# Load model\n",
    "print('Loading model...')\n",
    "model = tf.keras.models.load_model(\n",
    "    str(MODEL_PATH),\n",
    "    custom_objects={'TemporalAttention': TemporalAttention}\n",
    ")\n",
    "print(f'‚úÖ Model loaded: {model.name} ‚Äî {model.count_params():,} parameters')\n",
    "\n",
    "# Auto-detect feature count from model input shape\n",
    "model_input_shape = model.input_shape  # (None, SEQUENCE_LENGTH, NUM_FEATURES)\n",
    "NUM_FEATURES = model_input_shape[-1]\n",
    "NUM_HANDS = 2 if NUM_FEATURES == 126 else 1\n",
    "LANDMARKS_PER_HAND = 21 * 3  # 63\n",
    "\n",
    "print(f'üñêÔ∏è Model expects {NUM_FEATURES} features ‚Üí {NUM_HANDS} hand(s) mode')\n",
    "\n",
    "# Load class mapping\n",
    "class_df = pd.read_csv(CLASSES_CSV)\n",
    "vocab_df = pd.read_csv(SHARED_CSV)\n",
    "vocab_df = vocab_df.dropna(subset=['wlasl_class'])\n",
    "\n",
    "id_to_english = dict(zip(vocab_df['word_id'].astype(int), vocab_df['english']))\n",
    "id_to_category = dict(zip(vocab_df['word_id'].astype(int), vocab_df['category']))\n",
    "\n",
    "# Build model_index -> word name mapping\n",
    "index_to_word = {}\n",
    "for _, row in class_df.iterrows():\n",
    "    idx = int(row['model_class_index'])\n",
    "    wid = int(row['word_id'])\n",
    "    index_to_word[idx] = id_to_english.get(wid, f'word_{wid}')\n",
    "\n",
    "num_classes = len(index_to_word)\n",
    "print(f'üè∑Ô∏è {num_classes} word classes loaded')\n",
    "print(f'\\nüìã Sample words: {list(index_to_word.values())[:15]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4453e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ MediaPipe hand detector ready (1 hand(s) mode)\n",
      "   Features per frame: 63\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# CELL 4: MEDIAPIPE HAND DETECTOR\n",
    "# ===============================\n",
    "# Supports both 1-hand and 2-hand detection based on model requirements\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=NUM_HANDS,       # dynamically set based on model\n",
    "    min_detection_confidence=0.6,\n",
    "    min_tracking_confidence=0.6\n",
    ")\n",
    "\n",
    "def extract_landmarks(frame):\n",
    "    \"\"\"Extract hand landmarks from a single frame.\n",
    "\n",
    "    - 1-hand mode (63 features): returns landmarks for the first detected hand.\n",
    "    - 2-hand mode (126 features): returns concatenated landmarks for both hands.\n",
    "      If only one hand is detected, the other hand's landmarks are zero-padded.\n",
    "      Hands are ordered: Left hand first, Right hand second (consistent ordering).\n",
    "\n",
    "    Returns: (feature_vector, list_of_hand_landmarks_for_drawing)\n",
    "    \"\"\"\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb)\n",
    "\n",
    "    draw_landmarks = []\n",
    "\n",
    "    if NUM_HANDS == 1:\n",
    "        # Single-hand mode (63 features)\n",
    "        if results.multi_hand_landmarks:\n",
    "            lm = results.multi_hand_landmarks[0]\n",
    "            vec = np.array([[p.x, p.y, p.z] for p in lm.landmark], dtype=np.float32).flatten()\n",
    "            draw_landmarks = [lm]\n",
    "            return vec, draw_landmarks\n",
    "        return np.zeros(NUM_FEATURES, dtype=np.float32), draw_landmarks\n",
    "\n",
    "    else:\n",
    "        # Two-hand mode (126 features)\n",
    "        left_vec = np.zeros(LANDMARKS_PER_HAND, dtype=np.float32)\n",
    "        right_vec = np.zeros(LANDMARKS_PER_HAND, dtype=np.float32)\n",
    "\n",
    "        if results.multi_hand_landmarks and results.multi_handedness:\n",
    "            for hand_lm, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "                draw_landmarks.append(hand_lm)\n",
    "                label = handedness.classification[0].label  # 'Left' or 'Right'\n",
    "                vec = np.array([[p.x, p.y, p.z] for p in hand_lm.landmark], dtype=np.float32).flatten()\n",
    "\n",
    "                # Note: MediaPipe labels are mirrored (camera mirror effect)\n",
    "                # 'Left' in MediaPipe = right hand in real life (when image is flipped)\n",
    "                if label == 'Left':\n",
    "                    left_vec = vec\n",
    "                else:\n",
    "                    right_vec = vec\n",
    "\n",
    "        # Concatenate: [left_hand(63) | right_hand(63)] = 126 features\n",
    "        combined = np.concatenate([left_vec, right_vec])\n",
    "        return combined, draw_landmarks\n",
    "\n",
    "print(f'‚úÖ MediaPipe hand detector ready ({NUM_HANDS} hand(s) mode)')\n",
    "print(f'   Features per frame: {NUM_FEATURES}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eb54aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìπ Camera opened [2 hand(s), 126 features]. Press Q to quit, R to reset, SPACE to add space, BACKSPACE to delete.\n",
      "\n",
      "üìù Final sentence: \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "import mediapipe as mp\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP: 2-HAND MEDIAPIPE INITIALIZATION\n",
    "# ==========================================\n",
    "NUM_HANDS = 2\n",
    "NUM_FEATURES = 126  # 21 landmarks * 3 coordinates (x,y,z) * 2 hands\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Initialize MediaPipe Hands explicitly for 2 hands\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=False,\n",
    "    max_num_hands=NUM_HANDS,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. FEATURE EXTRACTION LOGIC\n",
    "# ==========================================\n",
    "def extract_landmarks(frame):\n",
    "    \"\"\"\n",
    "    Extracts landmarks for up to 2 hands.\n",
    "    Right Hand is mapped to the first 63 features, Left Hand to the last 63.\n",
    "    Missing hands are padded with zeros.\n",
    "    \"\"\"\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    rh_data = np.zeros(63)\n",
    "    lh_data = np.zeros(63)\n",
    "    hand_lm_list = []\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks, handedness in zip(results.multi_hand_landmarks, results.multi_handedness):\n",
    "            hand_lm_list.append(hand_landmarks)\n",
    "            coords = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark]).flatten()\n",
    "\n",
    "            if handedness.classification[0].label == 'Right':\n",
    "                rh_data = coords\n",
    "            else:\n",
    "                lh_data = coords\n",
    "\n",
    "    # Combine into a single 126-feature array\n",
    "    landmarks = np.concatenate([rh_data, lh_data])\n",
    "    return landmarks, hand_lm_list\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# 3. LIVE WEBCAM TESTING LOOP\n",
    "# ==========================================\n",
    "def run_live_test():\n",
    "    \"\"\"Main live testing loop with sliding window prediction.\"\"\"\n",
    "\n",
    "    cap = cv2.VideoCapture(CAMERA_INDEX)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, CAMERA_WIDTH)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, CAMERA_HEIGHT)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print('‚ùå Cannot open camera!')\n",
    "        return\n",
    "\n",
    "    hand_mode_str = f'{NUM_HANDS} hand(s), {NUM_FEATURES} features'\n",
    "    print(f'üìπ Camera opened [{hand_mode_str}]. Press Q to quit, R to reset, SPACE to add space, BACKSPACE to delete.')\n",
    "\n",
    "    # --- State variables ---\n",
    "    frame_buffer = deque(maxlen=SEQUENCE_LENGTH)\n",
    "    prediction_history = deque(maxlen=STABILITY_WINDOW)\n",
    "    sentence_words = []\n",
    "    current_word = ''\n",
    "    current_conf = 0.0\n",
    "    last_prediction_time = 0.0\n",
    "    last_confirmed_time = 0.0\n",
    "    hand_detected = False\n",
    "    hands_count = 0\n",
    "    fps_history = deque(maxlen=30)\n",
    "\n",
    "    # Colors\n",
    "    GREEN = (0, 200, 0)\n",
    "    RED = (0, 0, 200)\n",
    "    BLUE = (200, 100, 0)\n",
    "    WHITE = (255, 255, 255)\n",
    "    BLACK = (0, 0, 0)\n",
    "    YELLOW = (0, 220, 220)\n",
    "    ORANGE = (0, 140, 255)\n",
    "\n",
    "    while True:\n",
    "        frame_start = time.time()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "        # --- Extract landmarks ---\n",
    "        landmarks, hand_lm_list = extract_landmarks(frame)\n",
    "        hand_detected = len(hand_lm_list) > 0\n",
    "        hands_count = len(hand_lm_list)\n",
    "        frame_buffer.append(landmarks)\n",
    "\n",
    "        # --- Draw hand landmarks (all detected hands) ---\n",
    "        for hand_lm in hand_lm_list:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_lm, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style()\n",
    "            )\n",
    "\n",
    "        # --- Predict when buffer is full ---\n",
    "        now = time.time()\n",
    "        if len(frame_buffer) == SEQUENCE_LENGTH and (now - last_prediction_time) >= PREDICTION_INTERVAL:\n",
    "            last_prediction_time = now\n",
    "\n",
    "            # Build sequence\n",
    "            seq = np.array(list(frame_buffer), dtype=np.float32)\n",
    "            seq = np.expand_dims(seq, axis=0)  # Shape will be (1, 30, 126)\n",
    "\n",
    "            # Check if sequence has enough non-zero frames\n",
    "            non_zero = np.sum(np.any(seq[0] != 0, axis=1))\n",
    "            if non_zero >= SEQUENCE_LENGTH * 0.3:  # at least 30% non-zero frames\n",
    "                proba = model.predict(seq, verbose=0)[0]\n",
    "                pred_idx = np.argmax(proba)\n",
    "                pred_conf = proba[pred_idx]\n",
    "                pred_word = index_to_word.get(pred_idx, '?')\n",
    "\n",
    "                # Top-3 for display\n",
    "                top3_idx = np.argsort(proba)[-3:][::-1]\n",
    "                top3 = [(index_to_word.get(i, '?'), proba[i]) for i in top3_idx]\n",
    "\n",
    "                if pred_conf >= CONFIDENCE_THRESHOLD:\n",
    "                    current_word = pred_word\n",
    "                    current_conf = pred_conf\n",
    "                    prediction_history.append(pred_word)\n",
    "\n",
    "                    # Check stability: same word predicted N times in a row\n",
    "                    if (len(prediction_history) == STABILITY_WINDOW and\n",
    "                        len(set(prediction_history)) == 1 and\n",
    "                        (now - last_confirmed_time) >= COOLDOWN_TIME):\n",
    "                        \n",
    "                        # Confirm the word!\n",
    "                        sentence_words.append(current_word)\n",
    "                        last_confirmed_time = now\n",
    "                        prediction_history.clear()\n",
    "                        print(f'‚úÖ Confirmed: \"{current_word}\" ({current_conf:.1%})')\n",
    "                else:\n",
    "                    current_word = ''\n",
    "                    current_conf = 0.0\n",
    "            else:\n",
    "                current_word = ''\n",
    "                current_conf = 0.0\n",
    "\n",
    "        # --- Draw UI Overlay ---\n",
    "\n",
    "        # Top bar: prediction info\n",
    "        cv2.rectangle(frame, (0, 0), (w, 90), BLACK, -1)\n",
    "        cv2.rectangle(frame, (0, 0), (w, 90), WHITE, 2)\n",
    "\n",
    "        if current_word:\n",
    "            color = GREEN if current_conf >= 0.6 else YELLOW if current_conf >= 0.4 else ORANGE\n",
    "            cv2.putText(frame, f'Word: {current_word}', (15, 35),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.0, color, 2)\n",
    "            cv2.putText(frame, f'Confidence: {current_conf:.1%}', (15, 65),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "\n",
    "            # Confidence bar\n",
    "            bar_x = 450\n",
    "            bar_w = 200\n",
    "            bar_h = 20\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + bar_w, 20 + bar_h), (50, 50, 50), -1)\n",
    "            fill_w = int(bar_w * current_conf)\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + fill_w, 20 + bar_h), color, -1)\n",
    "            cv2.rectangle(frame, (bar_x, 20), (bar_x + bar_w, 20 + bar_h), WHITE, 1)\n",
    "\n",
    "            # Stability progress\n",
    "            stable_count = sum(1 for p in prediction_history if p == current_word)\n",
    "            cv2.putText(frame, f'Stability: {stable_count}/{STABILITY_WINDOW}',\n",
    "                        (bar_x, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, WHITE, 1)\n",
    "        else:\n",
    "            status = 'Show a sign...' if hand_detected else 'No hand detected'\n",
    "            cv2.putText(frame, status, (15, 45),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (150, 150, 150), 2)\n",
    "\n",
    "        # Top-3 predictions (right side)\n",
    "        if current_word and 'top3' in locals():\n",
    "            tx = w - 320\n",
    "            cv2.putText(frame, 'Top 3:', (tx, 25), cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 1)\n",
    "            for rank, (tw, tc) in enumerate(top3):\n",
    "                y_pos = 45 + rank * 20\n",
    "                cv2.putText(frame, f'{rank+1}. {tw} ({tc:.1%})', (tx, y_pos),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, WHITE, 1)\n",
    "\n",
    "        # Bottom bar: sentence\n",
    "        sentence_text = ' '.join(sentence_words) if sentence_words else '(sentence will appear here)'\n",
    "        cv2.rectangle(frame, (0, h - 55), (w, h), BLACK, -1)\n",
    "        cv2.rectangle(frame, (0, h - 55), (w, h), WHITE, 2)\n",
    "        cv2.putText(frame, f'Sentence: {sentence_text}', (15, h - 20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, WHITE, 2)\n",
    "\n",
    "        # Buffer indicator (bottom-left)\n",
    "        buf_fill = len(frame_buffer) / SEQUENCE_LENGTH\n",
    "        buf_color = GREEN if buf_fill >= 1.0 else YELLOW\n",
    "        cv2.putText(frame, f'Buffer: {len(frame_buffer)}/{SEQUENCE_LENGTH}',\n",
    "                    (15, h - 70), cv2.FONT_HERSHEY_SIMPLEX, 0.5, buf_color, 1)\n",
    "\n",
    "        # Hand status indicator (shows hand count for two-hand mode)\n",
    "        if NUM_HANDS == 2:\n",
    "            if hands_count == 2:\n",
    "                hand_color = GREEN\n",
    "                hand_text = f'HANDS: 2/2'\n",
    "            elif hands_count == 1:\n",
    "                hand_color = YELLOW\n",
    "                hand_text = f'HANDS: 1/2'\n",
    "            else:\n",
    "                hand_color = RED\n",
    "                hand_text = 'NO HANDS'\n",
    "        else:\n",
    "            hand_color = GREEN if hand_detected else RED\n",
    "            hand_text = 'HAND OK' if hand_detected else 'NO HAND'\n",
    "\n",
    "        cv2.circle(frame, (w - 80, h - 75), 8, hand_color, -1)\n",
    "        cv2.putText(frame, hand_text, (w - 170, h - 70),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, hand_color, 1)\n",
    "\n",
    "        # FPS counter\n",
    "        fps = 1.0 / max(time.time() - frame_start, 1e-6)\n",
    "        fps_history.append(fps)\n",
    "        avg_fps = sum(fps_history) / len(fps_history)\n",
    "        cv2.putText(frame, f'FPS: {avg_fps:.0f}', (w - 110, 115),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, WHITE, 1)\n",
    "\n",
    "        # Mode indicator\n",
    "        mode_text = f'Mode: {NUM_HANDS}H / {NUM_FEATURES}F'\n",
    "        cv2.putText(frame, mode_text, (w - 200, 135),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (180, 180, 180), 1)\n",
    "\n",
    "        # Cooldown indicator\n",
    "        cooldown_remaining = max(0, COOLDOWN_TIME - (now - last_confirmed_time))\n",
    "        if cooldown_remaining > 0:\n",
    "            cv2.putText(frame, f'Cooldown: {cooldown_remaining:.1f}s',\n",
    "                        (w // 2 - 80, 115), cv2.FONT_HERSHEY_SIMPLEX, 0.6, ORANGE, 2)\n",
    "\n",
    "        # --- Show frame ---\n",
    "        cv2.imshow('ASL Word Recognition ‚Äî Live Test', frame)\n",
    "\n",
    "        # --- Handle keyboard ---\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('r'):\n",
    "            sentence_words.clear()\n",
    "            prediction_history.clear()\n",
    "            current_word = ''\n",
    "            print('üîÑ Sentence reset')\n",
    "        elif key == 32:  # SPACE\n",
    "            sentence_words.append(' ')\n",
    "            print('   [space added]')\n",
    "        elif key == 8:   # BACKSPACE\n",
    "            if sentence_words:\n",
    "                removed = sentence_words.pop()\n",
    "                print(f'‚¨ÖÔ∏è Removed: \"{removed}\"')\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    final_sentence = ' '.join(sentence_words)\n",
    "    print(f'\\nüìù Final sentence: {final_sentence}')\n",
    "    return final_sentence\n",
    "\n",
    "# --- RUN ---\n",
    "result = run_live_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217a6ef",
   "metadata": {},
   "source": [
    "## Tips\n",
    "\n",
    "| Issue                      | Solution                                                                                       |\n",
    "| -------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **Low FPS**                | Close other apps, reduce `CAMERA_WIDTH`/`CAMERA_HEIGHT`                                        |\n",
    "| **Wrong predictions**      | Hold the sign steadily for ~2 seconds                                                          |\n",
    "| **Camera not opening**     | Change `CAMERA_INDEX` to 1 or 2                                                                |\n",
    "| **Too sensitive**          | Increase `STABILITY_WINDOW` to 4-5                                                             |\n",
    "| **Not detecting**          | Lower `CONFIDENCE_THRESHOLD` to 0.25                                                           |\n",
    "| **Too slow between words** | Decrease `COOLDOWN_TIME` to 1.0                                                                |\n",
    "| **Only 1 hand shown**      | The model auto-detects hand count from its input shape. Retrain with 2 hands for full support. |\n",
    "\n",
    "### How to perform a sign:\n",
    "\n",
    "1. Face the camera with your hand(s) clearly visible\n",
    "2. Perform the sign gesture smoothly\n",
    "3. Wait for the stability bar to fill up\n",
    "4. The word will be confirmed and added to the sentence\n",
    "\n",
    "### Two-Hand Mode Notes:\n",
    "\n",
    "- If your model was trained with 126 features (2 hands), both hands will be tracked\n",
    "- Hands are ordered consistently: Left first, Right second\n",
    "- If only one hand is visible, the other hand's landmarks are zero-padded\n",
    "- For best results with two-hand signs, keep both hands in the camera frame\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
